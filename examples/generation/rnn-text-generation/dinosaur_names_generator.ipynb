{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from neuralnetlib.layers import Input, Embedding, LSTM, Dense\n",
    "from neuralnetlib.models import Sequential\n",
    "from neuralnetlib.preprocessing import one_hot_encode\n",
    "from neuralnetlib.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aachenosaurus', 'Aardonyx', 'Abdallahsaurus', 'Abelisaurus', 'Abrictosaurus']\n"
     ]
    }
   ],
   "source": [
    "with open('dinos.txt', 'r', encoding='utf-8') as f:\n",
    "    names = [line.strip() for line in f]\n",
    "\n",
    "print(names[:5])  # display the first 5 names of the list to check if they were loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 26\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(name) for name in names]\n",
    "max_length = max(lengths)\n",
    "print(f\"Maximum length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c410380fa90e7694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.355387600Z",
     "start_time": "2024-11-12T00:29:31.337383Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 54\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "PAD_TOKEN = ''   # Padding token (index 0)\n",
    "EOS_TOKEN = '$'  # End of sequence token (index 1)\n",
    "max_length = 15  # Maximum sequence length\n",
    "\n",
    "# Mapping dictionaries\n",
    "char_to_index = {PAD_TOKEN: 0, EOS_TOKEN: 1}\n",
    "index_to_char = {0: PAD_TOKEN, 1: EOS_TOKEN}\n",
    "\n",
    "# Extract unique characters and sort them\n",
    "unique_chars = sorted(set(''.join(names)))\n",
    "\n",
    "# Build character <-> index mappings starting at index 2\n",
    "for idx, char in enumerate(unique_chars, start=2):\n",
    "    char_to_index[char] = idx\n",
    "    index_to_char[idx] = char\n",
    "\n",
    "vocab_size = len(char_to_index)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1364a6786997a8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.397400200Z",
     "start_time": "2024-11-12T00:29:31.376400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 54\n",
      "X data shape: (18374, 15)\n",
      "y labels shape: (18374, 54)\n",
      "\n",
      "Example for Aachenosaurus:\n",
      "Input sequence: [ 0  0  0  0  0  0  0  0  0 28 28 30 35 32 41]\n",
      "Expected output: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Decoding example sequence:\n",
      "['', '', '', '', '', '', '', '', '', 'a', 'a', 'c', 'h', 'e', 'n']\n",
      "Next character: o\n"
     ]
    }
   ],
   "source": [
    "# Training sequences\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "# Create sequences and next characters\n",
    "for name in names:\n",
    "    name = name.lower()\n",
    "    name_chars = list(name) + [EOS_TOKEN]\n",
    "\n",
    "    for i in range(len(name_chars) - 1):\n",
    "        # Extract sequence\n",
    "        seq = name_chars[max(0, i - max_length + 1):i + 1]\n",
    "\n",
    "        # Padding and conversion to indices\n",
    "        padded_seq = [0] * (max_length - len(seq)) + [char_to_index[char] for char in seq]\n",
    "\n",
    "        sequences.append(padded_seq)\n",
    "        next_chars.append(char_to_index[name_chars[i + 1]])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(sequences)\n",
    "y = one_hot_encode(np.array(next_chars), vocab_size)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"X data shape: {X.shape}\")\n",
    "print(f\"y labels shape: {y.shape}\")\n",
    "\n",
    "# Display an example for verification\n",
    "print(f\"\\nExample for {names[0]}:\")\n",
    "print(f\"Input sequence: {X[5]}\")\n",
    "print(f\"Expected output: {y[5]}\")\n",
    "\n",
    "# Visualize tokens for the first example\n",
    "print(\"\\nDecoding example sequence:\")\n",
    "print([index_to_char[idx] for idx in X[5]])\n",
    "print(f\"Next character: {index_to_char[next_chars[5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317820e906d2065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.405405300Z",
     "start_time": "2024-11-12T00:29:31.384400500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(temperature=1.2, gradient_clip_threshold=5.0, enable_padding=False, padding_size=32)\n",
      "-------------------------------------------------\n",
      "Layer 1: Input(input_shape=(15,))\n",
      "Layer 2: Embedding(input_dim=54, output_dim=32)\n",
      "Layer 3: LSTM(units=32, return_sequences=False, return_state=False, random_state=None, clip_value=5.0)\n",
      "Layer 4: Dense(units=54)\n",
      "Layer 5: Activation(Softmax)\n",
      "-------------------------------------------------\n",
      "Loss function: CategoricalCrossentropy\n",
      "Optimizer: Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clip_norm=None, clip_value=None)\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "embedding_dim = 32\n",
    "lstm_units = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(max_length))\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(LSTM(units=lstm_units))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss_function='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccca9fb1b43dd948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:33:50.723169900Z",
     "start_time": "2024-11-12T00:29:31.399406700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/100 - loss: 2.7950 -  - 2.13s\n",
      "[==============================] 100% Epoch 2/100 - loss: 2.1613 -  - 1.97s\n",
      "[==============================] 100% Epoch 3/100 - loss: 2.0037 -  - 1.95s\n",
      "[==============================] 100% Epoch 4/100 - loss: 1.9348 -  - 1.93s\n",
      "[==============================] 100% Epoch 5/100 - loss: 1.8931 -  - 1.91s\n",
      "[==============================] 100% Epoch 6/100 - loss: 1.8621 -  - 2.09s\n",
      "[==============================] 100% Epoch 7/100 - loss: 1.8377 -  - 2.01s\n",
      "[==============================] 100% Epoch 8/100 - loss: 1.8180 -  - 1.91s\n",
      "[==============================] 100% Epoch 9/100 - loss: 1.8014 -  - 2.20s\n",
      "[==============================] 100% Epoch 10/100 - loss: 1.7871 -  - 1.92s\n",
      "[==============================] 100% Epoch 11/100 - loss: 1.7746 -  - 2.26s\n",
      "[==============================] 100% Epoch 12/100 - loss: 1.7634 -  - 1.95s\n",
      "[==============================] 100% Epoch 13/100 - loss: 1.7534 -  - 2.10s\n",
      "[==============================] 100% Epoch 14/100 - loss: 1.7443 -  - 2.15s\n",
      "[==============================] 100% Epoch 15/100 - loss: 1.7360 -  - 1.88s\n",
      "[==============================] 100% Epoch 16/100 - loss: 1.7284 -  - 1.92s\n",
      "[==============================] 100% Epoch 17/100 - loss: 1.7213 -  - 1.91s\n",
      "[==============================] 100% Epoch 18/100 - loss: 1.7147 -  - 1.86s\n",
      "[==============================] 100% Epoch 19/100 - loss: 1.7086 -  - 1.88s\n",
      "[==============================] 100% Epoch 20/100 - loss: 1.7027 -  - 1.88s\n",
      "[==============================] 100% Epoch 21/100 - loss: 1.6973 -  - 1.89s\n",
      "[==============================] 100% Epoch 22/100 - loss: 1.6921 -  - 1.87s\n",
      "[==============================] 100% Epoch 23/100 - loss: 1.6872 -  - 2.04s\n",
      "[==============================] 100% Epoch 24/100 - loss: 1.6825 -  - 2.01s\n",
      "[==============================] 100% Epoch 25/100 - loss: 1.6781 -  - 1.90s\n",
      "[==============================] 100% Epoch 26/100 - loss: 1.6738 -  - 1.97s\n",
      "[==============================] 100% Epoch 27/100 - loss: 1.6698 -  - 1.91s\n",
      "[==============================] 100% Epoch 28/100 - loss: 1.6660 -  - 1.87s\n",
      "[==============================] 100% Epoch 29/100 - loss: 1.6623 -  - 2.00s\n",
      "[==============================] 100% Epoch 30/100 - loss: 1.6588 -  - 1.96s\n",
      "[==============================] 100% Epoch 31/100 - loss: 1.6555 -  - 1.90s\n",
      "[==============================] 100% Epoch 32/100 - loss: 1.6523 -  - 1.88s\n",
      "[==============================] 100% Epoch 33/100 - loss: 1.6492 -  - 1.95s\n",
      "[==============================] 100% Epoch 34/100 - loss: 1.6463 -  - 1.86s\n",
      "[==============================] 100% Epoch 35/100 - loss: 1.6434 -  - 1.90s\n",
      "[==============================] 100% Epoch 36/100 - loss: 1.6407 -  - 2.04s\n",
      "[==============================] 100% Epoch 37/100 - loss: 1.6381 -  - 2.09s\n",
      "[==============================] 100% Epoch 38/100 - loss: 1.6355 -  - 2.25s\n",
      "[==============================] 100% Epoch 39/100 - loss: 1.6331 -  - 1.86s\n",
      "[==============================] 100% Epoch 40/100 - loss: 1.6307 -  - 2.02s\n",
      "[==============================] 100% Epoch 41/100 - loss: 1.6284 -  - 1.98s\n",
      "[==============================] 100% Epoch 42/100 - loss: 1.6262 -  - 1.97s\n",
      "[==============================] 100% Epoch 43/100 - loss: 1.6240 -  - 1.88s\n",
      "[==============================] 100% Epoch 44/100 - loss: 1.6219 -  - 2.01s\n",
      "[==============================] 100% Epoch 45/100 - loss: 1.6199 -  - 1.94s\n",
      "[==============================] 100% Epoch 46/100 - loss: 1.6179 -  - 2.18s\n",
      "[==============================] 100% Epoch 47/100 - loss: 1.6160 -  - 1.87s\n",
      "[==============================] 100% Epoch 48/100 - loss: 1.6141 -  - 1.89s\n",
      "[==============================] 100% Epoch 49/100 - loss: 1.6123 -  - 2.06s\n",
      "[==============================] 100% Epoch 50/100 - loss: 1.6105 -  - 2.09s\n",
      "[==============================] 100% Epoch 51/100 - loss: 1.6088 -  - 2.00s\n",
      "[==============================] 100% Epoch 52/100 - loss: 1.6071 -  - 1.89s\n",
      "[==============================] 100% Epoch 53/100 - loss: 1.6055 -  - 1.95s\n",
      "[==============================] 100% Epoch 54/100 - loss: 1.6039 -  - 2.00s\n",
      "[==============================] 100% Epoch 55/100 - loss: 1.6023 -  - 1.88s\n",
      "[==============================] 100% Epoch 56/100 - loss: 1.6007 -  - 1.98s\n",
      "[==============================] 100% Epoch 57/100 - loss: 1.5992 -  - 1.92s\n",
      "[==============================] 100% Epoch 58/100 - loss: 1.5977 -  - 2.01s\n",
      "[==============================] 100% Epoch 59/100 - loss: 1.5963 -  - 1.86s\n",
      "[==============================] 100% Epoch 60/100 - loss: 1.5948 -  - 2.02s\n",
      "[==============================] 100% Epoch 61/100 - loss: 1.5935 -  - 2.16s\n",
      "[==============================] 100% Epoch 62/100 - loss: 1.5922 -  - 1.91s\n",
      "[==============================] 100% Epoch 63/100 - loss: 1.5909 -  - 1.91s\n",
      "[==============================] 100% Epoch 64/100 - loss: 1.5897 -  - 1.96s\n",
      "[==============================] 100% Epoch 65/100 - loss: 1.5884 -  - 1.86s\n",
      "[==============================] 100% Epoch 66/100 - loss: 1.5873 -  - 1.88s\n",
      "[==============================] 100% Epoch 67/100 - loss: 1.5861 -  - 1.97s\n",
      "[==============================] 100% Epoch 68/100 - loss: 1.5850 -  - 2.15s\n",
      "[==============================] 100% Epoch 69/100 - loss: 1.5839 -  - 1.96s\n",
      "[==============================] 100% Epoch 70/100 - loss: 1.5828 -  - 2.13s\n",
      "[==============================] 100% Epoch 71/100 - loss: 1.5817 -  - 1.88s\n",
      "[==============================] 100% Epoch 72/100 - loss: 1.5806 -  - 2.17s\n",
      "[==============================] 100% Epoch 73/100 - loss: 1.5796 -  - 2.07s\n",
      "[==============================] 100% Epoch 74/100 - loss: 1.5786 -  - 1.90s\n",
      "[==============================] 100% Epoch 75/100 - loss: 1.5776 -  - 1.87s\n",
      "[==============================] 100% Epoch 76/100 - loss: 1.5766 -  - 1.89s\n",
      "[==============================] 100% Epoch 77/100 - loss: 1.5756 -  - 1.98s\n",
      "[==============================] 100% Epoch 78/100 - loss: 1.5746 -  - 1.86s\n",
      "[==============================] 100% Epoch 79/100 - loss: 1.5737 -  - 1.86s\n",
      "[==============================] 100% Epoch 80/100 - loss: 1.5727 -  - 1.88s\n",
      "[==============================] 100% Epoch 81/100 - loss: 1.5718 -  - 1.91s\n",
      "[==============================] 100% Epoch 82/100 - loss: 1.5709 -  - 1.96s\n",
      "[==============================] 100% Epoch 83/100 - loss: 1.5700 -  - 2.80s\n",
      "[==============================] 100% Epoch 84/100 - loss: 1.5691 -  - 2.62s\n",
      "[==============================] 100% Epoch 85/100 - loss: 1.5682 -  - 2.16s\n",
      "[==============================] 100% Epoch 86/100 - loss: 1.5673 -  - 2.28s\n",
      "[==============================] 100% Epoch 87/100 - loss: 1.5665 -  - 2.29s\n",
      "[==============================] 100% Epoch 88/100 - loss: 1.5656 -  - 2.28s\n",
      "[==============================] 100% Epoch 89/100 - loss: 1.5648 -  - 2.07s\n",
      "[==============================] 100% Epoch 90/100 - loss: 1.5640 -  - 1.93s\n",
      "[==============================] 100% Epoch 91/100 - loss: 1.5632 -  - 2.12s\n",
      "[==============================] 100% Epoch 92/100 - loss: 1.5624 -  - 2.25s\n",
      "[==============================] 100% Epoch 93/100 - loss: 1.5616 -  - 1.90s\n",
      "[==============================] 100% Epoch 94/100 - loss: 1.5609 -  - 2.05s\n",
      "[==============================] 100% Epoch 95/100 - loss: 1.5601 -  - 1.91s\n",
      "[==============================] 100% Epoch 96/100 - loss: 1.5594 -  - 1.89s\n",
      "[==============================] 100% Epoch 97/100 - loss: 1.5586 -  - 1.89s\n",
      "[==============================] 100% Epoch 98/100 - loss: 1.5579 -  - 1.92s\n",
      "[==============================] 100% Epoch 99/100 - loss: 1.5572 -  - 1.88s\n",
      "[==============================] 100% Epoch 100/100 - loss: 1.5565 -  - 1.91s\n"
     ]
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Model training\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(X, y),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec75af38129a34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:35:16.841136700Z",
     "start_time": "2024-11-12T00:35:16.814132100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated names:\n",
      "Brosaucus (9 characters)\n",
      "Anuratops (9 characters)\n",
      "Afrinatopyl (11 characters)\n",
      "Epyota (6 characters)\n",
      "Amillusaurus (12 characters)\n",
      "\n",
      "Are all names original? True\n",
      "\n",
      "Average length: 8.2 characters\n",
      "Minimum length: 6 characters\n",
      "Maximum length: 12 characters\n",
      "\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# Generate multiple names\n",
    "generated_names = []\n",
    "number_of_names = 5\n",
    "min_length = 5\n",
    "\n",
    "while len(generated_names) < number_of_names:\n",
    "    # Start sequence with a single padding token\n",
    "    sequence_start = np.array([[char_to_index[PAD_TOKEN]]])  # shape: (1, 1)\n",
    "\n",
    "    # Generate a sequence\n",
    "    tokens = model.generate_sequence(\n",
    "        sequence_start=sequence_start,\n",
    "        max_length=max_length,\n",
    "        stop_token=char_to_index[EOS_TOKEN],\n",
    "        min_length=min_length,\n",
    "        temperature=1.2\n",
    "    )\n",
    "\n",
    "    # Convert indices to characters (excluding padding and end tokens)\n",
    "    name = ''.join(index_to_char[idx] for idx in tokens[0]\n",
    "                   if idx not in [char_to_index[PAD_TOKEN], char_to_index[EOS_TOKEN]])\n",
    "    name = name.capitalize()\n",
    "\n",
    "    # Check if the name is long enough, unique, and contains at least one vowel\n",
    "    if len(name) >= min_length and name not in generated_names and any(c in 'aeiou' for c in name.lower()):\n",
    "        generated_names.append(name)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGenerated names:\")\n",
    "for name in generated_names:\n",
    "    print(f\"{name} ({len(name)} characters)\")\n",
    "\n",
    "# Check originality\n",
    "print(\"\\nAre all names original?\", all(name.lower() not in [n.lower() for n in names] for name in generated_names))\n",
    "\n",
    "# Length statistics\n",
    "lengths = [len(name) for name in generated_names]\n",
    "print(f\"\\nAverage length: {sum(lengths)/len(lengths):.1f} characters\")\n",
    "print(f\"Minimum length: {min(lengths)} characters\")\n",
    "print(f\"Maximum length: {max(lengths)} characters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
