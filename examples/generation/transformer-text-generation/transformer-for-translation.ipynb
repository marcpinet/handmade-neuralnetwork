{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"je suis heureux.\",\n",
    "    \"j'aime les chats.\",\n",
    "    \"bonjour le monde.\",\n",
    "    \"au revoir.\",\n",
    "    \"comment allez-vous ?\",\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"i am happy.\",\n",
    "    \"i like cats.\",\n",
    "    \"hello world.\",\n",
    "    \"goodbye.\",\n",
    "    \"how are you?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "en_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "\n",
    "x_train_padded = pad_sequences(x_train, max_length=max_len_x, padding='post')\n",
    "y_train_padded = pad_sequences(y_train, max_length=max_len_y, padding='post')\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index) + 1\n",
    "vocab_size_en = len(en_tokenizer.word_index) + 1\n",
    "max_seq_len = max(max_len_x, max_len_y) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French sentences:\n",
      "['je suis heureux.', \"j'aime les chats.\", 'bonjour le monde.', 'au revoir.', 'comment allez-vous ?']\n",
      "English sentences:\n",
      "['i am happy.', 'i like cats.', 'hello world.', 'goodbye.', 'how are you?']\n",
      "French tokenizer:\n",
      "{'je': 1, 'suis': 2, 'heureux': 3, \"j'\": 4, 'aime': 5, 'les': 6, 'chats': 7, 'bonjour': 8, 'le': 9, 'monde': 10, 'au': 11, 'revoir': 12, 'comment': 13, 'allez': 14, 'vous': 15, '<OOV>': 16}\n",
      "English tokenizer:\n",
      "{'i': 1, 'am': 2, 'happy': 3, 'like': 4, 'cats': 5, 'hello': 6, 'world': 7, 'goodbye': 8, 'how': 9, 'are': 10, 'you': 11, '<OOV>': 12}\n",
      "Padded French sequences:\n",
      "[[ 1  2  3 16  0]\n",
      " [ 4  5  6  7 16]\n",
      " [ 8  9 10 16  0]\n",
      " [11 12 16  0  0]\n",
      " [13 14 16 15 16]]\n",
      "Padded English sequences:\n",
      "[[ 1  2  3 12]\n",
      " [ 1  4  5 12]\n",
      " [ 6  7 12  0]\n",
      " [ 8 12  0  0]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)\n",
    "print(\"Padded French sequences:\")\n",
    "print(x_train_padded)\n",
    "print(\"Padded English sequences:\")\n",
    "print(y_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  vocab_size=20,\n",
      "  d_model=128,\n",
      "  n_heads=4,\n",
      "  n_encoder_layers=3,\n",
      "  n_decoder_layers=3,\n",
      "  d_ff=512,\n",
      "  dropout_rate=0.1,\n",
      "  max_sequence_length=7\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=max(vocab_size_fr, vocab_size_en),\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=3,\n",
    "    n_decoder_layers=3,\n",
    "    d_ff=512,\n",
    "    dropout_rate=0.1,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add start and end tokens to the target sequences\n",
    "y_train_with_tokens = np.zeros((y_train_padded.shape[0], max_len_y + 2), dtype=int)\n",
    "y_train_with_tokens[:, 0] = model.SOS_IDX \n",
    "y_train_with_tokens[:, -1] = model.EOS_IDX\n",
    "y_train_with_tokens[:, 1:-1] = y_train_padded\n",
    "\n",
    "model.compile(\n",
    "    loss_function='sequencecrossentropy',\n",
    "    optimizer='adam',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/100 - loss: 13.0978 -  - 0.09s\n",
      "[==============================] 100% Epoch 2/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 3/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 4/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 5/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 6/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 7/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 8/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 9/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 10/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 11/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 12/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 13/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 14/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 15/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 16/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 17/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 18/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 19/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 20/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 21/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 22/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 23/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 24/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 25/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 26/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 27/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 28/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 29/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 30/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 31/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 32/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 33/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 34/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 35/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 36/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 37/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 38/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 39/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 40/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 41/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 42/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 43/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 44/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 45/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 46/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 47/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 48/100 - loss: 13.0978 -  - 0.10s\n",
      "[==============================] 100% Epoch 49/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 50/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 51/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 52/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 53/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 54/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 55/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 56/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 57/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 58/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 59/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 60/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 61/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 62/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 63/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 64/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 65/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 66/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 67/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 68/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 69/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 70/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 71/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 72/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 73/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 74/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 75/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 76/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 77/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 78/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 79/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 80/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 81/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 82/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 83/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 84/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 85/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 86/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 87/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 88/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 89/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 90/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 91/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 92/100 - loss: 13.0978 -  - 0.08s\n",
      "[==============================] 100% Epoch 93/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 94/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 95/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 96/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 97/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 98/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 99/100 - loss: 13.0978 -  - 0.07s\n",
      "[==============================] 100% Epoch 100/100 - loss: 13.0978 -  - 0.07s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train_padded, y_train_with_tokens,\n",
    "    epochs=100,\n",
    "    batch_size=1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: je suis heureux.\n",
      "EN: are are are are are are\n",
      "\n",
      "FR: comment allez-vous ?\n",
      "EN: are are are are are are\n",
      "\n",
      "FR: bonjour le monde.\n",
      "EN: are are are are are are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"je suis heureux.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour le monde.\"\n",
    "]\n",
    "\n",
    "for fr_sent in test_sentences:\n",
    "    # Convert input sentence to sequence\n",
    "    input_seq = [fr_tokenizer.word_index.get(word, model.PAD_IDX) for word in fr_sent.split()]\n",
    "    \n",
    "    # Pad the input sequence to max_seq_len\n",
    "    input_seq = np.pad(\n",
    "        [input_seq], \n",
    "        ((0, 0), (0, max_seq_len - len(input_seq))),\n",
    "        constant_values=model.PAD_IDX\n",
    "    )\n",
    "    \n",
    "    # Predict the output sequence\n",
    "    output_seq = model.predict(input_seq, max_length=max_seq_len)[0]\n",
    "    \n",
    "    # Convert output indices to words\n",
    "    output_words = []\n",
    "    for idx in output_seq[1:]:  # Skip <SOS> token\n",
    "        if idx == model.EOS_IDX:  # Stop at <EOS>\n",
    "            break\n",
    "        word = en_tokenizer.index_word.get(idx, \"<OOV>\")\n",
    "        output_words.append(word)\n",
    "    \n",
    "    print(f\"FR: {fr_sent}\")\n",
    "    print(f\"EN: {' '.join(output_words)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
