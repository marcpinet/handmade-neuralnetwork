{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.losses import SequenceCrossEntropy\n",
    "from neuralnetlib.optimizers import Adam\n",
    "from neuralnetlib.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfcb1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer, \n",
    "             temperature=0.8, beam_size=10, min_length=2) -> str:\n",
    "    print(f\"\\nProcessing: {sentence}\")\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Original words: {[fr_tokenizer.index_word.get(t, '<UNK>') for t in tokens]}\")\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    print(f\"With special tokens: {tokens}\")\n",
    "    \n",
    "    # Padding\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    print(f\"Padded sequence: {padded}\")\n",
    "    \n",
    "    pred = model.predict(padded, \n",
    "                        max_length=model.max_sequence_length,\n",
    "                        beam_size=beam_size,      \n",
    "                        alpha=0.6,         \n",
    "                        min_length=min_length,\n",
    "                        temperature=temperature)[0]\n",
    "    \n",
    "    print(f\"Raw prediction: {pred}\")\n",
    "    \n",
    "    words = []\n",
    "    for idx in pred[1:]:  # Skip SOS\n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.eos_token]:\n",
    "            print(f\"Found EOS token: {idx}\")\n",
    "            break\n",
    "            \n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.pad_token]:\n",
    "            print(f\"Skipping PAD token: {idx}\")\n",
    "            continue\n",
    "            \n",
    "        word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "        print(f\"Token {idx} -> word: {word}\")\n",
    "        words.append(word)\n",
    "    \n",
    "    if not words:\n",
    "        print(\"Warning: Empty translation, using default handling...\")\n",
    "        for idx in pred[1:]:  # Skip SOS\n",
    "            if idx not in [en_tokenizer.word_index[token] for token in \n",
    "                         [en_tokenizer.pad_token, en_tokenizer.eos_token]]:\n",
    "                word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "                words.append(word)\n",
    "    \n",
    "    translation = \" \".join(words) if words else \"[Translation failed]\"\n",
    "    print(f\"Final translation: {translation}\")\n",
    "    return translation\n",
    "\n",
    "class TrainingMonitor(Callback):\n",
    "    def __init__(self, model, fr_tokenizer, en_tokenizer, test_sentences):\n",
    "        self.model = model\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.test_sentences = test_sentences\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:  # Check every 5 epochs\n",
    "            print(f\"\\nEpoch {epoch} validation:\")\n",
    "            for sent in self.test_sentences:\n",
    "                translation = translate(\n",
    "                    sent, self.model, self.fr_tokenizer, self.en_tokenizer,\n",
    "                    temperature=0.8, beam_size=5, min_length=2\n",
    "                )\n",
    "                print(f\"Input: {sent}\")\n",
    "                print(f\"Output: {translation}\\n\")\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"je vais bien\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"bonjour.\",\n",
    "    \"au revoir.\",\n",
    "    \"merci beaucoup.\",\n",
    "    \"s'il vous plaît.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"je vais bien.\",\n",
    "    \"je suis fatigué.\",\n",
    "    \"je suis content.\",\n",
    "    \"quel est votre nom ?\",\n",
    "    \"mon nom est Jean.\",\n",
    "    \"enchanté de vous rencontrer.\",\n",
    "    \"bonne journée.\",\n",
    "    \"bonne soirée.\",\n",
    "    \"à demain.\",\n",
    "    \"j'aime le café.\",\n",
    "    \"je n'aime pas le thé.\",\n",
    "    \"quelle heure est-il ?\",\n",
    "    \"il est trois heures.\",\n",
    "    \"où est la gare ?\",\n",
    "    \"la gare est près d'ici.\",\n",
    "    \"combien ça coûte ?\",\n",
    "    \"c'est trop cher.\",\n",
    "    \"parlez-vous anglais ?\",\n",
    "    \"un peu.\",\n",
    "    \"je ne comprends pas.\",\n",
    "    \"pouvez-vous répéter ?\",\n",
    "    \"je suis désolé.\",\n",
    "    \"pas de problème.\",\n",
    "    \"bon appétit.\",\n",
    "    \"à votre santé.\",\n",
    "    \"j'ai faim.\",\n",
    "    \"j'ai soif.\",\n",
    "    \"il fait beau aujourd'hui.\",\n",
    "    \"il pleut.\",\n",
    "    \"il fait froid.\",\n",
    "    \"il fait chaud.\",\n",
    "    \"je travaille ici.\",\n",
    "    \"où habitez-vous ?\",\n",
    "    \"j'habite à Paris.\",\n",
    "    \"quel âge avez-vous ?\",\n",
    "    \"j'ai vingt-cinq ans.\",\n",
    "    \"avez-vous des frères et sœurs ?\",\n",
    "    \"j'ai une sœur.\",\n",
    "    \"j'ai un chat.\",\n",
    "    \"j'aime voyager.\",\n",
    "    \"je suis étudiant.\",\n",
    "    \"je suis professeur.\",\n",
    "    \"au secours !\",\n",
    "    \"joyeux anniversaire !\",\n",
    "    \"félicitations !\"\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"hello.\",\n",
    "    \"goodbye.\",\n",
    "    \"thank you very much.\",\n",
    "    \"please.\",\n",
    "    \"how are you?\",\n",
    "    \"i am fine.\",\n",
    "    \"i am tired.\",\n",
    "    \"i am happy.\",\n",
    "    \"what is your name?\",\n",
    "    \"my name is John.\",\n",
    "    \"nice to meet you.\",\n",
    "    \"have a nice day.\",\n",
    "    \"have a good evening.\",\n",
    "    \"see you tomorrow.\",\n",
    "    \"i like coffee.\",\n",
    "    \"i don't like tea.\",\n",
    "    \"what time is it?\",\n",
    "    \"it is three o'clock.\",\n",
    "    \"where is the train station?\",\n",
    "    \"the station is nearby.\",\n",
    "    \"how much is it?\",\n",
    "    \"it's too expensive.\",\n",
    "    \"do you speak english?\",\n",
    "    \"a little.\",\n",
    "    \"i don't understand.\",\n",
    "    \"can you repeat?\",\n",
    "    \"i am sorry.\",\n",
    "    \"no problem.\",\n",
    "    \"enjoy your meal.\",\n",
    "    \"cheers.\",\n",
    "    \"i am hungry.\",\n",
    "    \"i am thirsty.\",\n",
    "    \"the weather is nice today.\",\n",
    "    \"it's raining.\",\n",
    "    \"it's cold.\",\n",
    "    \"it's hot.\",\n",
    "    \"i work here.\",\n",
    "    \"where do you live?\",\n",
    "    \"i live in Paris.\",\n",
    "    \"how old are you?\",\n",
    "    \"i am twenty-five years old.\",\n",
    "    \"do you have brothers and sisters?\",\n",
    "    \"i have a sister.\",\n",
    "    \"i have a cat.\",\n",
    "    \"i like to travel.\",\n",
    "    \"i am a student.\",\n",
    "    \"i am a teacher.\",\n",
    "    \"help!\",\n",
    "    \"happy birthday!\",\n",
    "    \"congratulations!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "en_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 95, vocab_size_fr: 107\n",
      "max_len_x: 8, max_len_y: 8, max_vocab_size: 107, max_seq_len: 8\n",
      "French sentences:\n",
      "['bonjour.', 'au revoir.', 'merci beaucoup.', \"s'il vous plaît.\", 'comment allez-vous ?', 'je vais bien.', 'je suis fatigué.', 'je suis content.', 'quel est votre nom ?', 'mon nom est Jean.', 'enchanté de vous rencontrer.', 'bonne journée.', 'bonne soirée.', 'à demain.', \"j'aime le café.\", \"je n'aime pas le thé.\", 'quelle heure est-il ?', 'il est trois heures.', 'où est la gare ?', \"la gare est près d'ici.\", 'combien ça coûte ?', \"c'est trop cher.\", 'parlez-vous anglais ?', 'un peu.', 'je ne comprends pas.', 'pouvez-vous répéter ?', 'je suis désolé.', 'pas de problème.', 'bon appétit.', 'à votre santé.', \"j'ai faim.\", \"j'ai soif.\", \"il fait beau aujourd'hui.\", 'il pleut.', 'il fait froid.', 'il fait chaud.', 'je travaille ici.', 'où habitez-vous ?', \"j'habite à Paris.\", 'quel âge avez-vous ?', \"j'ai vingt-cinq ans.\", 'avez-vous des frères et sœurs ?', \"j'ai une sœur.\", \"j'ai un chat.\", \"j'aime voyager.\", 'je suis étudiant.', 'je suis professeur.', 'au secours !', 'joyeux anniversaire !', 'félicitations !']\n",
      "English sentences:\n",
      "['hello.', 'goodbye.', 'thank you very much.', 'please.', 'how are you?', 'i am fine.', 'i am tired.', 'i am happy.', 'what is your name?', 'my name is John.', 'nice to meet you.', 'have a nice day.', 'have a good evening.', 'see you tomorrow.', 'i like coffee.', \"i don't like tea.\", 'what time is it?', \"it is three o'clock.\", 'where is the train station?', 'the station is nearby.', 'how much is it?', \"it's too expensive.\", 'do you speak english?', 'a little.', \"i don't understand.\", 'can you repeat?', 'i am sorry.', 'no problem.', 'enjoy your meal.', 'cheers.', 'i am hungry.', 'i am thirsty.', 'the weather is nice today.', \"it's raining.\", \"it's cold.\", \"it's hot.\", 'i work here.', 'where do you live?', 'i live in Paris.', 'how old are you?', 'i am twenty-five years old.', 'do you have brothers and sisters?', 'i have a sister.', 'i have a cat.', 'i like to travel.', 'i am a student.', 'i am a teacher.', 'help!', 'happy birthday!', 'congratulations!']\n",
      "French tokenizer:\n",
      "{'<PAD>': 0, '.': 1, '?': 2, 'je': 3, 'vous': 4, '-': 5, \"j'\": 6, 'il': 7, 'est': 8, 'suis': 9, 'ai': 10, 'à': 11, 'aime': 12, 'pas': 13, 'fait': 14, '!': 15, 'au': 16, 'quel': 17, 'votre': 18, 'nom': 19, 'de': 20, 'bonne': 21, 'le': 22, 'où': 23, 'la': 24, 'gare': 25, 'ici': 26, 'un': 27, 'avez': 28, 'bonjour': 29, 'revoir': 30, 'merci': 31, 'beaucoup': 32, \"s'\": 33, 'plaît': 34, 'comment': 35, 'allez': 36, 'vais': 37, 'bien': 38, 'fatigué': 39, 'content': 40, 'mon': 41, 'jean': 42, 'enchanté': 43, 'rencontrer': 44, 'journée': 45, 'soirée': 46, 'demain': 47, 'café': 48, \"n'\": 49, 'thé': 50, 'quelle': 51, 'heure': 52, 'trois': 53, 'heures': 54, 'près': 55, \"d'\": 56, 'combien': 57, 'ça': 58, 'coûte': 59, \"c'\": 60, 'trop': 61, 'cher': 62, 'parlez': 63, 'anglais': 64, 'peu': 65, 'ne': 66, 'comprends': 67, 'pouvez': 68, 'répéter': 69, 'désolé': 70, 'problème': 71, 'bon': 72, 'appétit': 73, 'santé': 74, 'faim': 75, 'soif': 76, 'beau': 77, \"aujourd'hui\": 78, 'pleut': 79, 'froid': 80, 'chaud': 81, 'travaille': 82, 'habitez': 83, 'habite': 84, 'paris': 85, 'âge': 86, 'vingt': 87, 'cinq': 88, 'ans': 89, 'des': 90, 'frères': 91, 'et': 92, 'sœurs': 93, 'une': 94, 'sœur': 95, 'chat': 96, 'voyager': 97, 'étudiant': 98, 'professeur': 99, 'secours': 100, 'joyeux': 101, 'anniversaire': 102, 'félicitations': 103, '<UNK>': 105, '<SOS>': 106, '<EOS>': 107}\n",
      "English tokenizer:\n",
      "{'<PAD>': 0, '.': 1, 'i': 2, '?': 3, 'you': 4, 'am': 5, 'is': 6, 'a': 7, 'have': 8, \"it's\": 9, 'how': 10, 'nice': 11, 'like': 12, 'it': 13, 'the': 14, 'do': 15, '!': 16, 'much': 17, 'are': 18, 'happy': 19, 'what': 20, 'your': 21, 'name': 22, 'to': 23, \"don't\": 24, 'where': 25, 'station': 26, 'live': 27, 'old': 28, 'hello': 29, 'goodbye': 30, 'thank': 31, 'very': 32, 'please': 33, 'fine': 34, 'tired': 35, 'my': 36, 'john': 37, 'meet': 38, 'day': 39, 'good': 40, 'evening': 41, 'see': 42, 'tomorrow': 43, 'coffee': 44, 'tea': 45, 'time': 46, 'three': 47, \"o'\": 48, 'clock': 49, 'train': 50, 'nearby': 51, 'too': 52, 'expensive': 53, 'speak': 54, 'english': 55, 'little': 56, 'understand': 57, 'can': 58, 'repeat': 59, 'sorry': 60, 'no': 61, 'problem': 62, 'enjoy': 63, 'meal': 64, 'cheers': 65, 'hungry': 66, 'thirsty': 67, 'weather': 68, 'today': 69, 'raining': 70, 'cold': 71, 'hot': 72, 'work': 73, 'here': 74, 'in': 75, 'paris': 76, 'twenty': 77, '-': 78, 'five': 79, 'years': 80, 'brothers': 81, 'and': 82, 'sisters': 83, 'sister': 84, 'cat': 85, 'travel': 86, 'student': 87, 'teacher': 88, 'help': 89, 'birthday': 90, 'congratulations': 91, '<UNK>': 93, '<SOS>': 94, '<EOS>': 95}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  vocab_size=108,\n",
      "  d_model=32,\n",
      "  n_heads=2,\n",
      "  n_encoder_layers=2,\n",
      "  n_decoder_layers=2,\n",
      "  d_ff=64,\n",
      "  dropout_rate=0.4,\n",
      "  max_sequence_length=8\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=max_vocab_size,\n",
    "    d_model=32,        \n",
    "    n_heads=2,         \n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    d_ff=64,           \n",
    "    dropout_rate=0.4,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    random_state=42,\n",
    "    unk_idx=fr_tokenizer.word_index[fr_tokenizer.unk_token],\n",
    "    sos_idx=fr_tokenizer.word_index[fr_tokenizer.sos_token],\n",
    "    eos_idx=fr_tokenizer.word_index[fr_tokenizer.eos_token],\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function=SequenceCrossEntropy(\n",
    "        label_smoothing=0.1,\n",
    "    ),\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9\n",
    "    ),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "527483e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting training data:\n",
      "\n",
      "Example 1:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.26337567 -0.88846732  1.47795107 -0.91079202 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.21345575 -0.84524867  1.5465388  -0.8678127  -0.8678127\n",
      " -0.8678127  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Example 2:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.55359679 -0.24105097 -0.88846732  1.47795107 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.19089172 -0.84524867  1.5465388  -0.8678127  -0.8678127\n",
      " -0.8678127  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Example 3:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.21872627 -0.19640156 -0.88846732  1.47795107 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.16832768 -0.77755657 -0.14576365 -0.48422414 -0.84524867\n",
      "  1.5465388  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "def inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer):\n",
    "    print(\"\\nInspecting training data:\")\n",
    "    for i in range(min(3, len(x_train_padded))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"Input sequence:\")\n",
    "        print(f\"Raw: {x_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [fr_tokenizer.index_word.get(idx, '<UNK>') for idx in x_train_padded[i]])\n",
    "        \n",
    "        print(\"\\nOutput sequence:\")\n",
    "        print(f\"Raw: {y_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [en_tokenizer.index_word.get(idx, '<UNK>') for idx in y_train_padded[i]])\n",
    "\n",
    "inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/300 - loss: 4.7468 -  - 0.13s\n",
      "Epoch 0 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  83  83  84  79  92  92  89 107]\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 84 -> word: sister\n",
      "Token 79 -> word: five\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 89 -> word: help\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sisters sisters sister five <UNK> <UNK> help <UNK>\n",
      "Input: je vais bien\n",
      "Output: sisters sisters sister five <UNK> <UNK> help <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  81  80  82  80  92  92  90 107]\n",
      "Token 81 -> word: brothers\n",
      "Token 80 -> word: years\n",
      "Token 82 -> word: and\n",
      "Token 80 -> word: years\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 90 -> word: birthday\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: brothers years and years <UNK> <UNK> birthday <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: brothers years and years <UNK> <UNK> birthday <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  83  83  83  83  88  88  84 107]\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 88 -> word: teacher\n",
      "Token 88 -> word: teacher\n",
      "Token 84 -> word: sister\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sisters sisters sisters sisters teacher teacher sister <UNK>\n",
      "Input: bonjour\n",
      "Output: sisters sisters sisters sisters teacher teacher sister <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 2/300 - loss: 4.9383 -  - 0.13s\n",
      "[==============================] 100% Epoch 3/300 - loss: 4.9571 -  - 0.13s\n",
      "[==============================] 100% Epoch 4/300 - loss: 3.7296 -  - 0.12s\n",
      "[==============================] 100% Epoch 5/300 - loss: 2.7319 -  - 0.12s\n",
      "[==============================] 100% Epoch 6/300 - loss: 2.1833 -  - 0.12s\n",
      "Epoch 5 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  81  80  84  79  92  92  92 107]\n",
      "Token 81 -> word: brothers\n",
      "Token 80 -> word: years\n",
      "Token 84 -> word: sister\n",
      "Token 79 -> word: five\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: brothers years sister five <UNK> <UNK> <UNK> <UNK>\n",
      "Input: je vais bien\n",
      "Output: brothers years sister five <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  80  80  80  83  92  92  90 107]\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 83 -> word: sisters\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 90 -> word: birthday\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: years years years sisters <UNK> <UNK> birthday <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: years years years sisters <UNK> <UNK> birthday <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  81  81  83  83  84  81  84 107]\n",
      "Token 81 -> word: brothers\n",
      "Token 81 -> word: brothers\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 84 -> word: sister\n",
      "Token 81 -> word: brothers\n",
      "Token 84 -> word: sister\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: brothers brothers sisters sisters sister brothers sister <UNK>\n",
      "Input: bonjour\n",
      "Output: brothers brothers sisters sisters sister brothers sister <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 7/300 - loss: 2.0993 -  - 0.12s\n",
      "[==============================] 100% Epoch 8/300 - loss: 2.0207 -  - 0.13s\n",
      "[==============================] 100% Epoch 9/300 - loss: 2.1954 -  - 0.12s\n",
      "[==============================] 100% Epoch 10/300 - loss: 2.1649 -  - 0.12s\n",
      "[==============================] 100% Epoch 11/300 - loss: 2.1498 -  - 0.12s\n",
      "Epoch 10 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  78  78  83  83  92  87  84 107]\n",
      "Token 78 -> word: -\n",
      "Token 78 -> word: -\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 92 -> word: <UNK>\n",
      "Token 87 -> word: student\n",
      "Token 84 -> word: sister\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: - - sisters sisters <UNK> student sister <UNK>\n",
      "Input: je vais bien\n",
      "Output: - - sisters sisters <UNK> student sister <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  79  80  78  80  91  84  86 107]\n",
      "Token 79 -> word: five\n",
      "Token 80 -> word: years\n",
      "Token 78 -> word: -\n",
      "Token 80 -> word: years\n",
      "Token 91 -> word: congratulations\n",
      "Token 84 -> word: sister\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: five years - years congratulations sister travel <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: five years - years congratulations sister travel <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  80  78  81  82  74  81  83 107]\n",
      "Token 80 -> word: years\n",
      "Token 78 -> word: -\n",
      "Token 81 -> word: brothers\n",
      "Token 82 -> word: and\n",
      "Token 74 -> word: here\n",
      "Token 81 -> word: brothers\n",
      "Token 83 -> word: sisters\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: years - brothers and here brothers sisters <UNK>\n",
      "Input: bonjour\n",
      "Output: years - brothers and here brothers sisters <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 12/300 - loss: 2.1282 -  - 0.12s\n",
      "[==============================] 100% Epoch 13/300 - loss: 1.8797 -  - 0.12s\n",
      "[==============================] 100% Epoch 14/300 - loss: 1.8905 -  - 0.12s\n",
      "[==============================] 100% Epoch 15/300 - loss: 1.8993 -  - 0.12s\n",
      "[==============================] 100% Epoch 16/300 - loss: 1.8864 -  - 0.13s\n",
      "Epoch 15 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  77  76  83  83  92  92  93 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 76 -> word: paris\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 93 -> word: <UNK>\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty paris sisters sisters <UNK> <UNK> <UNK> <UNK>\n",
      "Input: je vais bien\n",
      "Output: twenty paris sisters sisters <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  77  80  79  84  92  90  92 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 80 -> word: years\n",
      "Token 79 -> word: five\n",
      "Token 84 -> word: sister\n",
      "Token 92 -> word: <UNK>\n",
      "Token 90 -> word: birthday\n",
      "Token 92 -> word: <UNK>\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty years five sister <UNK> birthday <UNK> <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: twenty years five sister <UNK> birthday <UNK> <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  77  76  83  75  78  75  77 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 76 -> word: paris\n",
      "Token 83 -> word: sisters\n",
      "Token 75 -> word: in\n",
      "Token 78 -> word: -\n",
      "Token 75 -> word: in\n",
      "Token 77 -> word: twenty\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty paris sisters in - in twenty <UNK>\n",
      "Input: bonjour\n",
      "Output: twenty paris sisters in - in twenty <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 17/300 - loss: 1.8592 -  - 0.11s\n",
      "[==============================] 100% Epoch 18/300 - loss: 1.8360 -  - 0.11s\n",
      "[==============================] 100% Epoch 19/300 - loss: 1.8331 -  - 0.13s\n",
      "[==============================] 100% Epoch 20/300 - loss: 1.8346 -  - 0.11s\n",
      "[==============================] 100% Epoch 21/300 - loss: 1.8397 -  - 0.11s\n",
      "Epoch 20 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  72  72  77  81  93  81  85 107]\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 77 -> word: twenty\n",
      "Token 81 -> word: brothers\n",
      "Token 93 -> word: <UNK>\n",
      "Token 81 -> word: brothers\n",
      "Token 85 -> word: cat\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: hot hot twenty brothers <UNK> brothers cat <UNK>\n",
      "Input: je vais bien\n",
      "Output: hot hot twenty brothers <UNK> brothers cat <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  74  81  77  86  90  84  89 107]\n",
      "Token 74 -> word: here\n",
      "Token 81 -> word: brothers\n",
      "Token 77 -> word: twenty\n",
      "Token 86 -> word: travel\n",
      "Token 90 -> word: birthday\n",
      "Token 84 -> word: sister\n",
      "Token 89 -> word: help\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: here brothers twenty travel birthday sister help <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: here brothers twenty travel birthday sister help <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  77  75  80  78  75  72  67 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 75 -> word: in\n",
      "Token 80 -> word: years\n",
      "Token 78 -> word: -\n",
      "Token 75 -> word: in\n",
      "Token 72 -> word: hot\n",
      "Token 67 -> word: thirsty\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty in years - in hot thirsty <UNK>\n",
      "Input: bonjour\n",
      "Output: twenty in years - in hot thirsty <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 22/300 - loss: 1.8495 -  - 0.13s\n",
      "[==============================] 100% Epoch 23/300 - loss: 1.8599 -  - 0.12s\n",
      "[==============================] 100% Epoch 24/300 - loss: 1.4806 -  - 0.12s\n",
      "[==============================] 100% Epoch 25/300 - loss: 1.4700 -  - 0.12s\n",
      "[==============================] 100% Epoch 26/300 - loss: 1.2052 -  - 0.12s\n",
      "Epoch 25 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  67  78  77  81  85  81  74 107]\n",
      "Token 67 -> word: thirsty\n",
      "Token 78 -> word: -\n",
      "Token 77 -> word: twenty\n",
      "Token 81 -> word: brothers\n",
      "Token 85 -> word: cat\n",
      "Token 81 -> word: brothers\n",
      "Token 74 -> word: here\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: thirsty - twenty brothers cat brothers here <UNK>\n",
      "Input: je vais bien\n",
      "Output: thirsty - twenty brothers cat brothers here <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  71  81  72  81  85  81  81 107]\n",
      "Token 71 -> word: cold\n",
      "Token 81 -> word: brothers\n",
      "Token 72 -> word: hot\n",
      "Token 81 -> word: brothers\n",
      "Token 85 -> word: cat\n",
      "Token 81 -> word: brothers\n",
      "Token 81 -> word: brothers\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cold brothers hot brothers cat brothers brothers <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: cold brothers hot brothers cat brothers brothers <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  74  78  78  78  72  66  68 107]\n",
      "Token 74 -> word: here\n",
      "Token 78 -> word: -\n",
      "Token 78 -> word: -\n",
      "Token 78 -> word: -\n",
      "Token 72 -> word: hot\n",
      "Token 66 -> word: hungry\n",
      "Token 68 -> word: weather\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: here - - - hot hungry weather <UNK>\n",
      "Input: bonjour\n",
      "Output: here - - - hot hungry weather <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 27/300 - loss: 1.1962 -  - 0.13s\n",
      "[==============================] 100% Epoch 28/300 - loss: 1.1982 -  - 0.12s\n",
      "[==============================] 100% Epoch 29/300 - loss: 1.1987 -  - 0.12s\n",
      "[==============================] 100% Epoch 30/300 - loss: 1.1938 -  - 0.14s\n",
      "[==============================] 100% Epoch 31/300 - loss: 1.1916 -  - 0.12s\n",
      "Epoch 30 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  67  78  74  81  81  63  58 107]\n",
      "Token 67 -> word: thirsty\n",
      "Token 78 -> word: -\n",
      "Token 74 -> word: here\n",
      "Token 81 -> word: brothers\n",
      "Token 81 -> word: brothers\n",
      "Token 63 -> word: enjoy\n",
      "Token 58 -> word: can\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: thirsty - here brothers brothers enjoy can <UNK>\n",
      "Input: je vais bien\n",
      "Output: thirsty - here brothers brothers enjoy can <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  55  80  66  81  80  74  71 107]\n",
      "Token 55 -> word: english\n",
      "Token 80 -> word: years\n",
      "Token 66 -> word: hungry\n",
      "Token 81 -> word: brothers\n",
      "Token 80 -> word: years\n",
      "Token 74 -> word: here\n",
      "Token 71 -> word: cold\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: english years hungry brothers years here cold <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: english years hungry brothers years here cold <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  72  74  81  72  68  66  66 107]\n",
      "Token 72 -> word: hot\n",
      "Token 74 -> word: here\n",
      "Token 81 -> word: brothers\n",
      "Token 72 -> word: hot\n",
      "Token 68 -> word: weather\n",
      "Token 66 -> word: hungry\n",
      "Token 66 -> word: hungry\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: hot here brothers hot weather hungry hungry <UNK>\n",
      "Input: bonjour\n",
      "Output: hot here brothers hot weather hungry hungry <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 32/300 - loss: 1.1922 -  - 0.12s\n",
      "[==============================] 100% Epoch 33/300 - loss: 1.1888 -  - 0.14s\n",
      "[==============================] 100% Epoch 34/300 - loss: 1.1837 -  - 0.12s\n",
      "[==============================] 100% Epoch 35/300 - loss: 1.1898 -  - 0.12s\n",
      "[==============================] 100% Epoch 36/300 - loss: 1.1974 -  - 0.12s\n",
      "Epoch 35 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  56  68  68  66  63  51  48 107]\n",
      "Token 56 -> word: little\n",
      "Token 68 -> word: weather\n",
      "Token 68 -> word: weather\n",
      "Token 66 -> word: hungry\n",
      "Token 63 -> word: enjoy\n",
      "Token 51 -> word: nearby\n",
      "Token 48 -> word: o'\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: little weather weather hungry enjoy nearby o' <UNK>\n",
      "Input: je vais bien\n",
      "Output: little weather weather hungry enjoy nearby o' <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  36  68  62  63  74  56  46 107]\n",
      "Token 36 -> word: my\n",
      "Token 68 -> word: weather\n",
      "Token 62 -> word: problem\n",
      "Token 63 -> word: enjoy\n",
      "Token 74 -> word: here\n",
      "Token 56 -> word: little\n",
      "Token 46 -> word: time\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: my weather problem enjoy here little time <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: my weather problem enjoy here little time <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  68  72  76  72  66  63  63 107]\n",
      "Token 68 -> word: weather\n",
      "Token 72 -> word: hot\n",
      "Token 76 -> word: paris\n",
      "Token 72 -> word: hot\n",
      "Token 66 -> word: hungry\n",
      "Token 63 -> word: enjoy\n",
      "Token 63 -> word: enjoy\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: weather hot paris hot hungry enjoy enjoy <UNK>\n",
      "Input: bonjour\n",
      "Output: weather hot paris hot hungry enjoy enjoy <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 37/300 - loss: 1.1966 -  - 0.12s\n",
      "[==============================] 100% Epoch 38/300 - loss: 1.1952 -  - 0.12s\n",
      "[==============================] 100% Epoch 39/300 - loss: 1.1857 -  - 0.12s\n",
      "[==============================] 100% Epoch 40/300 - loss: 1.1851 -  - 0.13s\n",
      "[==============================] 100% Epoch 41/300 - loss: 1.1885 -  - 0.12s\n",
      "Epoch 40 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  31  36  36  36  43  18   1 107]\n",
      "Token 31 -> word: thank\n",
      "Token 36 -> word: my\n",
      "Token 36 -> word: my\n",
      "Token 36 -> word: my\n",
      "Token 43 -> word: tomorrow\n",
      "Token 18 -> word: are\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: thank my my my tomorrow are . <UNK>\n",
      "Input: je vais bien\n",
      "Output: thank my my my tomorrow are . <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   3  36  31  33  36  13   3 107]\n",
      "Token 3 -> word: ?\n",
      "Token 36 -> word: my\n",
      "Token 31 -> word: thank\n",
      "Token 33 -> word: please\n",
      "Token 36 -> word: my\n",
      "Token 13 -> word: it\n",
      "Token 3 -> word: ?\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: ? my thank please my it ? <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: ? my thank please my it ? <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  46  46  59  36  51  36  55 107]\n",
      "Token 46 -> word: time\n",
      "Token 46 -> word: time\n",
      "Token 59 -> word: repeat\n",
      "Token 36 -> word: my\n",
      "Token 51 -> word: nearby\n",
      "Token 36 -> word: my\n",
      "Token 55 -> word: english\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: time time repeat my nearby my english <UNK>\n",
      "Input: bonjour\n",
      "Output: time time repeat my nearby my english <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 42/300 - loss: 1.2003 -  - 0.12s\n",
      "[==============================] 100% Epoch 43/300 - loss: 1.2026 -  - 0.12s\n",
      "[==============================] 100% Epoch 44/300 - loss: 1.2012 -  - 0.11s\n",
      "[==============================] 100% Epoch 45/300 - loss: 1.1945 -  - 0.11s\n",
      "[==============================] 100% Epoch 46/300 - loss: 1.1924 -  - 0.11s\n",
      "Epoch 45 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: je vais bien\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: bonjour\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 47/300 - loss: 1.1978 -  - 0.12s\n",
      "[==============================] 100% Epoch 48/300 - loss: 1.1978 -  - 0.11s\n",
      "[==============================] 100% Epoch 49/300 - loss: 1.1974 -  - 0.12s\n",
      "[==============================] 100% Epoch 50/300 - loss: 1.1997 -  - 0.12s\n",
      "[==============================] 100% Epoch 51/300 - loss: 1.2061 -  - 0.12s\n",
      "Epoch 50 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: je vais bien\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: bonjour\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 52/300 - loss: 1.2007 -  - 0.12s\n",
      "[==============================] 100% Epoch 53/300 - loss: 1.1961 -  - 0.12s\n",
      "[==============================] 100% Epoch 54/300 - loss: 1.2035 -  - 0.12s\n",
      "Early stopping triggered after epoch 54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monitor = TrainingMonitor(model, fr_tokenizer, en_tokenizer, test_sentences)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=300,\n",
    "    batch_size=2,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss', patience=20),\n",
    "        monitor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes:\n",
      "French vocab size: 107\n",
      "English vocab size: 95\n",
      "\n",
      "==================================================\n",
      "Testing: je vais bien\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Translation: . . . . . . . <UNK>\n",
      "\n",
      "==================================================\n",
      "Testing: comment allez-vous ?\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Translation: . . . . . . . <UNK>\n",
      "\n",
      "==================================================\n",
      "Testing: bonjour\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Translation: . . . . . . . <UNK>\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary sizes:\")\n",
    "print(f\"French vocab size: {len(fr_tokenizer.word_index)}\")\n",
    "print(f\"English vocab size: {len(en_tokenizer.word_index)}\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer, \n",
    "                      temperature=0.8,\n",
    "                      beam_size=10,\n",
    "                      min_length=2) \n",
    "    print(f\"Translation: {translation}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e205b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: je vais bien\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 2, 7, 8)\n",
      "First attention head values:\n",
      "[[0.548126   0.05809443 0.10045471 0.25164407 0.04168081 0.\n",
      "  0.         0.        ]\n",
      " [0.06540107 0.7977359  0.05699982 0.03561854 0.04424464 0.\n",
      "  0.         0.        ]\n",
      " [0.14340894 0.6132722  0.11157043 0.09929729 0.03245115 0.\n",
      "  0.         0.        ]\n",
      " [0.05271855 0.78609735 0.08468036 0.03814641 0.03835733 0.\n",
      "  0.         0.        ]\n",
      " [0.07826998 0.67554224 0.12110294 0.0294253  0.09565954 0.\n",
      "  0.         0.        ]\n",
      " [0.12984963 0.22510172 0.2395956  0.12365758 0.28179547 0.\n",
      "  0.         0.        ]\n",
      " [0.12025411 0.0256971  0.2389602  0.4139277  0.20116088 0.\n",
      "  0.         0.        ]]\n",
      "\n",
      "Analyzing: comment allez-vous ?\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 2, 7, 8)\n",
      "First attention head values:\n",
      "[[0.43360105 0.01540476 0.17140672 0.23671012 0.14287734 0.\n",
      "  0.         0.        ]\n",
      " [0.05159843 0.8232056  0.04667832 0.03720008 0.04131755 0.\n",
      "  0.         0.        ]\n",
      " [0.14063375 0.6759063  0.08416782 0.06457122 0.03472093 0.\n",
      "  0.         0.        ]\n",
      " [0.04780597 0.8232178  0.05100958 0.03971388 0.03825273 0.\n",
      "  0.         0.        ]\n",
      " [0.04571097 0.8089602  0.05473425 0.03814019 0.05245442 0.\n",
      "  0.         0.        ]\n",
      " [0.08721442 0.20564209 0.2096215  0.25468314 0.24283887 0.\n",
      "  0.         0.        ]\n",
      " [0.16587268 0.01543003 0.25221282 0.3283033  0.23818117 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def analyze_attention_weights(model, input_sentence, fr_tokenizer):\n",
    "    tokens = fr_tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    \n",
    "    _ = model.predict(padded)\n",
    "    \n",
    "    attention_weights = model.decoder_layers[-1].cross_attention.attention_weights\n",
    "    \n",
    "    print(\"\\nAttention Analysis:\")\n",
    "    print(f\"Attention shape: {attention_weights.shape}\")\n",
    "    print(\"First attention head values:\")\n",
    "    print(attention_weights[0, 0])\n",
    "\n",
    "for sent in test_sentences[:2]:\n",
    "    print(f\"\\nAnalyzing: {sent}\")\n",
    "    analyze_attention_weights(model, sent, fr_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
