{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.losses import SequenceCrossEntropy\n",
    "from neuralnetlib.optimizers import Adam\n",
    "from neuralnetlib.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcb1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer, \n",
    "             temperature=0.8, beam_size=10, min_length=2) -> str:\n",
    "    print(f\"\\nProcessing: {sentence}\")\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Original words: {[fr_tokenizer.index_word.get(t, '<UNK>') for t in tokens]}\")\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    print(f\"With special tokens: {tokens}\")\n",
    "    \n",
    "    # Padding\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    print(f\"Padded sequence: {padded}\")\n",
    "    \n",
    "    pred = model.predict(padded, \n",
    "                        max_length=model.max_sequence_length,\n",
    "                        beam_size=beam_size,      \n",
    "                        alpha=0.6,         \n",
    "                        min_length=min_length,\n",
    "                        temperature=temperature)[0]\n",
    "    \n",
    "    print(f\"Raw prediction: {pred}\")\n",
    "    \n",
    "    words = []\n",
    "    for idx in pred[1:]:  # Skip SOS\n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.eos_token]:\n",
    "            print(f\"Found EOS token: {idx}\")\n",
    "            break\n",
    "            \n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.pad_token]:\n",
    "            print(f\"Skipping PAD token: {idx}\")\n",
    "            continue\n",
    "            \n",
    "        word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "        print(f\"Token {idx} -> word: {word}\")\n",
    "        words.append(word)\n",
    "    \n",
    "    if not words:\n",
    "        print(\"Warning: Empty translation, using default handling...\")\n",
    "        for idx in pred[1:]:  # Skip SOS\n",
    "            if idx not in [en_tokenizer.word_index[token] for token in \n",
    "                         [en_tokenizer.pad_token, en_tokenizer.eos_token]]:\n",
    "                word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "                words.append(word)\n",
    "    \n",
    "    translation = \" \".join(words) if words else \"[Translation failed]\"\n",
    "    print(f\"Final translation: {translation}\")\n",
    "    return translation\n",
    "\n",
    "class TrainingMonitor(Callback):\n",
    "    def __init__(self, model, fr_tokenizer, en_tokenizer, test_sentences):\n",
    "        self.model = model\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.test_sentences = test_sentences\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:  # Check every 5 epochs\n",
    "            print(f\"\\nEpoch {epoch} validation:\")\n",
    "            for sent in self.test_sentences:\n",
    "                translation = translate(\n",
    "                    sent, self.model, self.fr_tokenizer, self.en_tokenizer,\n",
    "                    temperature=0.8, beam_size=5, min_length=2\n",
    "                )\n",
    "                print(f\"Input: {sent}\")\n",
    "                print(f\"Output: {translation}\\n\")\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"je vais bien\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"bonjour.\",\n",
    "    \"au revoir.\",\n",
    "    \"merci beaucoup.\",\n",
    "    \"s'il vous plaît.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"je vais bien.\",\n",
    "    \"je suis fatigué.\",\n",
    "    \"je suis content.\",\n",
    "    \"quel est votre nom ?\",\n",
    "    \"mon nom est Jean.\",\n",
    "    \"enchanté de vous rencontrer.\",\n",
    "    \"bonne journée.\",\n",
    "    \"bonne soirée.\",\n",
    "    \"à demain.\",\n",
    "    \"j'aime le café.\",\n",
    "    \"je n'aime pas le thé.\",\n",
    "    \"quelle heure est-il ?\",\n",
    "    \"il est trois heures.\",\n",
    "    \"où est la gare ?\",\n",
    "    \"la gare est près d'ici.\",\n",
    "    \"combien ça coûte ?\",\n",
    "    \"c'est trop cher.\",\n",
    "    \"parlez-vous anglais ?\",\n",
    "    \"un peu.\",\n",
    "    \"je ne comprends pas.\",\n",
    "    \"pouvez-vous répéter ?\",\n",
    "    \"je suis désolé.\",\n",
    "    \"pas de problème.\",\n",
    "    \"bon appétit.\",\n",
    "    \"à votre santé.\",\n",
    "    \"j'ai faim.\",\n",
    "    \"j'ai soif.\",\n",
    "    \"il fait beau aujourd'hui.\",\n",
    "    \"il pleut.\",\n",
    "    \"il fait froid.\",\n",
    "    \"il fait chaud.\",\n",
    "    \"je travaille ici.\",\n",
    "    \"où habitez-vous ?\",\n",
    "    \"j'habite à Paris.\",\n",
    "    \"quel âge avez-vous ?\",\n",
    "    \"j'ai vingt-cinq ans.\",\n",
    "    \"avez-vous des frères et sœurs ?\",\n",
    "    \"j'ai une sœur.\",\n",
    "    \"j'ai un chat.\",\n",
    "    \"j'aime voyager.\",\n",
    "    \"je suis étudiant.\",\n",
    "    \"je suis professeur.\",\n",
    "    \"au secours !\",\n",
    "    \"joyeux anniversaire !\",\n",
    "    \"félicitations !\"\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"hello.\",\n",
    "    \"goodbye.\",\n",
    "    \"thank you very much.\",\n",
    "    \"please.\",\n",
    "    \"how are you?\",\n",
    "    \"i am fine.\",\n",
    "    \"i am tired.\",\n",
    "    \"i am happy.\",\n",
    "    \"what is your name?\",\n",
    "    \"my name is John.\",\n",
    "    \"nice to meet you.\",\n",
    "    \"have a nice day.\",\n",
    "    \"have a good evening.\",\n",
    "    \"see you tomorrow.\",\n",
    "    \"i like coffee.\",\n",
    "    \"i don't like tea.\",\n",
    "    \"what time is it?\",\n",
    "    \"it is three o'clock.\",\n",
    "    \"where is the train station?\",\n",
    "    \"the station is nearby.\",\n",
    "    \"how much is it?\",\n",
    "    \"it's too expensive.\",\n",
    "    \"do you speak english?\",\n",
    "    \"a little.\",\n",
    "    \"i don't understand.\",\n",
    "    \"can you repeat?\",\n",
    "    \"i am sorry.\",\n",
    "    \"no problem.\",\n",
    "    \"enjoy your meal.\",\n",
    "    \"cheers.\",\n",
    "    \"i am hungry.\",\n",
    "    \"i am thirsty.\",\n",
    "    \"the weather is nice today.\",\n",
    "    \"it's raining.\",\n",
    "    \"it's cold.\",\n",
    "    \"it's hot.\",\n",
    "    \"i work here.\",\n",
    "    \"where do you live?\",\n",
    "    \"i live in Paris.\",\n",
    "    \"how old are you?\",\n",
    "    \"i am twenty-five years old.\",\n",
    "    \"do you have brothers and sisters?\",\n",
    "    \"i have a sister.\",\n",
    "    \"i have a cat.\",\n",
    "    \"i like to travel.\",\n",
    "    \"i am a student.\",\n",
    "    \"i am a teacher.\",\n",
    "    \"help!\",\n",
    "    \"happy birthday!\",\n",
    "    \"congratulations!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "en_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 95, vocab_size_fr: 107\n",
      "max_len_x: 8, max_len_y: 8, max_vocab_size: 107, max_seq_len: 8\n",
      "French sentences:\n",
      "['bonjour.', 'au revoir.', 'merci beaucoup.', \"s'il vous plaît.\", 'comment allez-vous ?', 'je vais bien.', 'je suis fatigué.', 'je suis content.', 'quel est votre nom ?', 'mon nom est Jean.', 'enchanté de vous rencontrer.', 'bonne journée.', 'bonne soirée.', 'à demain.', \"j'aime le café.\", \"je n'aime pas le thé.\", 'quelle heure est-il ?', 'il est trois heures.', 'où est la gare ?', \"la gare est près d'ici.\", 'combien ça coûte ?', \"c'est trop cher.\", 'parlez-vous anglais ?', 'un peu.', 'je ne comprends pas.', 'pouvez-vous répéter ?', 'je suis désolé.', 'pas de problème.', 'bon appétit.', 'à votre santé.', \"j'ai faim.\", \"j'ai soif.\", \"il fait beau aujourd'hui.\", 'il pleut.', 'il fait froid.', 'il fait chaud.', 'je travaille ici.', 'où habitez-vous ?', \"j'habite à Paris.\", 'quel âge avez-vous ?', \"j'ai vingt-cinq ans.\", 'avez-vous des frères et sœurs ?', \"j'ai une sœur.\", \"j'ai un chat.\", \"j'aime voyager.\", 'je suis étudiant.', 'je suis professeur.', 'au secours !', 'joyeux anniversaire !', 'félicitations !']\n",
      "English sentences:\n",
      "['hello.', 'goodbye.', 'thank you very much.', 'please.', 'how are you?', 'i am fine.', 'i am tired.', 'i am happy.', 'what is your name?', 'my name is John.', 'nice to meet you.', 'have a nice day.', 'have a good evening.', 'see you tomorrow.', 'i like coffee.', \"i don't like tea.\", 'what time is it?', \"it is three o'clock.\", 'where is the train station?', 'the station is nearby.', 'how much is it?', \"it's too expensive.\", 'do you speak english?', 'a little.', \"i don't understand.\", 'can you repeat?', 'i am sorry.', 'no problem.', 'enjoy your meal.', 'cheers.', 'i am hungry.', 'i am thirsty.', 'the weather is nice today.', \"it's raining.\", \"it's cold.\", \"it's hot.\", 'i work here.', 'where do you live?', 'i live in Paris.', 'how old are you?', 'i am twenty-five years old.', 'do you have brothers and sisters?', 'i have a sister.', 'i have a cat.', 'i like to travel.', 'i am a student.', 'i am a teacher.', 'help!', 'happy birthday!', 'congratulations!']\n",
      "French tokenizer:\n",
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, '.': 4, '?': 5, 'je': 6, 'vous': 7, '-': 8, \"j'\": 9, 'il': 10, 'est': 11, 'suis': 12, 'ai': 13, 'à': 14, 'aime': 15, 'pas': 16, 'fait': 17, '!': 18, 'au': 19, 'quel': 20, 'votre': 21, 'nom': 22, 'de': 23, 'bonne': 24, 'le': 25, 'où': 26, 'la': 27, 'gare': 28, 'ici': 29, 'un': 30, 'avez': 31, 'bonjour': 32, 'revoir': 33, 'merci': 34, 'beaucoup': 35, \"s'\": 36, 'plaît': 37, 'comment': 38, 'allez': 39, 'vais': 40, 'bien': 41, 'fatigué': 42, 'content': 43, 'mon': 44, 'jean': 45, 'enchanté': 46, 'rencontrer': 47, 'journée': 48, 'soirée': 49, 'demain': 50, 'café': 51, \"n'\": 52, 'thé': 53, 'quelle': 54, 'heure': 55, 'trois': 56, 'heures': 57, 'près': 58, \"d'\": 59, 'combien': 60, 'ça': 61, 'coûte': 62, \"c'\": 63, 'trop': 64, 'cher': 65, 'parlez': 66, 'anglais': 67, 'peu': 68, 'ne': 69, 'comprends': 70, 'pouvez': 71, 'répéter': 72, 'désolé': 73, 'problème': 74, 'bon': 75, 'appétit': 76, 'santé': 77, 'faim': 78, 'soif': 79, 'beau': 80, \"aujourd'hui\": 81, 'pleut': 82, 'froid': 83, 'chaud': 84, 'travaille': 85, 'habitez': 86, 'habite': 87, 'paris': 88, 'âge': 89, 'vingt': 90, 'cinq': 91, 'ans': 92, 'des': 93, 'frères': 94, 'et': 95, 'sœurs': 96, 'une': 97, 'sœur': 98, 'chat': 99, 'voyager': 100, 'étudiant': 101, 'professeur': 102, 'secours': 103, 'joyeux': 104, 'anniversaire': 105, 'félicitations': 106}\n",
      "English tokenizer:\n",
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, '.': 4, 'i': 5, '?': 6, 'you': 7, 'am': 8, 'is': 9, 'a': 10, 'have': 11, \"it's\": 12, 'how': 13, 'nice': 14, 'like': 15, 'it': 16, 'the': 17, 'do': 18, '!': 19, 'much': 20, 'are': 21, 'happy': 22, 'what': 23, 'your': 24, 'name': 25, 'to': 26, \"don't\": 27, 'where': 28, 'station': 29, 'live': 30, 'old': 31, 'hello': 32, 'goodbye': 33, 'thank': 34, 'very': 35, 'please': 36, 'fine': 37, 'tired': 38, 'my': 39, 'john': 40, 'meet': 41, 'day': 42, 'good': 43, 'evening': 44, 'see': 45, 'tomorrow': 46, 'coffee': 47, 'tea': 48, 'time': 49, 'three': 50, \"o'\": 51, 'clock': 52, 'train': 53, 'nearby': 54, 'too': 55, 'expensive': 56, 'speak': 57, 'english': 58, 'little': 59, 'understand': 60, 'can': 61, 'repeat': 62, 'sorry': 63, 'no': 64, 'problem': 65, 'enjoy': 66, 'meal': 67, 'cheers': 68, 'hungry': 69, 'thirsty': 70, 'weather': 71, 'today': 72, 'raining': 73, 'cold': 74, 'hot': 75, 'work': 76, 'here': 77, 'in': 78, 'paris': 79, 'twenty': 80, '-': 81, 'five': 82, 'years': 83, 'brothers': 84, 'and': 85, 'sisters': 86, 'sister': 87, 'cat': 88, 'travel': 89, 'student': 90, 'teacher': 91, 'help': 92, 'birthday': 93, 'congratulations': 94}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  src_vocab_size=107,\n",
      "  tgt_vocab_size=95,\n",
      "  d_model=256,\n",
      "  n_heads=8,\n",
      "  n_encoder_layers=4,\n",
      "  n_decoder_layers=4,\n",
      "  d_ff=1024,\n",
      "  dropout_rate=0.3,\n",
      "  max_sequence_length=8\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size_fr,\n",
    "    tgt_vocab_size=vocab_size_en,\n",
    "    d_model=256,        \n",
    "    n_heads=8,         \n",
    "    n_encoder_layers=4,\n",
    "    n_decoder_layers=4,\n",
    "    d_ff=1024,           \n",
    "    dropout_rate=0.3,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function=SequenceCrossEntropy(\n",
    "        label_smoothing=0.1,\n",
    "    ),\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9\n",
    "    ),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527483e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting training data:\n",
      "\n",
      "Example 1:\n",
      "Input sequence:\n",
      "Raw: [ 2 32  4  3  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'bonjour', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 2 32  4  3  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'hello', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 2:\n",
      "Input sequence:\n",
      "Raw: [ 2 19 33  4  3  0  0  0]\n",
      "Tokens: ['<SOS>', 'au', 'revoir', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 2 33  4  3  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'goodbye', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 3:\n",
      "Input sequence:\n",
      "Raw: [ 2 34 35  4  3  0  0  0]\n",
      "Tokens: ['<SOS>', 'merci', 'beaucoup', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 2 34  7 35 20  4  3  0]\n",
      "Tokens: ['<SOS>', 'thank', 'you', 'very', 'much', '.', '<EOS>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "def inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer):\n",
    "    print(\"\\nInspecting training data:\")\n",
    "    for i in range(min(3, len(x_train_padded))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"Input sequence:\")\n",
    "        print(f\"Raw: {x_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [fr_tokenizer.index_word.get(idx, '<UNK>') for idx in x_train_padded[i]])\n",
    "        \n",
    "        print(\"\\nOutput sequence:\")\n",
    "        print(f\"Raw: {y_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [en_tokenizer.index_word.get(idx, '<UNK>') for idx in y_train_padded[i]])\n",
    "\n",
    "inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/20 - loss: 12.0892 -  - 1.77s\n",
      "Epoch 0 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 62 62 62 62 62 62 62  3]\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Found EOS token: 3\n",
      "Final translation: repeat repeat repeat repeat repeat repeat repeat\n",
      "Input: je vais bien\n",
      "Output: repeat repeat repeat repeat repeat repeat repeat\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 62 62 62 62 62 62 62  3]\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Found EOS token: 3\n",
      "Final translation: repeat repeat repeat repeat repeat repeat repeat\n",
      "Input: comment allez-vous ?\n",
      "Output: repeat repeat repeat repeat repeat repeat repeat\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 62 62 62 62 62 62 62  3]\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Token 62 -> word: repeat\n",
      "Found EOS token: 3\n",
      "Final translation: repeat repeat repeat repeat repeat repeat repeat\n",
      "Input: bonjour\n",
      "Output: repeat repeat repeat repeat repeat repeat repeat\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 2/20 - loss: 7.4340 -  - 1.70s\n",
      "[==============================] 100% Epoch 3/20 - loss: 5.0437 -  - 1.71s\n",
      "[==============================] 100% Epoch 4/20 - loss: 3.4804 -  - 1.68s\n",
      "[==============================] 100% Epoch 5/20 - loss: 2.3147 -  - 1.64s\n",
      "[==============================] 100% Epoch 6/20 - loss: 1.7115 -  - 1.67s\n",
      "Epoch 5 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 75 75 75 75 75 75 75  3]\n",
      "Token 75 -> word: hot\n",
      "Token 75 -> word: hot\n",
      "Token 75 -> word: hot\n",
      "Token 75 -> word: hot\n",
      "Token 75 -> word: hot\n",
      "Token 75 -> word: hot\n",
      "Token 75 -> word: hot\n",
      "Found EOS token: 3\n",
      "Final translation: hot hot hot hot hot hot hot\n",
      "Input: je vais bien\n",
      "Output: hot hot hot hot hot hot hot\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 76 76 76 76 76 76 76  3]\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: work work work work work work work\n",
      "Input: comment allez-vous ?\n",
      "Output: work work work work work work work\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 76 76 76 76 76 76 76  3]\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: work work work work work work work\n",
      "Input: bonjour\n",
      "Output: work work work work work work work\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 7/20 - loss: 1.0517 -  - 1.64s\n",
      "[==============================] 100% Epoch 8/20 - loss: 0.5203 -  - 1.66s\n",
      "[==============================] 100% Epoch 9/20 - loss: 0.2869 -  - 1.65s\n",
      "[==============================] 100% Epoch 10/20 - loss: 0.2225 -  - 1.70s\n",
      "[==============================] 100% Epoch 11/20 - loss: 0.1241 -  - 1.73s\n",
      "Epoch 10 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 71 71 71 71 71 71 71  3]\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Found EOS token: 3\n",
      "Final translation: weather weather weather weather weather weather weather\n",
      "Input: je vais bien\n",
      "Output: weather weather weather weather weather weather weather\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 71 71 71 71 71 71 71  3]\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Found EOS token: 3\n",
      "Final translation: weather weather weather weather weather weather weather\n",
      "Input: comment allez-vous ?\n",
      "Output: weather weather weather weather weather weather weather\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 71 71 71 71 71 71 71  3]\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Token 71 -> word: weather\n",
      "Found EOS token: 3\n",
      "Final translation: weather weather weather weather weather weather weather\n",
      "Input: bonjour\n",
      "Output: weather weather weather weather weather weather weather\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 12/20 - loss: 0.1034 -  - 1.72s\n",
      "[==============================] 100% Epoch 13/20 - loss: 0.0851 -  - 1.70s\n",
      "[==============================] 100% Epoch 14/20 - loss: 0.0713 -  - 1.68s\n",
      "[==============================] 100% Epoch 15/20 - loss: 0.0615 -  - 1.86s\n",
      "[==============================] 100% Epoch 16/20 - loss: 0.0576 -  - 1.76s\n",
      "Epoch 15 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 72 72 72 72 72 72 72  3]\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Found EOS token: 3\n",
      "Final translation: today today today today today today today\n",
      "Input: je vais bien\n",
      "Output: today today today today today today today\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 72 72 72 72 72 72 72  3]\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Found EOS token: 3\n",
      "Final translation: today today today today today today today\n",
      "Input: comment allez-vous ?\n",
      "Output: today today today today today today today\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 72 72 72 72 72 72 72  3]\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Found EOS token: 3\n",
      "Final translation: today today today today today today today\n",
      "Input: bonjour\n",
      "Output: today today today today today today today\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 17/20 - loss: 0.0561 -  - 1.87s\n",
      "[==============================] 100% Epoch 18/20 - loss: 0.0554 -  - 1.88s\n",
      "[==============================] 100% Epoch 19/20 - loss: 0.0518 -  - 1.88s\n",
      "[==============================] 100% Epoch 20/20 - loss: 0.0480 -  - 1.65s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monitor = TrainingMonitor(model, fr_tokenizer, en_tokenizer, test_sentences)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=20,\n",
    "    batch_size=12,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss', patience=20),\n",
    "        monitor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes:\n",
      "French vocab size: 107\n",
      "English vocab size: 95\n",
      "\n",
      "==================================================\n",
      "Testing: je vais bien\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 72 72 72 72 72 72 72  3]\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Found EOS token: 3\n",
      "Final translation: today today today today today today today\n",
      "Translation: today today today today today today today\n",
      "\n",
      "==================================================\n",
      "Testing: comment allez-vous ?\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 72 72 72 72 72 72 72  3]\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Found EOS token: 3\n",
      "Final translation: today today today today today today today\n",
      "Translation: today today today today today today today\n",
      "\n",
      "==================================================\n",
      "Testing: bonjour\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 72 72 72 72 72 72 72  3]\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Token 72 -> word: today\n",
      "Found EOS token: 3\n",
      "Final translation: today today today today today today today\n",
      "Translation: today today today today today today today\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary sizes:\")\n",
    "print(f\"French vocab size: {len(fr_tokenizer.word_index)}\")\n",
    "print(f\"English vocab size: {len(en_tokenizer.word_index)}\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer, \n",
    "                      temperature=0.8,\n",
    "                      beam_size=10,\n",
    "                      min_length=2) \n",
    "    print(f\"Translation: {translation}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e205b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: je vais bien\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 8, 7, 8)\n",
      "First attention head values:\n",
      "[[0.19997799 0.19998388 0.20006888 0.1999819  0.19998734 0.\n",
      "  0.         0.        ]\n",
      " [0.19997799 0.19998388 0.20006888 0.1999819  0.19998734 0.\n",
      "  0.         0.        ]\n",
      " [0.19997799 0.19998388 0.20006888 0.1999819  0.19998734 0.\n",
      "  0.         0.        ]\n",
      " [0.19997799 0.19998388 0.20006888 0.1999819  0.19998734 0.\n",
      "  0.         0.        ]\n",
      " [0.19997799 0.19998388 0.20006888 0.1999819  0.19998734 0.\n",
      "  0.         0.        ]\n",
      " [0.19997799 0.19998388 0.20006888 0.1999819  0.19998734 0.\n",
      "  0.         0.        ]\n",
      " [0.19997799 0.19998389 0.20006888 0.1999819  0.19998734 0.\n",
      "  0.         0.        ]]\n",
      "\n",
      "Analyzing: comment allez-vous ?\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 8, 7, 8)\n",
      "First attention head values:\n",
      "[[0.19998052 0.19998315 0.2000699  0.199978   0.19998842 0.\n",
      "  0.         0.        ]\n",
      " [0.19998052 0.19998315 0.2000699  0.199978   0.19998842 0.\n",
      "  0.         0.        ]\n",
      " [0.19998052 0.19998315 0.2000699  0.199978   0.19998843 0.\n",
      "  0.         0.        ]\n",
      " [0.19998052 0.19998315 0.2000699  0.199978   0.19998843 0.\n",
      "  0.         0.        ]\n",
      " [0.19998052 0.19998315 0.2000699  0.199978   0.19998843 0.\n",
      "  0.         0.        ]\n",
      " [0.19998052 0.19998315 0.2000699  0.199978   0.19998843 0.\n",
      "  0.         0.        ]\n",
      " [0.19998052 0.19998315 0.2000699  0.199978   0.19998843 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def analyze_attention_weights(model, input_sentence, fr_tokenizer):\n",
    "    tokens = fr_tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    \n",
    "    _ = model.predict(padded)\n",
    "    \n",
    "    attention_weights = model.decoder_layers[-1].cross_attention.attention_weights\n",
    "    \n",
    "    print(\"\\nAttention Analysis:\")\n",
    "    print(f\"Attention shape: {attention_weights.shape}\")\n",
    "    print(\"First attention head values:\")\n",
    "    print(attention_weights[0, 0])\n",
    "\n",
    "for sent in test_sentences[:2]:\n",
    "    print(f\"\\nAnalyzing: {sent}\")\n",
    "    analyze_attention_weights(model, sent, fr_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
