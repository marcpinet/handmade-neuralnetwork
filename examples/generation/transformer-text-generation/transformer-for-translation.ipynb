{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.losses import SequenceCrossEntropy\n",
    "from neuralnetlib.optimizers import Adam\n",
    "from neuralnetlib.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcb1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer, \n",
    "             temperature=0.8, beam_size=10, min_length=2) -> str:\n",
    "    print(f\"\\nProcessing: {sentence}\")\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Original words: {[fr_tokenizer.index_word.get(t, '<UNK>') for t in tokens]}\")\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    print(f\"With special tokens: {tokens}\")\n",
    "    \n",
    "    # Padding\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    print(f\"Padded sequence: {padded}\")\n",
    "    \n",
    "    pred = model.predict(padded, \n",
    "                        max_length=model.max_sequence_length,\n",
    "                        beam_size=beam_size,      \n",
    "                        alpha=0.6,         \n",
    "                        min_length=min_length,\n",
    "                        temperature=temperature)[0]\n",
    "    \n",
    "    print(f\"Raw prediction: {pred}\")\n",
    "    \n",
    "    words = []\n",
    "    for idx in pred[1:]:  # Skip SOS\n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.eos_token]:\n",
    "            print(f\"Found EOS token: {idx}\")\n",
    "            break\n",
    "            \n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.pad_token]:\n",
    "            print(f\"Skipping PAD token: {idx}\")\n",
    "            continue\n",
    "            \n",
    "        word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "        print(f\"Token {idx} -> word: {word}\")\n",
    "        words.append(word)\n",
    "    \n",
    "    if not words:\n",
    "        print(\"Warning: Empty translation, using default handling...\")\n",
    "        for idx in pred[1:]:  # Skip SOS\n",
    "            if idx not in [en_tokenizer.word_index[token] for token in \n",
    "                         [en_tokenizer.pad_token, en_tokenizer.eos_token]]:\n",
    "                word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "                words.append(word)\n",
    "    \n",
    "    translation = \" \".join(words) if words else \"[Translation failed]\"\n",
    "    print(f\"Final translation: {translation}\")\n",
    "    return translation\n",
    "\n",
    "class TrainingMonitor(Callback):\n",
    "    def __init__(self, model, fr_tokenizer, en_tokenizer, test_sentences):\n",
    "        self.model = model\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.test_sentences = test_sentences\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:  # Check every 5 epochs\n",
    "            print(f\"\\nEpoch {epoch} validation:\")\n",
    "            for sent in self.test_sentences:\n",
    "                translation = translate(\n",
    "                    sent, self.model, self.fr_tokenizer, self.en_tokenizer,\n",
    "                    temperature=0.8, beam_size=5, min_length=2\n",
    "                )\n",
    "                print(f\"Input: {sent}\")\n",
    "                print(f\"Output: {translation}\\n\")\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"je vais bien\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"bonjour.\",\n",
    "    \"au revoir.\",\n",
    "    \"merci beaucoup.\",\n",
    "    \"s'il vous plaît.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"je vais bien.\",\n",
    "    \"je suis fatigué.\",\n",
    "    \"je suis content.\",\n",
    "    \"quel est votre nom ?\",\n",
    "    \"mon nom est Jean.\",\n",
    "    \"enchanté de vous rencontrer.\",\n",
    "    \"bonne journée.\",\n",
    "    \"bonne soirée.\",\n",
    "    \"à demain.\",\n",
    "    \"j'aime le café.\",\n",
    "    \"je n'aime pas le thé.\",\n",
    "    \"quelle heure est-il ?\",\n",
    "    \"il est trois heures.\",\n",
    "    \"où est la gare ?\",\n",
    "    \"la gare est près d'ici.\",\n",
    "    \"combien ça coûte ?\",\n",
    "    \"c'est trop cher.\",\n",
    "    \"parlez-vous anglais ?\",\n",
    "    \"un peu.\",\n",
    "    \"je ne comprends pas.\",\n",
    "    \"pouvez-vous répéter ?\",\n",
    "    \"je suis désolé.\",\n",
    "    \"pas de problème.\",\n",
    "    \"bon appétit.\",\n",
    "    \"à votre santé.\",\n",
    "    \"j'ai faim.\",\n",
    "    \"j'ai soif.\",\n",
    "    \"il fait beau aujourd'hui.\",\n",
    "    \"il pleut.\",\n",
    "    \"il fait froid.\",\n",
    "    \"il fait chaud.\",\n",
    "    \"je travaille ici.\",\n",
    "    \"où habitez-vous ?\",\n",
    "    \"j'habite à Paris.\",\n",
    "    \"quel âge avez-vous ?\",\n",
    "    \"j'ai vingt-cinq ans.\",\n",
    "    \"avez-vous des frères et sœurs ?\",\n",
    "    \"j'ai une sœur.\",\n",
    "    \"j'ai un chat.\",\n",
    "    \"j'aime voyager.\",\n",
    "    \"je suis étudiant.\",\n",
    "    \"je suis professeur.\",\n",
    "    \"au secours !\",\n",
    "    \"joyeux anniversaire !\",\n",
    "    \"félicitations !\"\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"hello.\",\n",
    "    \"goodbye.\",\n",
    "    \"thank you very much.\",\n",
    "    \"please.\",\n",
    "    \"how are you?\",\n",
    "    \"i am fine.\",\n",
    "    \"i am tired.\",\n",
    "    \"i am happy.\",\n",
    "    \"what is your name?\",\n",
    "    \"my name is John.\",\n",
    "    \"nice to meet you.\",\n",
    "    \"have a nice day.\",\n",
    "    \"have a good evening.\",\n",
    "    \"see you tomorrow.\",\n",
    "    \"i like coffee.\",\n",
    "    \"i don't like tea.\",\n",
    "    \"what time is it?\",\n",
    "    \"it is three o'clock.\",\n",
    "    \"where is the train station?\",\n",
    "    \"the station is nearby.\",\n",
    "    \"how much is it?\",\n",
    "    \"it's too expensive.\",\n",
    "    \"do you speak english?\",\n",
    "    \"a little.\",\n",
    "    \"i don't understand.\",\n",
    "    \"can you repeat?\",\n",
    "    \"i am sorry.\",\n",
    "    \"no problem.\",\n",
    "    \"enjoy your meal.\",\n",
    "    \"cheers.\",\n",
    "    \"i am hungry.\",\n",
    "    \"i am thirsty.\",\n",
    "    \"the weather is nice today.\",\n",
    "    \"it's raining.\",\n",
    "    \"it's cold.\",\n",
    "    \"it's hot.\",\n",
    "    \"i work here.\",\n",
    "    \"where do you live?\",\n",
    "    \"i live in Paris.\",\n",
    "    \"how old are you?\",\n",
    "    \"i am twenty-five years old.\",\n",
    "    \"do you have brothers and sisters?\",\n",
    "    \"i have a sister.\",\n",
    "    \"i have a cat.\",\n",
    "    \"i like to travel.\",\n",
    "    \"i am a student.\",\n",
    "    \"i am a teacher.\",\n",
    "    \"help!\",\n",
    "    \"happy birthday!\",\n",
    "    \"congratulations!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "en_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 95, vocab_size_fr: 107\n",
      "max_len_x: 8, max_len_y: 8, max_vocab_size: 107, max_seq_len: 8\n",
      "French sentences:\n",
      "['bonjour.', 'au revoir.', 'merci beaucoup.', \"s'il vous plaît.\", 'comment allez-vous ?', 'je vais bien.', 'je suis fatigué.', 'je suis content.', 'quel est votre nom ?', 'mon nom est Jean.', 'enchanté de vous rencontrer.', 'bonne journée.', 'bonne soirée.', 'à demain.', \"j'aime le café.\", \"je n'aime pas le thé.\", 'quelle heure est-il ?', 'il est trois heures.', 'où est la gare ?', \"la gare est près d'ici.\", 'combien ça coûte ?', \"c'est trop cher.\", 'parlez-vous anglais ?', 'un peu.', 'je ne comprends pas.', 'pouvez-vous répéter ?', 'je suis désolé.', 'pas de problème.', 'bon appétit.', 'à votre santé.', \"j'ai faim.\", \"j'ai soif.\", \"il fait beau aujourd'hui.\", 'il pleut.', 'il fait froid.', 'il fait chaud.', 'je travaille ici.', 'où habitez-vous ?', \"j'habite à Paris.\", 'quel âge avez-vous ?', \"j'ai vingt-cinq ans.\", 'avez-vous des frères et sœurs ?', \"j'ai une sœur.\", \"j'ai un chat.\", \"j'aime voyager.\", 'je suis étudiant.', 'je suis professeur.', 'au secours !', 'joyeux anniversaire !', 'félicitations !']\n",
      "English sentences:\n",
      "['hello.', 'goodbye.', 'thank you very much.', 'please.', 'how are you?', 'i am fine.', 'i am tired.', 'i am happy.', 'what is your name?', 'my name is John.', 'nice to meet you.', 'have a nice day.', 'have a good evening.', 'see you tomorrow.', 'i like coffee.', \"i don't like tea.\", 'what time is it?', \"it is three o'clock.\", 'where is the train station?', 'the station is nearby.', 'how much is it?', \"it's too expensive.\", 'do you speak english?', 'a little.', \"i don't understand.\", 'can you repeat?', 'i am sorry.', 'no problem.', 'enjoy your meal.', 'cheers.', 'i am hungry.', 'i am thirsty.', 'the weather is nice today.', \"it's raining.\", \"it's cold.\", \"it's hot.\", 'i work here.', 'where do you live?', 'i live in Paris.', 'how old are you?', 'i am twenty-five years old.', 'do you have brothers and sisters?', 'i have a sister.', 'i have a cat.', 'i like to travel.', 'i am a student.', 'i am a teacher.', 'help!', 'happy birthday!', 'congratulations!']\n",
      "French tokenizer:\n",
      "{'<PAD>': 0, '.': 1, '?': 2, 'je': 3, 'vous': 4, '-': 5, \"j'\": 6, 'il': 7, 'est': 8, 'suis': 9, 'ai': 10, 'à': 11, 'aime': 12, 'pas': 13, 'fait': 14, '!': 15, 'au': 16, 'quel': 17, 'votre': 18, 'nom': 19, 'de': 20, 'bonne': 21, 'le': 22, 'où': 23, 'la': 24, 'gare': 25, 'ici': 26, 'un': 27, 'avez': 28, 'bonjour': 29, 'revoir': 30, 'merci': 31, 'beaucoup': 32, \"s'\": 33, 'plaît': 34, 'comment': 35, 'allez': 36, 'vais': 37, 'bien': 38, 'fatigué': 39, 'content': 40, 'mon': 41, 'jean': 42, 'enchanté': 43, 'rencontrer': 44, 'journée': 45, 'soirée': 46, 'demain': 47, 'café': 48, \"n'\": 49, 'thé': 50, 'quelle': 51, 'heure': 52, 'trois': 53, 'heures': 54, 'près': 55, \"d'\": 56, 'combien': 57, 'ça': 58, 'coûte': 59, \"c'\": 60, 'trop': 61, 'cher': 62, 'parlez': 63, 'anglais': 64, 'peu': 65, 'ne': 66, 'comprends': 67, 'pouvez': 68, 'répéter': 69, 'désolé': 70, 'problème': 71, 'bon': 72, 'appétit': 73, 'santé': 74, 'faim': 75, 'soif': 76, 'beau': 77, \"aujourd'hui\": 78, 'pleut': 79, 'froid': 80, 'chaud': 81, 'travaille': 82, 'habitez': 83, 'habite': 84, 'paris': 85, 'âge': 86, 'vingt': 87, 'cinq': 88, 'ans': 89, 'des': 90, 'frères': 91, 'et': 92, 'sœurs': 93, 'une': 94, 'sœur': 95, 'chat': 96, 'voyager': 97, 'étudiant': 98, 'professeur': 99, 'secours': 100, 'joyeux': 101, 'anniversaire': 102, 'félicitations': 103, '<UNK>': 105, '<SOS>': 106, '<EOS>': 107}\n",
      "English tokenizer:\n",
      "{'<PAD>': 0, '.': 1, 'i': 2, '?': 3, 'you': 4, 'am': 5, 'is': 6, 'a': 7, 'have': 8, \"it's\": 9, 'how': 10, 'nice': 11, 'like': 12, 'it': 13, 'the': 14, 'do': 15, '!': 16, 'much': 17, 'are': 18, 'happy': 19, 'what': 20, 'your': 21, 'name': 22, 'to': 23, \"don't\": 24, 'where': 25, 'station': 26, 'live': 27, 'old': 28, 'hello': 29, 'goodbye': 30, 'thank': 31, 'very': 32, 'please': 33, 'fine': 34, 'tired': 35, 'my': 36, 'john': 37, 'meet': 38, 'day': 39, 'good': 40, 'evening': 41, 'see': 42, 'tomorrow': 43, 'coffee': 44, 'tea': 45, 'time': 46, 'three': 47, \"o'\": 48, 'clock': 49, 'train': 50, 'nearby': 51, 'too': 52, 'expensive': 53, 'speak': 54, 'english': 55, 'little': 56, 'understand': 57, 'can': 58, 'repeat': 59, 'sorry': 60, 'no': 61, 'problem': 62, 'enjoy': 63, 'meal': 64, 'cheers': 65, 'hungry': 66, 'thirsty': 67, 'weather': 68, 'today': 69, 'raining': 70, 'cold': 71, 'hot': 72, 'work': 73, 'here': 74, 'in': 75, 'paris': 76, 'twenty': 77, '-': 78, 'five': 79, 'years': 80, 'brothers': 81, 'and': 82, 'sisters': 83, 'sister': 84, 'cat': 85, 'travel': 86, 'student': 87, 'teacher': 88, 'help': 89, 'birthday': 90, 'congratulations': 91, '<UNK>': 93, '<SOS>': 94, '<EOS>': 95}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  vocab_size=108,\n",
      "  d_model=32,\n",
      "  n_heads=2,\n",
      "  n_encoder_layers=2,\n",
      "  n_decoder_layers=2,\n",
      "  d_ff=64,\n",
      "  dropout_rate=0.4,\n",
      "  max_sequence_length=8\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=max_vocab_size,\n",
    "    d_model=32,        \n",
    "    n_heads=2,         \n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    d_ff=64,           \n",
    "    dropout_rate=0.4,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    random_state=42,\n",
    "    unk_idx=fr_tokenizer.word_index[fr_tokenizer.unk_token],\n",
    "    sos_idx=fr_tokenizer.word_index[fr_tokenizer.sos_token],\n",
    "    eos_idx=fr_tokenizer.word_index[fr_tokenizer.eos_token],\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function=SequenceCrossEntropy(\n",
    "        label_smoothing=0.1,\n",
    "    ),\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9\n",
    "    ),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527483e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting training data:\n",
      "\n",
      "Example 1:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.26337567 -0.88846732  1.47795107 -0.91079202 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.21345575 -0.84524867  1.5465388  -0.8678127  -0.8678127\n",
      " -0.8678127  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Example 2:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.55359679 -0.24105097 -0.88846732  1.47795107 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.19089172 -0.84524867  1.5465388  -0.8678127  -0.8678127\n",
      " -0.8678127  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Example 3:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.21872627 -0.19640156 -0.88846732  1.47795107 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.16832768 -0.77755657 -0.14576365 -0.48422414 -0.84524867\n",
      "  1.5465388  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "def inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer):\n",
    "    print(\"\\nInspecting training data:\")\n",
    "    for i in range(min(3, len(x_train_padded))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"Input sequence:\")\n",
    "        print(f\"Raw: {x_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [fr_tokenizer.index_word.get(idx, '<UNK>') for idx in x_train_padded[i]])\n",
    "        \n",
    "        print(\"\\nOutput sequence:\")\n",
    "        print(f\"Raw: {y_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [en_tokenizer.index_word.get(idx, '<UNK>') for idx in y_train_padded[i]])\n",
    "\n",
    "inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-----------------------------] 4% Epoch 1/300 - loss: 1.2685 -  - 0.01s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/300 - loss: 5.9576 -  - 0.25s\n",
      "Epoch 0 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  83  83  83  79  92  92  86 107]\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 79 -> word: five\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sisters sisters sisters five <UNK> <UNK> travel <UNK>\n",
      "Input: je vais bien\n",
      "Output: sisters sisters sisters five <UNK> <UNK> travel <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  80  80  79  80  92  92  88 107]\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 79 -> word: five\n",
      "Token 80 -> word: years\n",
      "Token 92 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 88 -> word: teacher\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: years years five years <UNK> <UNK> teacher <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: years years five years <UNK> <UNK> teacher <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  83  83  83  83  89  86  84 107]\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 89 -> word: help\n",
      "Token 86 -> word: travel\n",
      "Token 84 -> word: sister\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sisters sisters sisters sisters help travel sister <UNK>\n",
      "Input: bonjour\n",
      "Output: sisters sisters sisters sisters help travel sister <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 2/300 - loss: 4.5211 -  - 0.22s\n",
      "[==============================] 100% Epoch 3/300 - loss: 4.4560 -  - 0.20s\n",
      "[==============================] 100% Epoch 4/300 - loss: 4.4248 -  - 0.22s\n",
      "[==============================] 100% Epoch 5/300 - loss: 4.3924 -  - 0.21s\n",
      "[==============================] 100% Epoch 6/300 - loss: 4.1658 -  - 0.22s\n",
      "Epoch 5 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  81  83  83  79  92  86  88 107]\n",
      "Token 81 -> word: brothers\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 79 -> word: five\n",
      "Token 92 -> word: <UNK>\n",
      "Token 86 -> word: travel\n",
      "Token 88 -> word: teacher\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: brothers sisters sisters five <UNK> travel teacher <UNK>\n",
      "Input: je vais bien\n",
      "Output: brothers sisters sisters five <UNK> travel teacher <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  80  80  79  81  90  92  88 107]\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 79 -> word: five\n",
      "Token 81 -> word: brothers\n",
      "Token 90 -> word: birthday\n",
      "Token 92 -> word: <UNK>\n",
      "Token 88 -> word: teacher\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: years years five brothers birthday <UNK> teacher <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: years years five brothers birthday <UNK> teacher <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  83  82  83  83  86  83  84 107]\n",
      "Token 83 -> word: sisters\n",
      "Token 82 -> word: and\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 86 -> word: travel\n",
      "Token 83 -> word: sisters\n",
      "Token 84 -> word: sister\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sisters and sisters sisters travel sisters sister <UNK>\n",
      "Input: bonjour\n",
      "Output: sisters and sisters sisters travel sisters sister <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 7/300 - loss: 2.1656 -  - 0.25s\n",
      "[==============================] 100% Epoch 8/300 - loss: 1.4050 -  - 0.23s\n",
      "[==============================] 100% Epoch 9/300 - loss: 1.3249 -  - 0.23s\n",
      "[==============================] 100% Epoch 10/300 - loss: 1.2918 -  - 0.23s\n",
      "[==============================] 100% Epoch 11/300 - loss: 1.2596 -  - 0.22s\n",
      "Epoch 10 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  79  80  79  79  86  84  83 107]\n",
      "Token 79 -> word: five\n",
      "Token 80 -> word: years\n",
      "Token 79 -> word: five\n",
      "Token 79 -> word: five\n",
      "Token 86 -> word: travel\n",
      "Token 84 -> word: sister\n",
      "Token 83 -> word: sisters\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: five years five five travel sister sisters <UNK>\n",
      "Input: je vais bien\n",
      "Output: five years five five travel sister sisters <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  77  77  77  77  84  86  86 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 77 -> word: twenty\n",
      "Token 77 -> word: twenty\n",
      "Token 77 -> word: twenty\n",
      "Token 84 -> word: sister\n",
      "Token 86 -> word: travel\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty twenty twenty twenty sister travel travel <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: twenty twenty twenty twenty sister travel travel <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  82  80  80  79  84  80  83 107]\n",
      "Token 82 -> word: and\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 79 -> word: five\n",
      "Token 84 -> word: sister\n",
      "Token 80 -> word: years\n",
      "Token 83 -> word: sisters\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: and years years five sister years sisters <UNK>\n",
      "Input: bonjour\n",
      "Output: and years years five sister years sisters <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 12/300 - loss: 1.2458 -  - 0.22s\n",
      "[==============================] 100% Epoch 13/300 - loss: 1.2156 -  - 0.24s\n",
      "[==============================] 100% Epoch 14/300 - loss: 1.1927 -  - 0.20s\n",
      "[==============================] 100% Epoch 15/300 - loss: 1.1611 -  - 0.22s\n",
      "[==============================] 100% Epoch 16/300 - loss: 1.1438 -  - 0.23s\n",
      "Epoch 15 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  77  76  83  77  79  80  80 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 76 -> word: paris\n",
      "Token 83 -> word: sisters\n",
      "Token 77 -> word: twenty\n",
      "Token 79 -> word: five\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty paris sisters twenty five years years <UNK>\n",
      "Input: je vais bien\n",
      "Output: twenty paris sisters twenty five years years <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  77  74  77  76  86  84  82 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 74 -> word: here\n",
      "Token 77 -> word: twenty\n",
      "Token 76 -> word: paris\n",
      "Token 86 -> word: travel\n",
      "Token 84 -> word: sister\n",
      "Token 82 -> word: and\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty here twenty paris travel sister and <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: twenty here twenty paris travel sister and <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  80  80  79  79  83  80  82 107]\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 79 -> word: five\n",
      "Token 79 -> word: five\n",
      "Token 83 -> word: sisters\n",
      "Token 80 -> word: years\n",
      "Token 82 -> word: and\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: years years five five sisters years and <UNK>\n",
      "Input: bonjour\n",
      "Output: years years five five sisters years and <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 17/300 - loss: 1.1375 -  - 0.22s\n",
      "[==============================] 100% Epoch 18/300 - loss: 1.1390 -  - 0.21s\n",
      "[==============================] 100% Epoch 19/300 - loss: 1.1343 -  - 0.23s\n",
      "[==============================] 100% Epoch 20/300 - loss: 1.1345 -  - 0.22s\n",
      "[==============================] 100% Epoch 21/300 - loss: 1.1373 -  - 0.21s\n",
      "Epoch 20 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  77  76  82  73  81  80  80 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 76 -> word: paris\n",
      "Token 82 -> word: and\n",
      "Token 73 -> word: work\n",
      "Token 81 -> word: brothers\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty paris and work brothers years years <UNK>\n",
      "Input: je vais bien\n",
      "Output: twenty paris and work brothers years years <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  77  74  79  79  83  84  82 107]\n",
      "Token 77 -> word: twenty\n",
      "Token 74 -> word: here\n",
      "Token 79 -> word: five\n",
      "Token 79 -> word: five\n",
      "Token 83 -> word: sisters\n",
      "Token 84 -> word: sister\n",
      "Token 82 -> word: and\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: twenty here five five sisters sister and <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: twenty here five five sisters sister and <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  79  79  81  80  83  82  83 107]\n",
      "Token 79 -> word: five\n",
      "Token 79 -> word: five\n",
      "Token 81 -> word: brothers\n",
      "Token 80 -> word: years\n",
      "Token 83 -> word: sisters\n",
      "Token 82 -> word: and\n",
      "Token 83 -> word: sisters\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: five five brothers years sisters and sisters <UNK>\n",
      "Input: bonjour\n",
      "Output: five five brothers years sisters and sisters <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 22/300 - loss: 1.1416 -  - 0.22s\n",
      "[==============================] 100% Epoch 23/300 - loss: 1.1329 -  - 0.21s\n",
      "[==============================] 100% Epoch 24/300 - loss: 1.1295 -  - 0.23s\n",
      "[==============================] 100% Epoch 25/300 - loss: 1.1358 -  - 0.21s\n",
      "[==============================] 100% Epoch 26/300 - loss: 1.1455 -  - 0.22s\n",
      "Epoch 25 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  76  80  77  77  80  80  79 107]\n",
      "Token 76 -> word: paris\n",
      "Token 80 -> word: years\n",
      "Token 77 -> word: twenty\n",
      "Token 77 -> word: twenty\n",
      "Token 80 -> word: years\n",
      "Token 80 -> word: years\n",
      "Token 79 -> word: five\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: paris years twenty twenty years years five <UNK>\n",
      "Input: je vais bien\n",
      "Output: paris years twenty twenty years years five <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  72  79  77  73  77  77  74 107]\n",
      "Token 72 -> word: hot\n",
      "Token 79 -> word: five\n",
      "Token 77 -> word: twenty\n",
      "Token 73 -> word: work\n",
      "Token 77 -> word: twenty\n",
      "Token 77 -> word: twenty\n",
      "Token 74 -> word: here\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: hot five twenty work twenty twenty here <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: hot five twenty work twenty twenty here <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  78  79  82  74  86  82  82 107]\n",
      "Token 78 -> word: -\n",
      "Token 79 -> word: five\n",
      "Token 82 -> word: and\n",
      "Token 74 -> word: here\n",
      "Token 86 -> word: travel\n",
      "Token 82 -> word: and\n",
      "Token 82 -> word: and\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: - five and here travel and and <UNK>\n",
      "Input: bonjour\n",
      "Output: - five and here travel and and <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 27/300 - loss: 1.1506 -  - 0.22s\n",
      "[==============================] 100% Epoch 28/300 - loss: 1.1551 -  - 0.22s\n",
      "[==============================] 100% Epoch 29/300 - loss: 1.1577 -  - 0.21s\n",
      "[==============================] 100% Epoch 30/300 - loss: 1.1616 -  - 0.23s\n",
      "[==============================] 100% Epoch 31/300 - loss: 1.1651 -  - 0.22s\n",
      "Epoch 30 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  72  79  74  73  74  77  72 107]\n",
      "Token 72 -> word: hot\n",
      "Token 79 -> word: five\n",
      "Token 74 -> word: here\n",
      "Token 73 -> word: work\n",
      "Token 74 -> word: here\n",
      "Token 77 -> word: twenty\n",
      "Token 72 -> word: hot\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: hot five here work here twenty hot <UNK>\n",
      "Input: je vais bien\n",
      "Output: hot five here work here twenty hot <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  72  72  72  72  72  72  71 107]\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 71 -> word: cold\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: hot hot hot hot hot hot cold <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: hot hot hot hot hot hot cold <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  73  79  80  73  77  77  74 107]\n",
      "Token 73 -> word: work\n",
      "Token 79 -> word: five\n",
      "Token 80 -> word: years\n",
      "Token 73 -> word: work\n",
      "Token 77 -> word: twenty\n",
      "Token 77 -> word: twenty\n",
      "Token 74 -> word: here\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: work five years work twenty twenty here <UNK>\n",
      "Input: bonjour\n",
      "Output: work five years work twenty twenty here <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 32/300 - loss: 1.1667 -  - 0.22s\n",
      "[==============================] 100% Epoch 33/300 - loss: 1.1743 -  - 0.22s\n",
      "[==============================] 100% Epoch 34/300 - loss: 1.2663 -  - 0.21s\n",
      "[==============================] 100% Epoch 35/300 - loss: 1.5834 -  - 0.22s\n",
      "[==============================] 100% Epoch 36/300 - loss: 1.8454 -  - 0.20s\n",
      "Epoch 35 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  68  72  72  72  72  68  68 107]\n",
      "Token 68 -> word: weather\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 68 -> word: weather\n",
      "Token 68 -> word: weather\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: weather hot hot hot hot weather weather <UNK>\n",
      "Input: je vais bien\n",
      "Output: weather hot hot hot hot weather weather <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  56  72  72  72  65  59  56 107]\n",
      "Token 56 -> word: little\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 65 -> word: cheers\n",
      "Token 59 -> word: repeat\n",
      "Token 56 -> word: little\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: little hot hot hot cheers repeat little <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: little hot hot hot cheers repeat little <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  72  80  72  72  73  72  70 107]\n",
      "Token 72 -> word: hot\n",
      "Token 80 -> word: years\n",
      "Token 72 -> word: hot\n",
      "Token 72 -> word: hot\n",
      "Token 73 -> word: work\n",
      "Token 72 -> word: hot\n",
      "Token 70 -> word: raining\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: hot years hot hot work hot raining <UNK>\n",
      "Input: bonjour\n",
      "Output: hot years hot hot work hot raining <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 37/300 - loss: 1.8433 -  - 0.20s\n",
      "[==============================] 100% Epoch 38/300 - loss: 1.8462 -  - 0.20s\n",
      "[==============================] 100% Epoch 39/300 - loss: 1.8468 -  - 0.20s\n",
      "[==============================] 100% Epoch 40/300 - loss: 1.8406 -  - 0.20s\n",
      "[==============================] 100% Epoch 41/300 - loss: 1.8396 -  - 0.21s\n",
      "Epoch 40 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  52  63  56  56  56  56  56 107]\n",
      "Token 52 -> word: too\n",
      "Token 63 -> word: enjoy\n",
      "Token 56 -> word: little\n",
      "Token 56 -> word: little\n",
      "Token 56 -> word: little\n",
      "Token 56 -> word: little\n",
      "Token 56 -> word: little\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: too enjoy little little little little little <UNK>\n",
      "Input: je vais bien\n",
      "Output: too enjoy little little little little little <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  44  68  56  55  56  53  48 107]\n",
      "Token 44 -> word: coffee\n",
      "Token 68 -> word: weather\n",
      "Token 56 -> word: little\n",
      "Token 55 -> word: english\n",
      "Token 56 -> word: little\n",
      "Token 53 -> word: expensive\n",
      "Token 48 -> word: o'\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: coffee weather little english little expensive o' <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: coffee weather little english little expensive o' <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  55  70  56  56  61  56  56 107]\n",
      "Token 55 -> word: english\n",
      "Token 70 -> word: raining\n",
      "Token 56 -> word: little\n",
      "Token 56 -> word: little\n",
      "Token 61 -> word: no\n",
      "Token 56 -> word: little\n",
      "Token 56 -> word: little\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: english raining little little no little little <UNK>\n",
      "Input: bonjour\n",
      "Output: english raining little little no little little <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 42/300 - loss: 1.8401 -  - 0.20s\n",
      "[==============================] 100% Epoch 43/300 - loss: 1.8473 -  - 0.20s\n",
      "[==============================] 100% Epoch 44/300 - loss: 1.8545 -  - 0.21s\n",
      "Early stopping triggered after epoch 44\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monitor = TrainingMonitor(model, fr_tokenizer, en_tokenizer, test_sentences)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=300,\n",
    "    batch_size=2,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss', patience=20),\n",
    "        monitor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes:\n",
      "French vocab size: 107\n",
      "English vocab size: 95\n",
      "\n",
      "==================================================\n",
      "Testing: je vais bien\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106   1  17  14  14  27   7  25 107]\n",
      "Token 1 -> word: .\n",
      "Token 17 -> word: much\n",
      "Token 14 -> word: the\n",
      "Token 14 -> word: the\n",
      "Token 27 -> word: live\n",
      "Token 7 -> word: a\n",
      "Token 25 -> word: where\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . much the the live a where <UNK>\n",
      "Translation: . much the the live a where <UNK>\n",
      "\n",
      "==================================================\n",
      "Testing: comment allez-vous ?\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   1  25  10   1  16   5   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 25 -> word: where\n",
      "Token 10 -> word: how\n",
      "Token 1 -> word: .\n",
      "Token 16 -> word: !\n",
      "Token 5 -> word: am\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . where how . ! am . <UNK>\n",
      "Translation: . where how . ! am . <UNK>\n",
      "\n",
      "==================================================\n",
      "Testing: bonjour\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106   1  30  14  14  27   5   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 30 -> word: goodbye\n",
      "Token 14 -> word: the\n",
      "Token 14 -> word: the\n",
      "Token 27 -> word: live\n",
      "Token 5 -> word: am\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . goodbye the the live am . <UNK>\n",
      "Translation: . goodbye the the live am . <UNK>\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary sizes:\")\n",
    "print(f\"French vocab size: {len(fr_tokenizer.word_index)}\")\n",
    "print(f\"English vocab size: {len(en_tokenizer.word_index)}\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer, \n",
    "                      temperature=0.8,\n",
    "                      beam_size=10,\n",
    "                      min_length=2) \n",
    "    print(f\"Translation: {translation}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e205b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: je vais bien\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (2, 2, 7, 8)\n",
      "First attention head values:\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "Analyzing: comment allez-vous ?\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (2, 2, 7, 8)\n",
      "First attention head values:\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def analyze_attention_weights(model, input_sentence, fr_tokenizer):\n",
    "    tokens = fr_tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    \n",
    "    _ = model.predict(padded)\n",
    "    \n",
    "    attention_weights = model.decoder_layers[-1].cross_attention.attention_weights\n",
    "    \n",
    "    print(\"\\nAttention Analysis:\")\n",
    "    print(f\"Attention shape: {attention_weights.shape}\")\n",
    "    print(\"First attention head values:\")\n",
    "    print(attention_weights[0, 0])\n",
    "\n",
    "for sent in test_sentences[:2]:\n",
    "    print(f\"\\nAnalyzing: {sent}\")\n",
    "    analyze_attention_weights(model, sent, fr_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
