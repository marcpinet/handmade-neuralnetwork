{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"je suis heureux.\",\n",
    "    \"j'aime les chats.\",\n",
    "    \"bonjour le monde.\",\n",
    "    \"au revoir.\",\n",
    "    \"comment allez-vous ?\",\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"i am happy.\",\n",
    "    \"i like cats.\",\n",
    "    \"hello world.\",\n",
    "    \"goodbye.\",\n",
    "    \"how are you?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "en_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 12, vocab_size_fr: 16\n",
      "max_len_x: 5, max_len_y: 4, max_vocab_size: 20, max_seq_len: 5\n",
      "French sentences:\n",
      "['je suis heureux.', \"j'aime les chats.\", 'bonjour le monde.', 'au revoir.', 'comment allez-vous ?']\n",
      "English sentences:\n",
      "['i am happy.', 'i like cats.', 'hello world.', 'goodbye.', 'how are you?']\n",
      "French tokenizer:\n",
      "{'je': 1, 'suis': 2, 'heureux': 3, \"j'\": 4, 'aime': 5, 'les': 6, 'chats': 7, 'bonjour': 8, 'le': 9, 'monde': 10, 'au': 11, 'revoir': 12, 'comment': 13, 'allez': 14, 'vous': 15, '<OOV>': 16}\n",
      "English tokenizer:\n",
      "{'i': 1, 'am': 2, 'happy': 3, 'like': 4, 'cats': 5, 'hello': 6, 'world': 7, 'goodbye': 8, 'how': 9, 'are': 10, 'you': 11, '<OOV>': 12}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  vocab_size=24,\n",
      "  d_model=128,\n",
      "  n_heads=4,\n",
      "  n_encoder_layers=2,\n",
      "  n_decoder_layers=2,\n",
      "  d_ff=256,\n",
      "  dropout_rate=0.2,\n",
      "  max_sequence_length=5\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=max_vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    d_ff=256,\n",
    "    dropout_rate=0.2,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    temperature=0.7,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function='sequencecrossentropy',\n",
    "    optimizer='adam',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/50 - loss: 12.6405 -  - 0.07s\n",
      "[==============================] 100% Epoch 2/50 - loss: 8.8913 -  - 0.04s\n",
      "[==============================] 100% Epoch 3/50 - loss: 5.5905 -  - 0.03s\n",
      "[==============================] 100% Epoch 4/50 - loss: 1.8309 -  - 0.03s\n",
      "[==============================] 100% Epoch 5/50 - loss: 1.3206 -  - 0.03s\n",
      "[==============================] 100% Epoch 6/50 - loss: 0.0618 -  - 0.03s\n",
      "[==============================] 100% Epoch 7/50 - loss: 0.0073 -  - 0.04s\n",
      "[==============================] 100% Epoch 8/50 - loss: 0.0071 -  - 0.03s\n",
      "[==============================] 100% Epoch 9/50 - loss: 0.0077 -  - 0.03s\n",
      "[==============================] 100% Epoch 10/50 - loss: 0.0088 -  - 0.03s\n",
      "[==============================] 100% Epoch 11/50 - loss: 0.0137 -  - 0.03s\n",
      "[==============================] 100% Epoch 12/50 - loss: 0.0133 -  - 0.03s\n",
      "[==============================] 100% Epoch 13/50 - loss: 0.0125 -  - 0.03s\n",
      "[==============================] 100% Epoch 14/50 - loss: 0.0065 -  - 0.03s\n",
      "[==============================] 100% Epoch 15/50 - loss: 0.0057 -  - 0.03s\n",
      "[==============================] 100% Epoch 16/50 - loss: 0.0051 -  - 0.03s\n",
      "[==============================] 100% Epoch 17/50 - loss: 0.0045 -  - 0.03s\n",
      "[==============================] 100% Epoch 18/50 - loss: 0.0040 -  - 0.03s\n",
      "[==============================] 100% Epoch 19/50 - loss: 0.0036 -  - 0.03s\n",
      "[==============================] 100% Epoch 20/50 - loss: 0.0033 -  - 0.04s\n",
      "[==============================] 100% Epoch 21/50 - loss: 0.0030 -  - 0.04s\n",
      "[==============================] 100% Epoch 22/50 - loss: 0.0027 -  - 0.03s\n",
      "[==============================] 100% Epoch 23/50 - loss: 0.0025 -  - 0.03s\n",
      "[==============================] 100% Epoch 24/50 - loss: 0.0023 -  - 0.03s\n",
      "[==============================] 100% Epoch 25/50 - loss: 0.0021 -  - 0.04s\n",
      "[==============================] 100% Epoch 26/50 - loss: 0.0020 -  - 0.04s\n",
      "[==============================] 100% Epoch 27/50 - loss: 0.0018 -  - 0.03s\n",
      "[==============================] 100% Epoch 28/50 - loss: 0.0017 -  - 0.04s\n",
      "[==============================] 100% Epoch 29/50 - loss: 0.0017 -  - 0.06s\n",
      "[==============================] 100% Epoch 30/50 - loss: 0.0018 -  - 0.03s\n",
      "[==============================] 100% Epoch 31/50 - loss: 0.0020 -  - 0.03s\n",
      "[==============================] 100% Epoch 32/50 - loss: 0.0024 -  - 0.03s\n",
      "[==============================] 100% Epoch 33/50 - loss: 0.0030 -  - 0.03s\n",
      "[==============================] 100% Epoch 34/50 - loss: 0.0086 -  - 0.03s\n",
      "[==============================] 100% Epoch 35/50 - loss: 0.0030 -  - 0.03s\n",
      "[==============================] 100% Epoch 36/50 - loss: 0.0030 -  - 0.03s\n",
      "[==============================] 100% Epoch 37/50 - loss: 0.0079 -  - 0.03s\n",
      "[==============================] 100% Epoch 38/50 - loss: 0.0032 -  - 0.03s\n",
      "[==============================] 100% Epoch 39/50 - loss: 0.0035 -  - 0.03s\n",
      "[==============================] 100% Epoch 40/50 - loss: 0.0043 -  - 0.04s\n",
      "[==============================] 100% Epoch 41/50 - loss: 0.0093 -  - 0.03s\n",
      "[==============================] 100% Epoch 42/50 - loss: 0.0043 -  - 0.03s\n",
      "[==============================] 100% Epoch 43/50 - loss: 0.0042 -  - 0.03s\n",
      "[==============================] 100% Epoch 44/50 - loss: 0.0044 -  - 0.03s\n",
      "[==============================] 100% Epoch 45/50 - loss: 0.0047 -  - 0.03s\n",
      "[==============================] 100% Epoch 46/50 - loss: 0.0093 -  - 0.03s\n",
      "[==============================] 100% Epoch 47/50 - loss: 0.0039 -  - 0.03s\n",
      "[==============================] 100% Epoch 48/50 - loss: 0.0034 -  - 0.03s\n",
      "[==============================] 100% Epoch 49/50 - loss: 0.0032 -  - 0.04s\n",
      "[==============================] 100% Epoch 50/50 - loss: 0.0030 -  - 0.04s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=50,\n",
    "    batch_size=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: je suis heureux.\n",
      "EN: goodbye goodbye goodbye goodbye\n",
      "\n",
      "FR: comment allez-vous ?\n",
      "EN: goodbye goodbye goodbye goodbye\n",
      "\n",
      "FR: bonjour le monde.\n",
      "EN: goodbye goodbye goodbye goodbye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer) -> str:\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    tokens = [model.SOS_IDX] + [t + 4 for t in tokens] + [model.EOS_IDX]  # Shift indices by 4\n",
    "    padded = pad_sequences([tokens], max_length=max_len_x, padding='post', pad_value=model.PAD_IDX)\n",
    "    \n",
    "    pred = model.predict(padded, max_length=max_seq_len)[0]\n",
    "    words = []\n",
    "    for idx in pred[1:]:\n",
    "        if idx == model.EOS_IDX:\n",
    "            break\n",
    "        if idx in [model.PAD_IDX, model.UNK_IDX, model.SOS_IDX]:\n",
    "            continue\n",
    "        word = en_tokenizer.index_word.get(idx - 4, \"<OOV>\")\n",
    "        words.append(word)\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "test_sentences = [\n",
    "    \"je suis heureux.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour le monde.\"\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(f\"FR: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer)\n",
    "    print(f\"EN: {translation}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
