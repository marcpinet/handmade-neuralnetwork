{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"bonjour.\",\n",
    "    \"au revoir.\",\n",
    "    \"merci beaucoup.\",\n",
    "    \"s'il vous plaît.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"je vais bien.\",\n",
    "    \"je suis fatigué.\",\n",
    "    \"je suis content.\",\n",
    "    \"quel est votre nom ?\",\n",
    "    \"mon nom est Jean.\",\n",
    "    \"enchanté de vous rencontrer.\",\n",
    "    \"bonne journée.\",\n",
    "    \"bonne soirée.\",\n",
    "    \"à demain.\",\n",
    "    \"j'aime le café.\",\n",
    "    \"je n'aime pas le thé.\",\n",
    "    \"quelle heure est-il ?\",\n",
    "    \"il est trois heures.\",\n",
    "    \"où est la gare ?\",\n",
    "    \"la gare est près d'ici.\",\n",
    "    \"combien ça coûte ?\",\n",
    "    \"c'est trop cher.\",\n",
    "    \"parlez-vous anglais ?\",\n",
    "    \"un peu.\",\n",
    "    \"je ne comprends pas.\",\n",
    "    \"pouvez-vous répéter ?\",\n",
    "    \"je suis désolé.\",\n",
    "    \"pas de problème.\",\n",
    "    \"bon appétit.\",\n",
    "    \"à votre santé.\",\n",
    "    \"j'ai faim.\",\n",
    "    \"j'ai soif.\",\n",
    "    \"il fait beau aujourd'hui.\",\n",
    "    \"il pleut.\",\n",
    "    \"il fait froid.\",\n",
    "    \"il fait chaud.\",\n",
    "    \"je travaille ici.\",\n",
    "    \"où habitez-vous ?\",\n",
    "    \"j'habite à Paris.\",\n",
    "    \"quel âge avez-vous ?\",\n",
    "    \"j'ai vingt-cinq ans.\",\n",
    "    \"avez-vous des frères et sœurs ?\",\n",
    "    \"j'ai une sœur.\",\n",
    "    \"j'ai un chat.\",\n",
    "    \"j'aime voyager.\",\n",
    "    \"je suis étudiant.\",\n",
    "    \"je suis professeur.\",\n",
    "    \"au secours !\",\n",
    "    \"joyeux anniversaire !\",\n",
    "    \"félicitations !\"\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"hello.\",\n",
    "    \"goodbye.\",\n",
    "    \"thank you very much.\",\n",
    "    \"please.\",\n",
    "    \"how are you?\",\n",
    "    \"i am fine.\",\n",
    "    \"i am tired.\",\n",
    "    \"i am happy.\",\n",
    "    \"what is your name?\",\n",
    "    \"my name is John.\",\n",
    "    \"nice to meet you.\",\n",
    "    \"have a nice day.\",\n",
    "    \"have a good evening.\",\n",
    "    \"see you tomorrow.\",\n",
    "    \"i like coffee.\",\n",
    "    \"i don't like tea.\",\n",
    "    \"what time is it?\",\n",
    "    \"it is three o'clock.\",\n",
    "    \"where is the train station?\",\n",
    "    \"the station is nearby.\",\n",
    "    \"how much is it?\",\n",
    "    \"it's too expensive.\",\n",
    "    \"do you speak english?\",\n",
    "    \"a little.\",\n",
    "    \"i don't understand.\",\n",
    "    \"can you repeat?\",\n",
    "    \"i am sorry.\",\n",
    "    \"no problem.\",\n",
    "    \"enjoy your meal.\",\n",
    "    \"cheers.\",\n",
    "    \"i am hungry.\",\n",
    "    \"i am thirsty.\",\n",
    "    \"the weather is nice today.\",\n",
    "    \"it's raining.\",\n",
    "    \"it's cold.\",\n",
    "    \"it's hot.\",\n",
    "    \"i work here.\",\n",
    "    \"where do you live?\",\n",
    "    \"i live in Paris.\",\n",
    "    \"how old are you?\",\n",
    "    \"i am twenty-five years old.\",\n",
    "    \"do you have brothers and sisters?\",\n",
    "    \"i have a sister.\",\n",
    "    \"i have a cat.\",\n",
    "    \"i like to travel.\",\n",
    "    \"i am a student.\",\n",
    "    \"i am a teacher.\",\n",
    "    \"help!\",\n",
    "    \"happy birthday!\",\n",
    "    \"congratulations!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer()\n",
    "en_tokenizer = Tokenizer()\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 91, vocab_size_fr: 103\n",
      "max_len_x: 8, max_len_y: 8, max_vocab_size: 103, max_seq_len: 8\n",
      "French sentences:\n",
      "['bonjour.', 'au revoir.', 'merci beaucoup.', \"s'il vous plaît.\", 'comment allez-vous ?', 'je vais bien.', 'je suis fatigué.', 'je suis content.', 'quel est votre nom ?', 'mon nom est Jean.', 'enchanté de vous rencontrer.', 'bonne journée.', 'bonne soirée.', 'à demain.', \"j'aime le café.\", \"je n'aime pas le thé.\", 'quelle heure est-il ?', 'il est trois heures.', 'où est la gare ?', \"la gare est près d'ici.\", 'combien ça coûte ?', \"c'est trop cher.\", 'parlez-vous anglais ?', 'un peu.', 'je ne comprends pas.', 'pouvez-vous répéter ?', 'je suis désolé.', 'pas de problème.', 'bon appétit.', 'à votre santé.', \"j'ai faim.\", \"j'ai soif.\", \"il fait beau aujourd'hui.\", 'il pleut.', 'il fait froid.', 'il fait chaud.', 'je travaille ici.', 'où habitez-vous ?', \"j'habite à Paris.\", 'quel âge avez-vous ?', \"j'ai vingt-cinq ans.\", 'avez-vous des frères et sœurs ?', \"j'ai une sœur.\", \"j'ai un chat.\", \"j'aime voyager.\", 'je suis étudiant.', 'je suis professeur.', 'au secours !', 'joyeux anniversaire !', 'félicitations !']\n",
      "English sentences:\n",
      "['hello.', 'goodbye.', 'thank you very much.', 'please.', 'how are you?', 'i am fine.', 'i am tired.', 'i am happy.', 'what is your name?', 'my name is John.', 'nice to meet you.', 'have a nice day.', 'have a good evening.', 'see you tomorrow.', 'i like coffee.', \"i don't like tea.\", 'what time is it?', \"it is three o'clock.\", 'where is the train station?', 'the station is nearby.', 'how much is it?', \"it's too expensive.\", 'do you speak english?', 'a little.', \"i don't understand.\", 'can you repeat?', 'i am sorry.', 'no problem.', 'enjoy your meal.', 'cheers.', 'i am hungry.', 'i am thirsty.', 'the weather is nice today.', \"it's raining.\", \"it's cold.\", \"it's hot.\", 'i work here.', 'where do you live?', 'i live in Paris.', 'how old are you?', 'i am twenty-five years old.', 'do you have brothers and sisters?', 'i have a sister.', 'i have a cat.', 'i like to travel.', 'i am a student.', 'i am a teacher.', 'help!', 'happy birthday!', 'congratulations!']\n",
      "French tokenizer:\n",
      "{'<PAD>': 0, 'je': 1, 'vous': 2, \"j'\": 3, 'il': 4, 'est': 5, 'suis': 6, 'ai': 7, 'à': 8, 'aime': 9, 'pas': 10, 'fait': 11, 'au': 12, 'quel': 13, 'votre': 14, 'nom': 15, 'de': 16, 'bonne': 17, 'le': 18, 'où': 19, 'la': 20, 'gare': 21, 'ici': 22, 'un': 23, 'avez': 24, 'bonjour': 25, 'revoir': 26, 'merci': 27, 'beaucoup': 28, \"s'\": 29, 'plaît': 30, 'comment': 31, 'allez': 32, 'vais': 33, 'bien': 34, 'fatigué': 35, 'content': 36, 'mon': 37, 'jean': 38, 'enchanté': 39, 'rencontrer': 40, 'journée': 41, 'soirée': 42, 'demain': 43, 'café': 44, \"n'\": 45, 'thé': 46, 'quelle': 47, 'heure': 48, 'trois': 49, 'heures': 50, 'près': 51, \"d'\": 52, 'combien': 53, 'ça': 54, 'coûte': 55, \"c'\": 56, 'trop': 57, 'cher': 58, 'parlez': 59, 'anglais': 60, 'peu': 61, 'ne': 62, 'comprends': 63, 'pouvez': 64, 'répéter': 65, 'désolé': 66, 'problème': 67, 'bon': 68, 'appétit': 69, 'santé': 70, 'faim': 71, 'soif': 72, 'beau': 73, \"aujourd'hui\": 74, 'pleut': 75, 'froid': 76, 'chaud': 77, 'travaille': 78, 'habitez': 79, 'habite': 80, 'paris': 81, 'âge': 82, 'vingt': 83, 'cinq': 84, 'ans': 85, 'des': 86, 'frères': 87, 'et': 88, 'sœurs': 89, 'une': 90, 'sœur': 91, 'chat': 92, 'voyager': 93, 'étudiant': 94, 'professeur': 95, 'secours': 96, 'joyeux': 97, 'anniversaire': 98, 'félicitations': 99, '<UNK>': 101, '<SOS>': 102, '<EOS>': 103}\n",
      "English tokenizer:\n",
      "{'<PAD>': 0, 'i': 1, 'you': 2, 'am': 3, 'is': 4, 'a': 5, 'have': 6, \"it's\": 7, 'how': 8, 'nice': 9, 'like': 10, 'it': 11, 'the': 12, 'do': 13, 'much': 14, 'are': 15, 'happy': 16, 'what': 17, 'your': 18, 'name': 19, 'to': 20, \"don't\": 21, 'where': 22, 'station': 23, 'live': 24, 'old': 25, 'hello': 26, 'goodbye': 27, 'thank': 28, 'very': 29, 'please': 30, 'fine': 31, 'tired': 32, 'my': 33, 'john': 34, 'meet': 35, 'day': 36, 'good': 37, 'evening': 38, 'see': 39, 'tomorrow': 40, 'coffee': 41, 'tea': 42, 'time': 43, 'three': 44, \"o'\": 45, 'clock': 46, 'train': 47, 'nearby': 48, 'too': 49, 'expensive': 50, 'speak': 51, 'english': 52, 'little': 53, 'understand': 54, 'can': 55, 'repeat': 56, 'sorry': 57, 'no': 58, 'problem': 59, 'enjoy': 60, 'meal': 61, 'cheers': 62, 'hungry': 63, 'thirsty': 64, 'weather': 65, 'today': 66, 'raining': 67, 'cold': 68, 'hot': 69, 'work': 70, 'here': 71, 'in': 72, 'paris': 73, 'twenty': 74, 'five': 75, 'years': 76, 'brothers': 77, 'and': 78, 'sisters': 79, 'sister': 80, 'cat': 81, 'travel': 82, 'student': 83, 'teacher': 84, 'help': 85, 'birthday': 86, 'congratulations': 87, '<UNK>': 89, '<SOS>': 90, '<EOS>': 91}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  vocab_size=107,\n",
      "  d_model=64,\n",
      "  n_heads=4,\n",
      "  n_encoder_layers=1,\n",
      "  n_decoder_layers=1,\n",
      "  d_ff=128,\n",
      "  dropout_rate=0.1,\n",
      "  max_sequence_length=512\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=max_vocab_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=1,\n",
    "    n_decoder_layers=1,\n",
    "    d_ff=128,\n",
    "    dropout_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function='sequencecrossentropy',\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/50 - loss: 11.5804 -  - 7.56s\n",
      "[==============================] 100% Epoch 2/50 - loss: 10.8054 -  - 7.40s\n",
      "[==============================] 100% Epoch 3/50 - loss: 8.1660 -  - 7.42s\n",
      "[==============================] 100% Epoch 4/50 - loss: 5.0352 -  - 7.37s\n",
      "[==============================] 100% Epoch 5/50 - loss: 1.4511 -  - 7.36s\n",
      "[==============================] 100% Epoch 6/50 - loss: 0.3899 -  - 7.35s\n",
      "[==============================] 100% Epoch 7/50 - loss: 0.1863 -  - 7.35s\n",
      "[==============================] 100% Epoch 8/50 - loss: 0.1264 -  - 7.38s\n",
      "[==============================] 100% Epoch 9/50 - loss: 0.0901 -  - 7.35s\n",
      "[==============================] 100% Epoch 10/50 - loss: 0.0717 -  - 7.36s\n",
      "[==============================] 100% Epoch 11/50 - loss: 0.0599 -  - 7.35s\n",
      "[==============================] 100% Epoch 12/50 - loss: 0.0518 -  - 7.38s\n",
      "[==============================] 100% Epoch 13/50 - loss: 0.0462 -  - 7.30s\n",
      "[==============================] 100% Epoch 14/50 - loss: 0.0429 -  - 7.37s\n",
      "[==============================] 100% Epoch 15/50 - loss: 0.0407 -  - 7.36s\n",
      "[==============================] 100% Epoch 16/50 - loss: 0.0391 -  - 7.38s\n",
      "[==============================] 100% Epoch 17/50 - loss: 0.0378 -  - 7.41s\n",
      "[==============================] 100% Epoch 18/50 - loss: 0.0367 -  - 7.35s\n",
      "[==============================] 100% Epoch 19/50 - loss: 0.0359 -  - 7.33s\n",
      "[==============================] 100% Epoch 20/50 - loss: 0.0352 -  - 7.39s\n",
      "[==============================] 100% Epoch 21/50 - loss: 0.0346 -  - 7.38s\n",
      "[==============================] 100% Epoch 22/50 - loss: 0.0340 -  - 7.34s\n",
      "[==============================] 100% Epoch 23/50 - loss: 0.0335 -  - 7.41s\n",
      "[==============================] 100% Epoch 24/50 - loss: 0.0330 -  - 7.39s\n",
      "[==============================] 100% Epoch 25/50 - loss: 0.0325 -  - 7.33s\n",
      "[==============================] 100% Epoch 26/50 - loss: 0.0320 -  - 7.35s\n",
      "[==============================] 100% Epoch 27/50 - loss: 0.0315 -  - 7.32s\n",
      "[==============================] 100% Epoch 28/50 - loss: 0.0311 -  - 7.33s\n",
      "[==============================] 100% Epoch 29/50 - loss: 0.0307 -  - 7.30s\n",
      "[==============================] 100% Epoch 30/50 - loss: 0.0303 -  - 7.33s\n",
      "[==============================] 100% Epoch 31/50 - loss: 0.0300 -  - 7.28s\n",
      "[==============================] 100% Epoch 32/50 - loss: 0.0297 -  - 7.29s\n",
      "[==============================] 100% Epoch 33/50 - loss: 0.0293 -  - 7.26s\n",
      "[==============================] 100% Epoch 34/50 - loss: 0.0290 -  - 7.28s\n",
      "[==============================] 100% Epoch 35/50 - loss: 0.0287 -  - 7.33s\n",
      "[==============================] 100% Epoch 36/50 - loss: 0.0283 -  - 7.33s\n",
      "[==============================] 100% Epoch 37/50 - loss: 0.0280 -  - 7.38s\n",
      "[==============================] 100% Epoch 38/50 - loss: 0.0277 -  - 7.34s\n",
      "[==============================] 100% Epoch 39/50 - loss: 0.0274 -  - 7.31s\n",
      "[==============================] 100% Epoch 40/50 - loss: 0.0270 -  - 7.30s\n",
      "[==============================] 100% Epoch 41/50 - loss: 0.0267 -  - 7.34s\n",
      "[==============================] 100% Epoch 42/50 - loss: 0.0264 -  - 7.33s\n",
      "[==============================] 100% Epoch 43/50 - loss: 0.0261 -  - 7.34s\n",
      "[==============================] 100% Epoch 44/50 - loss: 0.0258 -  - 7.35s\n",
      "[==============================] 100% Epoch 45/50 - loss: 0.0255 -  - 7.34s\n",
      "[==============================] 100% Epoch 46/50 - loss: 0.0252 -  - 7.35s\n",
      "[==============================] 100% Epoch 47/50 - loss: 0.0250 -  - 7.36s\n",
      "[==============================] 100% Epoch 48/50 - loss: 0.0247 -  - 7.36s\n",
      "[==============================] 100% Epoch 49/50 - loss: 0.0244 -  - 7.36s\n",
      "[==============================] 100% Epoch 50/50 - loss: 0.0241 -  - 7.37s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes:\n",
      "French vocab size: 103\n",
      "English vocab size: 91\n",
      "\n",
      "==================================================\n",
      "Testing: je vais bien.\n",
      "\n",
      "Debug tokenization:\n",
      "Original sentence: je vais bien.\n",
      "Tokens after initial tokenization: [1, 33, 34, 101]\n",
      "Corresponding words: je vais bien <UNK>\n",
      "Tokens after special tokens: [102, 1, 33, 34, 101, 103]\n",
      "Padded sequence: [[102   1  33  34 101 103   0   0]]\n",
      "Raw prediction: [105  19]\n",
      "\n",
      "Token conversion:\n",
      "Index 19 maps to word: name\n",
      "\n",
      "Final translation: name\n",
      "\n",
      "==================================================\n",
      "Testing: comment allez-vous ?\n",
      "\n",
      "Debug tokenization:\n",
      "Original sentence: comment allez-vous ?\n",
      "Tokens after initial tokenization: [31, 32, 101, 2, 101]\n",
      "Corresponding words: comment allez <UNK> vous <UNK>\n",
      "Tokens after special tokens: [102, 31, 32, 101, 2, 101, 103]\n",
      "Padded sequence: [[102  31  32 101   2 101 103   0]]\n",
      "Raw prediction: [105  19]\n",
      "\n",
      "Token conversion:\n",
      "Index 19 maps to word: name\n",
      "\n",
      "Final translation: name\n",
      "\n",
      "==================================================\n",
      "Testing: bonjour.\n",
      "\n",
      "Debug tokenization:\n",
      "Original sentence: bonjour.\n",
      "Tokens after initial tokenization: [25, 101]\n",
      "Corresponding words: bonjour <UNK>\n",
      "Tokens after special tokens: [102, 25, 101, 103]\n",
      "Padded sequence: [[102  25 101 103   0   0   0   0]]\n",
      "Raw prediction: [105  19]\n",
      "\n",
      "Token conversion:\n",
      "Index 19 maps to word: name\n",
      "\n",
      "Final translation: name\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer) -> str:\n",
    "    print(\"\\nDebug tokenization:\")\n",
    "    print(f\"Original sentence: {sentence}\")\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    print(f\"Tokens after initial tokenization: {tokens}\")\n",
    "    print(f\"Corresponding words: {fr_tokenizer.sequences_to_texts([tokens])[0]}\")\n",
    "    \n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    print(f\"Tokens after special tokens: {tokens}\")\n",
    "    \n",
    "    padded = pad_sequences([tokens], max_length=max_len_x, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    print(f\"Padded sequence: {padded}\")\n",
    "    \n",
    "    pred = model.predict(padded, max_length=max_seq_len, beam_size=10, temperature=1.0)[0]\n",
    "    print(f\"Raw prediction: {pred}\")\n",
    "    \n",
    "    words = []\n",
    "    print(\"\\nToken conversion:\")\n",
    "    for idx in pred[1:]:  # Skip SOS\n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.eos_token]:\n",
    "            print(f\"Found EOS token at index {idx}\")\n",
    "            break\n",
    "        if idx in [en_tokenizer.word_index[t] for t in [en_tokenizer.pad_token, \n",
    "                                                       en_tokenizer.unk_token, \n",
    "                                                       en_tokenizer.sos_token]]:\n",
    "            print(f\"Skipping special token {idx}\")\n",
    "            continue\n",
    "        word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "        print(f\"Index {idx} maps to word: {word}\")\n",
    "        words.append(word)\n",
    "    \n",
    "    result = \" \".join(words)\n",
    "    print(f\"\\nFinal translation: {result}\")\n",
    "    return result\n",
    "\n",
    "# Test avec des phrases du jeu d'entraînement\n",
    "test_sentences = [\n",
    "    \"je vais bien.\",        # Phrase simple\n",
    "    \"comment allez-vous ?\", # Phrase interrogative\n",
    "    \"bonjour.\"             # Phrase très courte\n",
    "]\n",
    "\n",
    "print(\"Vocabulary sizes:\")\n",
    "print(f\"French vocab size: {len(fr_tokenizer.word_index)}\")\n",
    "print(f\"English vocab size: {len(en_tokenizer.word_index)}\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
