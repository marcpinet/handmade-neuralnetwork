{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.losses import SequenceCrossEntropy\n",
    "from neuralnetlib.optimizers import Adam\n",
    "from neuralnetlib.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcb1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer, \n",
    "             temperature=0.8, beam_size=10, min_length=2) -> str:\n",
    "    print(f\"\\nProcessing: {sentence}\")\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Original words: {[fr_tokenizer.index_word.get(t, '<UNK>') for t in tokens]}\")\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    print(f\"With special tokens: {tokens}\")\n",
    "    \n",
    "    # Padding\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    print(f\"Padded sequence: {padded}\")\n",
    "    \n",
    "    pred = model.predict(padded, \n",
    "                        max_length=model.max_sequence_length,\n",
    "                        beam_size=beam_size,      \n",
    "                        alpha=0.6,         \n",
    "                        min_length=min_length,\n",
    "                        temperature=temperature)[0]\n",
    "    \n",
    "    print(f\"Raw prediction: {pred}\")\n",
    "    \n",
    "    words = []\n",
    "    for idx in pred[1:]:  # Skip SOS\n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.eos_token]:\n",
    "            print(f\"Found EOS token: {idx}\")\n",
    "            break\n",
    "            \n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.pad_token]:\n",
    "            print(f\"Skipping PAD token: {idx}\")\n",
    "            continue\n",
    "            \n",
    "        word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "        print(f\"Token {idx} -> word: {word}\")\n",
    "        words.append(word)\n",
    "    \n",
    "    if not words:\n",
    "        print(\"Warning: Empty translation, using default handling...\")\n",
    "        for idx in pred[1:]:  # Skip SOS\n",
    "            if idx not in [en_tokenizer.word_index[token] for token in \n",
    "                         [en_tokenizer.pad_token, en_tokenizer.eos_token]]:\n",
    "                word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "                words.append(word)\n",
    "    \n",
    "    translation = \" \".join(words) if words else \"[Translation failed]\"\n",
    "    print(f\"Final translation: {translation}\")\n",
    "    return translation\n",
    "\n",
    "class TrainingMonitor(Callback):\n",
    "    def __init__(self, model, fr_tokenizer, en_tokenizer, test_sentences):\n",
    "        self.model = model\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.test_sentences = test_sentences\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:  # Check every 5 epochs\n",
    "            print(f\"\\nEpoch {epoch} validation:\")\n",
    "            for sent in self.test_sentences:\n",
    "                translation = translate(\n",
    "                    sent, self.model, self.fr_tokenizer, self.en_tokenizer,\n",
    "                    temperature=0.8, beam_size=5, min_length=2\n",
    "                )\n",
    "                print(f\"Input: {sent}\")\n",
    "                print(f\"Output: {translation}\\n\")\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"je vais bien\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"bonjour.\",\n",
    "    \"au revoir.\",\n",
    "    \"merci beaucoup.\",\n",
    "    \"s'il vous plaît.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"je vais bien.\",\n",
    "    \"je suis fatigué.\",\n",
    "    \"je suis content.\",\n",
    "    \"quel est votre nom ?\",\n",
    "    \"mon nom est Jean.\",\n",
    "    \"enchanté de vous rencontrer.\",\n",
    "    \"bonne journée.\",\n",
    "    \"bonne soirée.\",\n",
    "    \"à demain.\",\n",
    "    \"j'aime le café.\",\n",
    "    \"je n'aime pas le thé.\",\n",
    "    \"quelle heure est-il ?\",\n",
    "    \"il est trois heures.\",\n",
    "    \"où est la gare ?\",\n",
    "    \"la gare est près d'ici.\",\n",
    "    \"combien ça coûte ?\",\n",
    "    \"c'est trop cher.\",\n",
    "    \"parlez-vous anglais ?\",\n",
    "    \"un peu.\",\n",
    "    \"je ne comprends pas.\",\n",
    "    \"pouvez-vous répéter ?\",\n",
    "    \"je suis désolé.\",\n",
    "    \"pas de problème.\",\n",
    "    \"bon appétit.\",\n",
    "    \"à votre santé.\",\n",
    "    \"j'ai faim.\",\n",
    "    \"j'ai soif.\",\n",
    "    \"il fait beau aujourd'hui.\",\n",
    "    \"il pleut.\",\n",
    "    \"il fait froid.\",\n",
    "    \"il fait chaud.\",\n",
    "    \"je travaille ici.\",\n",
    "    \"où habitez-vous ?\",\n",
    "    \"j'habite à Paris.\",\n",
    "    \"quel âge avez-vous ?\",\n",
    "    \"j'ai vingt-cinq ans.\",\n",
    "    \"avez-vous des frères et sœurs ?\",\n",
    "    \"j'ai une sœur.\",\n",
    "    \"j'ai un chat.\",\n",
    "    \"j'aime voyager.\",\n",
    "    \"je suis étudiant.\",\n",
    "    \"je suis professeur.\",\n",
    "    \"au secours !\",\n",
    "    \"joyeux anniversaire !\",\n",
    "    \"félicitations !\"\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"hello.\",\n",
    "    \"goodbye.\",\n",
    "    \"thank you very much.\",\n",
    "    \"please.\",\n",
    "    \"how are you?\",\n",
    "    \"i am fine.\",\n",
    "    \"i am tired.\",\n",
    "    \"i am happy.\",\n",
    "    \"what is your name?\",\n",
    "    \"my name is John.\",\n",
    "    \"nice to meet you.\",\n",
    "    \"have a nice day.\",\n",
    "    \"have a good evening.\",\n",
    "    \"see you tomorrow.\",\n",
    "    \"i like coffee.\",\n",
    "    \"i don't like tea.\",\n",
    "    \"what time is it?\",\n",
    "    \"it is three o'clock.\",\n",
    "    \"where is the train station?\",\n",
    "    \"the station is nearby.\",\n",
    "    \"how much is it?\",\n",
    "    \"it's too expensive.\",\n",
    "    \"do you speak english?\",\n",
    "    \"a little.\",\n",
    "    \"i don't understand.\",\n",
    "    \"can you repeat?\",\n",
    "    \"i am sorry.\",\n",
    "    \"no problem.\",\n",
    "    \"enjoy your meal.\",\n",
    "    \"cheers.\",\n",
    "    \"i am hungry.\",\n",
    "    \"i am thirsty.\",\n",
    "    \"the weather is nice today.\",\n",
    "    \"it's raining.\",\n",
    "    \"it's cold.\",\n",
    "    \"it's hot.\",\n",
    "    \"i work here.\",\n",
    "    \"where do you live?\",\n",
    "    \"i live in Paris.\",\n",
    "    \"how old are you?\",\n",
    "    \"i am twenty-five years old.\",\n",
    "    \"do you have brothers and sisters?\",\n",
    "    \"i have a sister.\",\n",
    "    \"i have a cat.\",\n",
    "    \"i like to travel.\",\n",
    "    \"i am a student.\",\n",
    "    \"i am a teacher.\",\n",
    "    \"help!\",\n",
    "    \"happy birthday!\",\n",
    "    \"congratulations!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "en_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 95, vocab_size_fr: 107\n",
      "max_len_x: 8, max_len_y: 8, max_vocab_size: 107, max_seq_len: 8\n",
      "French sentences:\n",
      "['bonjour.', 'au revoir.', 'merci beaucoup.', \"s'il vous plaît.\", 'comment allez-vous ?', 'je vais bien.', 'je suis fatigué.', 'je suis content.', 'quel est votre nom ?', 'mon nom est Jean.', 'enchanté de vous rencontrer.', 'bonne journée.', 'bonne soirée.', 'à demain.', \"j'aime le café.\", \"je n'aime pas le thé.\", 'quelle heure est-il ?', 'il est trois heures.', 'où est la gare ?', \"la gare est près d'ici.\", 'combien ça coûte ?', \"c'est trop cher.\", 'parlez-vous anglais ?', 'un peu.', 'je ne comprends pas.', 'pouvez-vous répéter ?', 'je suis désolé.', 'pas de problème.', 'bon appétit.', 'à votre santé.', \"j'ai faim.\", \"j'ai soif.\", \"il fait beau aujourd'hui.\", 'il pleut.', 'il fait froid.', 'il fait chaud.', 'je travaille ici.', 'où habitez-vous ?', \"j'habite à Paris.\", 'quel âge avez-vous ?', \"j'ai vingt-cinq ans.\", 'avez-vous des frères et sœurs ?', \"j'ai une sœur.\", \"j'ai un chat.\", \"j'aime voyager.\", 'je suis étudiant.', 'je suis professeur.', 'au secours !', 'joyeux anniversaire !', 'félicitations !']\n",
      "English sentences:\n",
      "['hello.', 'goodbye.', 'thank you very much.', 'please.', 'how are you?', 'i am fine.', 'i am tired.', 'i am happy.', 'what is your name?', 'my name is John.', 'nice to meet you.', 'have a nice day.', 'have a good evening.', 'see you tomorrow.', 'i like coffee.', \"i don't like tea.\", 'what time is it?', \"it is three o'clock.\", 'where is the train station?', 'the station is nearby.', 'how much is it?', \"it's too expensive.\", 'do you speak english?', 'a little.', \"i don't understand.\", 'can you repeat?', 'i am sorry.', 'no problem.', 'enjoy your meal.', 'cheers.', 'i am hungry.', 'i am thirsty.', 'the weather is nice today.', \"it's raining.\", \"it's cold.\", \"it's hot.\", 'i work here.', 'where do you live?', 'i live in Paris.', 'how old are you?', 'i am twenty-five years old.', 'do you have brothers and sisters?', 'i have a sister.', 'i have a cat.', 'i like to travel.', 'i am a student.', 'i am a teacher.', 'help!', 'happy birthday!', 'congratulations!']\n",
      "French tokenizer:\n",
      "{'<PAD>': 0, '.': 1, '?': 2, 'je': 3, 'vous': 4, '-': 5, \"j'\": 6, 'il': 7, 'est': 8, 'suis': 9, 'ai': 10, 'à': 11, 'aime': 12, 'pas': 13, 'fait': 14, '!': 15, 'au': 16, 'quel': 17, 'votre': 18, 'nom': 19, 'de': 20, 'bonne': 21, 'le': 22, 'où': 23, 'la': 24, 'gare': 25, 'ici': 26, 'un': 27, 'avez': 28, 'bonjour': 29, 'revoir': 30, 'merci': 31, 'beaucoup': 32, \"s'\": 33, 'plaît': 34, 'comment': 35, 'allez': 36, 'vais': 37, 'bien': 38, 'fatigué': 39, 'content': 40, 'mon': 41, 'jean': 42, 'enchanté': 43, 'rencontrer': 44, 'journée': 45, 'soirée': 46, 'demain': 47, 'café': 48, \"n'\": 49, 'thé': 50, 'quelle': 51, 'heure': 52, 'trois': 53, 'heures': 54, 'près': 55, \"d'\": 56, 'combien': 57, 'ça': 58, 'coûte': 59, \"c'\": 60, 'trop': 61, 'cher': 62, 'parlez': 63, 'anglais': 64, 'peu': 65, 'ne': 66, 'comprends': 67, 'pouvez': 68, 'répéter': 69, 'désolé': 70, 'problème': 71, 'bon': 72, 'appétit': 73, 'santé': 74, 'faim': 75, 'soif': 76, 'beau': 77, \"aujourd'hui\": 78, 'pleut': 79, 'froid': 80, 'chaud': 81, 'travaille': 82, 'habitez': 83, 'habite': 84, 'paris': 85, 'âge': 86, 'vingt': 87, 'cinq': 88, 'ans': 89, 'des': 90, 'frères': 91, 'et': 92, 'sœurs': 93, 'une': 94, 'sœur': 95, 'chat': 96, 'voyager': 97, 'étudiant': 98, 'professeur': 99, 'secours': 100, 'joyeux': 101, 'anniversaire': 102, 'félicitations': 103, '<UNK>': 105, '<SOS>': 106, '<EOS>': 107}\n",
      "English tokenizer:\n",
      "{'<PAD>': 0, '.': 1, 'i': 2, '?': 3, 'you': 4, 'am': 5, 'is': 6, 'a': 7, 'have': 8, \"it's\": 9, 'how': 10, 'nice': 11, 'like': 12, 'it': 13, 'the': 14, 'do': 15, '!': 16, 'much': 17, 'are': 18, 'happy': 19, 'what': 20, 'your': 21, 'name': 22, 'to': 23, \"don't\": 24, 'where': 25, 'station': 26, 'live': 27, 'old': 28, 'hello': 29, 'goodbye': 30, 'thank': 31, 'very': 32, 'please': 33, 'fine': 34, 'tired': 35, 'my': 36, 'john': 37, 'meet': 38, 'day': 39, 'good': 40, 'evening': 41, 'see': 42, 'tomorrow': 43, 'coffee': 44, 'tea': 45, 'time': 46, 'three': 47, \"o'\": 48, 'clock': 49, 'train': 50, 'nearby': 51, 'too': 52, 'expensive': 53, 'speak': 54, 'english': 55, 'little': 56, 'understand': 57, 'can': 58, 'repeat': 59, 'sorry': 60, 'no': 61, 'problem': 62, 'enjoy': 63, 'meal': 64, 'cheers': 65, 'hungry': 66, 'thirsty': 67, 'weather': 68, 'today': 69, 'raining': 70, 'cold': 71, 'hot': 72, 'work': 73, 'here': 74, 'in': 75, 'paris': 76, 'twenty': 77, '-': 78, 'five': 79, 'years': 80, 'brothers': 81, 'and': 82, 'sisters': 83, 'sister': 84, 'cat': 85, 'travel': 86, 'student': 87, 'teacher': 88, 'help': 89, 'birthday': 90, 'congratulations': 91, '<UNK>': 93, '<SOS>': 94, '<EOS>': 95}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  src_vocab_size=107,\n",
      "  tgt_vocab_size=95,\n",
      "  d_model=256,\n",
      "  n_heads=8,\n",
      "  n_encoder_layers=4,\n",
      "  n_decoder_layers=4,\n",
      "  d_ff=1024,\n",
      "  dropout_rate=0.3,\n",
      "  max_sequence_length=8\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size_fr,\n",
    "    tgt_vocab_size=vocab_size_en,\n",
    "    d_model=256,        \n",
    "    n_heads=8,         \n",
    "    n_encoder_layers=4,\n",
    "    n_decoder_layers=4,\n",
    "    d_ff=1024,           \n",
    "    dropout_rate=0.3,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    random_state=42,\n",
    "    src_unk_idx=fr_tokenizer.word_index[fr_tokenizer.unk_token],\n",
    "    src_sos_idx=fr_tokenizer.word_index[fr_tokenizer.sos_token],\n",
    "    src_eos_idx=fr_tokenizer.word_index[fr_tokenizer.eos_token],\n",
    "    tgt_unk_idx=en_tokenizer.word_index[en_tokenizer.unk_token],\n",
    "    tgt_sos_idx=en_tokenizer.word_index[en_tokenizer.sos_token],\n",
    "    tgt_eos_idx=en_tokenizer.word_index[en_tokenizer.eos_token],\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function=SequenceCrossEntropy(\n",
    "        label_smoothing=0.1,\n",
    "    ),\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9\n",
    "    ),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527483e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting training data:\n",
      "\n",
      "Example 1:\n",
      "Input sequence:\n",
      "Raw: [106  29   1 107   0   0   0   0]\n",
      "Tokens: ['<SOS>', 'bonjour', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [94 29  1 95  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'hello', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 2:\n",
      "Input sequence:\n",
      "Raw: [106  16  30   1 107   0   0   0]\n",
      "Tokens: ['<SOS>', 'au', 'revoir', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [94 30  1 95  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'goodbye', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 3:\n",
      "Input sequence:\n",
      "Raw: [106  31  32   1 107   0   0   0]\n",
      "Tokens: ['<SOS>', 'merci', 'beaucoup', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [94 31  4 32 17  1 95  0]\n",
      "Tokens: ['<SOS>', 'thank', 'you', 'very', 'much', '.', '<EOS>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "def inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer):\n",
    "    print(\"\\nInspecting training data:\")\n",
    "    for i in range(min(3, len(x_train_padded))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"Input sequence:\")\n",
    "        print(f\"Raw: {x_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [fr_tokenizer.index_word.get(idx, '<UNK>') for idx in x_train_padded[i]])\n",
    "        \n",
    "        print(\"\\nOutput sequence:\")\n",
    "        print(f\"Raw: {y_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [en_tokenizer.index_word.get(idx, '<UNK>') for idx in y_train_padded[i]])\n",
    "\n",
    "inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/20 - loss: 12.1243 -  - 1.78s\n",
      "Epoch 0 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [94 62 62 62 62 62 62 62 95]\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Found EOS token: 95\n",
      "Final translation: problem problem problem problem problem problem problem\n",
      "Input: je vais bien\n",
      "Output: problem problem problem problem problem problem problem\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [94 62 62 62 62 62 62 62 95]\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Token 62 -> word: problem\n",
      "Found EOS token: 95\n",
      "Final translation: problem problem problem problem problem problem problem\n",
      "Input: comment allez-vous ?\n",
      "Output: problem problem problem problem problem problem problem\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [94 60 59 60 60 60 60 60 95]\n",
      "Token 60 -> word: sorry\n",
      "Token 59 -> word: repeat\n",
      "Token 60 -> word: sorry\n",
      "Token 60 -> word: sorry\n",
      "Token 60 -> word: sorry\n",
      "Token 60 -> word: sorry\n",
      "Token 60 -> word: sorry\n",
      "Found EOS token: 95\n",
      "Final translation: sorry repeat sorry sorry sorry sorry sorry\n",
      "Input: bonjour\n",
      "Output: sorry repeat sorry sorry sorry sorry sorry\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 2/20 - loss: 8.1549 -  - 1.69s\n",
      "[==============================] 100% Epoch 3/20 - loss: 4.5677 -  - 1.67s\n",
      "[==============================] 100% Epoch 4/20 - loss: 3.3121 -  - 1.62s\n",
      "[==============================] 100% Epoch 5/20 - loss: 1.7997 -  - 1.65s\n",
      "[==============================] 100% Epoch 6/20 - loss: 0.7629 -  - 1.62s\n",
      "Epoch 5 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [94 75 75 75 75 75 75 75 95]\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Found EOS token: 95\n",
      "Final translation: in in in in in in in\n",
      "Input: je vais bien\n",
      "Output: in in in in in in in\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [94 75 75 75 75 75 75 75 95]\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Found EOS token: 95\n",
      "Final translation: in in in in in in in\n",
      "Input: comment allez-vous ?\n",
      "Output: in in in in in in in\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [94 75 75 75 75 75 75 75 95]\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Found EOS token: 95\n",
      "Final translation: in in in in in in in\n",
      "Input: bonjour\n",
      "Output: in in in in in in in\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 7/20 - loss: 0.7170 -  - 1.62s\n",
      "[==============================] 100% Epoch 8/20 - loss: 0.5159 -  - 1.63s\n",
      "[==============================] 100% Epoch 9/20 - loss: 0.3170 -  - 1.62s\n",
      "[==============================] 100% Epoch 10/20 - loss: 0.1330 -  - 1.64s\n",
      "[==============================] 100% Epoch 11/20 - loss: 0.1112 -  - 1.62s\n",
      "Epoch 10 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [94 71 71 71 71 71 71 71 95]\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Found EOS token: 95\n",
      "Final translation: cold cold cold cold cold cold cold\n",
      "Input: je vais bien\n",
      "Output: cold cold cold cold cold cold cold\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [94 71 71 71 71 71 71 71 95]\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Found EOS token: 95\n",
      "Final translation: cold cold cold cold cold cold cold\n",
      "Input: comment allez-vous ?\n",
      "Output: cold cold cold cold cold cold cold\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [94 71 71 71 71 71 71 71 95]\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Token 71 -> word: cold\n",
      "Found EOS token: 95\n",
      "Final translation: cold cold cold cold cold cold cold\n",
      "Input: bonjour\n",
      "Output: cold cold cold cold cold cold cold\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 12/20 - loss: 0.0980 -  - 1.67s\n",
      "[==============================] 100% Epoch 13/20 - loss: 0.0879 -  - 1.64s\n",
      "[==============================] 100% Epoch 14/20 - loss: 0.0801 -  - 1.63s\n",
      "[==============================] 100% Epoch 15/20 - loss: 0.0745 -  - 1.61s\n",
      "[==============================] 100% Epoch 16/20 - loss: 0.0698 -  - 1.67s\n",
      "Epoch 15 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [94 69 69 69 69 69 69 69 95]\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Found EOS token: 95\n",
      "Final translation: today today today today today today today\n",
      "Input: je vais bien\n",
      "Output: today today today today today today today\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [94 69 69 69 69 69 69 69 95]\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Found EOS token: 95\n",
      "Final translation: today today today today today today today\n",
      "Input: comment allez-vous ?\n",
      "Output: today today today today today today today\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [94 69 69 69 69 69 69 69 95]\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Found EOS token: 95\n",
      "Final translation: today today today today today today today\n",
      "Input: bonjour\n",
      "Output: today today today today today today today\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 17/20 - loss: 0.0655 -  - 1.66s\n",
      "[==============================] 100% Epoch 18/20 - loss: 0.0611 -  - 1.68s\n",
      "[==============================] 100% Epoch 19/20 - loss: 0.0575 -  - 1.65s\n",
      "[==============================] 100% Epoch 20/20 - loss: 0.0539 -  - 1.71s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monitor = TrainingMonitor(model, fr_tokenizer, en_tokenizer, test_sentences)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=20,\n",
    "    batch_size=12,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss', patience=20),\n",
    "        monitor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes:\n",
      "French vocab size: 107\n",
      "English vocab size: 95\n",
      "\n",
      "==================================================\n",
      "Testing: je vais bien\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [94 69 69 69 69 69 69 69 95]\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Found EOS token: 95\n",
      "Final translation: today today today today today today today\n",
      "Translation: today today today today today today today\n",
      "\n",
      "==================================================\n",
      "Testing: comment allez-vous ?\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [94 69 69 69 69 69 69 69 95]\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Found EOS token: 95\n",
      "Final translation: today today today today today today today\n",
      "Translation: today today today today today today today\n",
      "\n",
      "==================================================\n",
      "Testing: bonjour\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [94 69 69 69 69 69 69 69 95]\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Token 69 -> word: today\n",
      "Found EOS token: 95\n",
      "Final translation: today today today today today today today\n",
      "Translation: today today today today today today today\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary sizes:\")\n",
    "print(f\"French vocab size: {len(fr_tokenizer.word_index)}\")\n",
    "print(f\"English vocab size: {len(en_tokenizer.word_index)}\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer, \n",
    "                      temperature=0.8,\n",
    "                      beam_size=10,\n",
    "                      min_length=2) \n",
    "    print(f\"Translation: {translation}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e205b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: je vais bien\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 8, 7, 8)\n",
      "First attention head values:\n",
      "[[0.19994144 0.19994379 0.20008439 0.20006183 0.19996854 0.\n",
      "  0.         0.        ]\n",
      " [0.19994144 0.19994379 0.20008439 0.20006183 0.19996855 0.\n",
      "  0.         0.        ]\n",
      " [0.19994144 0.19994379 0.20008439 0.20006183 0.19996855 0.\n",
      "  0.         0.        ]\n",
      " [0.19994144 0.19994379 0.20008439 0.20006183 0.19996855 0.\n",
      "  0.         0.        ]\n",
      " [0.19994144 0.19994379 0.20008439 0.20006183 0.19996855 0.\n",
      "  0.         0.        ]\n",
      " [0.19994144 0.19994379 0.20008439 0.20006183 0.19996855 0.\n",
      "  0.         0.        ]\n",
      " [0.19994144 0.19994379 0.20008439 0.20006183 0.19996854 0.\n",
      "  0.         0.        ]]\n",
      "\n",
      "Analyzing: comment allez-vous ?\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 8, 7, 8)\n",
      "First attention head values:\n",
      "[[0.19994183 0.19994643 0.2000777  0.20006477 0.19996926 0.\n",
      "  0.         0.        ]\n",
      " [0.19994183 0.19994643 0.2000777  0.20006477 0.19996927 0.\n",
      "  0.         0.        ]\n",
      " [0.19994183 0.19994643 0.2000777  0.20006477 0.19996927 0.\n",
      "  0.         0.        ]\n",
      " [0.19994183 0.19994643 0.20007771 0.20006477 0.19996927 0.\n",
      "  0.         0.        ]\n",
      " [0.19994183 0.19994643 0.2000777  0.20006477 0.19996927 0.\n",
      "  0.         0.        ]\n",
      " [0.19994183 0.19994643 0.2000777  0.20006477 0.19996927 0.\n",
      "  0.         0.        ]\n",
      " [0.19994183 0.19994643 0.2000777  0.20006477 0.19996927 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def analyze_attention_weights(model, input_sentence, fr_tokenizer):\n",
    "    tokens = fr_tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    \n",
    "    _ = model.predict(padded)\n",
    "    \n",
    "    attention_weights = model.decoder_layers[-1].cross_attention.attention_weights\n",
    "    \n",
    "    print(\"\\nAttention Analysis:\")\n",
    "    print(f\"Attention shape: {attention_weights.shape}\")\n",
    "    print(\"First attention head values:\")\n",
    "    print(attention_weights[0, 0])\n",
    "\n",
    "for sent in test_sentences[:2]:\n",
    "    print(f\"\\nAnalyzing: {sent}\")\n",
    "    analyze_attention_weights(model, sent, fr_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
