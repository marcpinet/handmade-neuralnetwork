{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.losses import SequenceCrossEntropy\n",
    "from neuralnetlib.optimizers import Adam\n",
    "from neuralnetlib.callbacks import EarlyStopping, Callback, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcb1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer, \n",
    "             temperature=0.8, beam_size=10, min_length=2) -> str:\n",
    "    print(f\"\\nProcessing: {sentence}\")\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Original words: {[fr_tokenizer.index_word.get(t, '<UNK>') for t in tokens]}\")\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    print(f\"With special tokens: {tokens}\")\n",
    "    \n",
    "    # Padding\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    print(f\"Padded sequence: {padded}\")\n",
    "    \n",
    "    pred = model.predict(padded, \n",
    "                        max_length=model.max_sequence_length,\n",
    "                        beam_size=beam_size,      \n",
    "                        alpha=0.6,         \n",
    "                        min_length=min_length,\n",
    "                        temperature=temperature)[0]\n",
    "    \n",
    "    print(f\"Raw prediction: {pred}\")\n",
    "    \n",
    "    words = []\n",
    "    for idx in pred[1:]:  # Skip SOS\n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.eos_token]:\n",
    "            print(f\"Found EOS token: {idx}\")\n",
    "            break\n",
    "            \n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.pad_token]:\n",
    "            print(f\"Skipping PAD token: {idx}\")\n",
    "            continue\n",
    "            \n",
    "        word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "        print(f\"Token {idx} -> word: {word}\")\n",
    "        words.append(word)\n",
    "    \n",
    "    if not words:\n",
    "        print(\"Warning: Empty translation, using default handling...\")\n",
    "        for idx in pred[1:]:  # Skip SOS\n",
    "            if idx not in [en_tokenizer.word_index[token] for token in \n",
    "                         [en_tokenizer.pad_token, en_tokenizer.eos_token]]:\n",
    "                word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "                words.append(word)\n",
    "    \n",
    "    translation = \" \".join(words) if words else \"[Translation failed]\"\n",
    "    print(f\"Final translation: {translation}\")\n",
    "    return translation\n",
    "\n",
    "class TrainingMonitor(Callback):\n",
    "    def __init__(self, model, fr_tokenizer, en_tokenizer, test_sentences):\n",
    "        self.model = model\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.test_sentences = test_sentences\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:  # Check every 5 epochs\n",
    "            print(f\"\\nEpoch {epoch} validation:\")\n",
    "            for sent in self.test_sentences:\n",
    "                translation = translate(\n",
    "                    sent, self.model, self.fr_tokenizer, self.en_tokenizer,\n",
    "                    temperature=0.8, beam_size=5, min_length=2\n",
    "                )\n",
    "                print(f\"Input: {sent}\")\n",
    "                print(f\"Output: {translation}\\n\")\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"je vais bien\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"bonjour.\",\n",
    "    \"au revoir.\",\n",
    "    \"merci beaucoup.\",\n",
    "    \"s'il vous plaît.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"je vais bien.\",\n",
    "    \"je suis fatigué.\",\n",
    "    \"je suis content.\",\n",
    "    \"quel est votre nom ?\",\n",
    "    \"mon nom est Jean.\",\n",
    "    \"enchanté de vous rencontrer.\",\n",
    "    \"bonne journée.\",\n",
    "    \"bonne soirée.\",\n",
    "    \"à demain.\",\n",
    "    \"j'aime le café.\",\n",
    "    \"je n'aime pas le thé.\",\n",
    "    \"quelle heure est-il ?\",\n",
    "    \"il est trois heures.\",\n",
    "    \"où est la gare ?\",\n",
    "    \"la gare est près d'ici.\",\n",
    "    \"combien ça coûte ?\",\n",
    "    \"c'est trop cher.\",\n",
    "    \"parlez-vous anglais ?\",\n",
    "    \"un peu.\",\n",
    "    \"je ne comprends pas.\",\n",
    "    \"pouvez-vous répéter ?\",\n",
    "    \"je suis désolé.\",\n",
    "    \"pas de problème.\",\n",
    "    \"bon appétit.\",\n",
    "    \"à votre santé.\",\n",
    "    \"j'ai faim.\",\n",
    "    \"j'ai soif.\",\n",
    "    \"il fait beau aujourd'hui.\",\n",
    "    \"il pleut.\",\n",
    "    \"il fait froid.\",\n",
    "    \"il fait chaud.\",\n",
    "    \"je travaille ici.\",\n",
    "    \"où habitez-vous ?\",\n",
    "    \"j'habite à Paris.\",\n",
    "    \"quel âge avez-vous ?\",\n",
    "    \"j'ai vingt-cinq ans.\",\n",
    "    \"avez-vous des frères et sœurs ?\",\n",
    "    \"j'ai une sœur.\",\n",
    "    \"j'ai un chat.\",\n",
    "    \"j'aime voyager.\",\n",
    "    \"je suis étudiant.\",\n",
    "    \"je suis professeur.\",\n",
    "    \"au secours !\",\n",
    "    \"joyeux anniversaire !\",\n",
    "    \"félicitations !\"\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"hello.\",\n",
    "    \"goodbye.\",\n",
    "    \"thank you very much.\",\n",
    "    \"please.\",\n",
    "    \"how are you?\",\n",
    "    \"i am fine.\",\n",
    "    \"i am tired.\",\n",
    "    \"i am happy.\",\n",
    "    \"what is your name?\",\n",
    "    \"my name is John.\",\n",
    "    \"nice to meet you.\",\n",
    "    \"have a nice day.\",\n",
    "    \"have a good evening.\",\n",
    "    \"see you tomorrow.\",\n",
    "    \"i like coffee.\",\n",
    "    \"i don't like tea.\",\n",
    "    \"what time is it?\",\n",
    "    \"it is three o'clock.\",\n",
    "    \"where is the train station?\",\n",
    "    \"the station is nearby.\",\n",
    "    \"how much is it?\",\n",
    "    \"it's too expensive.\",\n",
    "    \"do you speak english?\",\n",
    "    \"a little.\",\n",
    "    \"i don't understand.\",\n",
    "    \"can you repeat?\",\n",
    "    \"i am sorry.\",\n",
    "    \"no problem.\",\n",
    "    \"enjoy your meal.\",\n",
    "    \"cheers.\",\n",
    "    \"i am hungry.\",\n",
    "    \"i am thirsty.\",\n",
    "    \"the weather is nice today.\",\n",
    "    \"it's raining.\",\n",
    "    \"it's cold.\",\n",
    "    \"it's hot.\",\n",
    "    \"i work here.\",\n",
    "    \"where do you live?\",\n",
    "    \"i live in Paris.\",\n",
    "    \"how old are you?\",\n",
    "    \"i am twenty-five years old.\",\n",
    "    \"do you have brothers and sisters?\",\n",
    "    \"i have a sister.\",\n",
    "    \"i have a cat.\",\n",
    "    \"i like to travel.\",\n",
    "    \"i am a student.\",\n",
    "    \"i am a teacher.\",\n",
    "    \"help!\",\n",
    "    \"happy birthday!\",\n",
    "    \"congratulations!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "en_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 95, vocab_size_fr: 107\n",
      "max_len_x: 8, max_len_y: 8, max_vocab_size: 107, max_seq_len: 8\n",
      "French sentences:\n",
      "['bonjour.', 'au revoir.', 'merci beaucoup.', \"s'il vous plaît.\", 'comment allez-vous ?', 'je vais bien.', 'je suis fatigué.', 'je suis content.', 'quel est votre nom ?', 'mon nom est Jean.', 'enchanté de vous rencontrer.', 'bonne journée.', 'bonne soirée.', 'à demain.', \"j'aime le café.\", \"je n'aime pas le thé.\", 'quelle heure est-il ?', 'il est trois heures.', 'où est la gare ?', \"la gare est près d'ici.\", 'combien ça coûte ?', \"c'est trop cher.\", 'parlez-vous anglais ?', 'un peu.', 'je ne comprends pas.', 'pouvez-vous répéter ?', 'je suis désolé.', 'pas de problème.', 'bon appétit.', 'à votre santé.', \"j'ai faim.\", \"j'ai soif.\", \"il fait beau aujourd'hui.\", 'il pleut.', 'il fait froid.', 'il fait chaud.', 'je travaille ici.', 'où habitez-vous ?', \"j'habite à Paris.\", 'quel âge avez-vous ?', \"j'ai vingt-cinq ans.\", 'avez-vous des frères et sœurs ?', \"j'ai une sœur.\", \"j'ai un chat.\", \"j'aime voyager.\", 'je suis étudiant.', 'je suis professeur.', 'au secours !', 'joyeux anniversaire !', 'félicitations !']\n",
      "English sentences:\n",
      "['hello.', 'goodbye.', 'thank you very much.', 'please.', 'how are you?', 'i am fine.', 'i am tired.', 'i am happy.', 'what is your name?', 'my name is John.', 'nice to meet you.', 'have a nice day.', 'have a good evening.', 'see you tomorrow.', 'i like coffee.', \"i don't like tea.\", 'what time is it?', \"it is three o'clock.\", 'where is the train station?', 'the station is nearby.', 'how much is it?', \"it's too expensive.\", 'do you speak english?', 'a little.', \"i don't understand.\", 'can you repeat?', 'i am sorry.', 'no problem.', 'enjoy your meal.', 'cheers.', 'i am hungry.', 'i am thirsty.', 'the weather is nice today.', \"it's raining.\", \"it's cold.\", \"it's hot.\", 'i work here.', 'where do you live?', 'i live in Paris.', 'how old are you?', 'i am twenty-five years old.', 'do you have brothers and sisters?', 'i have a sister.', 'i have a cat.', 'i like to travel.', 'i am a student.', 'i am a teacher.', 'help!', 'happy birthday!', 'congratulations!']\n",
      "French tokenizer:\n",
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, '.': 4, '?': 5, 'je': 6, 'vous': 7, '-': 8, \"j'\": 9, 'il': 10, 'est': 11, 'suis': 12, 'ai': 13, 'à': 14, 'aime': 15, 'pas': 16, 'fait': 17, '!': 18, 'au': 19, 'quel': 20, 'votre': 21, 'nom': 22, 'de': 23, 'bonne': 24, 'le': 25, 'où': 26, 'la': 27, 'gare': 28, 'ici': 29, 'un': 30, 'avez': 31, 'bonjour': 32, 'revoir': 33, 'merci': 34, 'beaucoup': 35, \"s'\": 36, 'plaît': 37, 'comment': 38, 'allez': 39, 'vais': 40, 'bien': 41, 'fatigué': 42, 'content': 43, 'mon': 44, 'jean': 45, 'enchanté': 46, 'rencontrer': 47, 'journée': 48, 'soirée': 49, 'demain': 50, 'café': 51, \"n'\": 52, 'thé': 53, 'quelle': 54, 'heure': 55, 'trois': 56, 'heures': 57, 'près': 58, \"d'\": 59, 'combien': 60, 'ça': 61, 'coûte': 62, \"c'\": 63, 'trop': 64, 'cher': 65, 'parlez': 66, 'anglais': 67, 'peu': 68, 'ne': 69, 'comprends': 70, 'pouvez': 71, 'répéter': 72, 'désolé': 73, 'problème': 74, 'bon': 75, 'appétit': 76, 'santé': 77, 'faim': 78, 'soif': 79, 'beau': 80, \"aujourd'hui\": 81, 'pleut': 82, 'froid': 83, 'chaud': 84, 'travaille': 85, 'habitez': 86, 'habite': 87, 'paris': 88, 'âge': 89, 'vingt': 90, 'cinq': 91, 'ans': 92, 'des': 93, 'frères': 94, 'et': 95, 'sœurs': 96, 'une': 97, 'sœur': 98, 'chat': 99, 'voyager': 100, 'étudiant': 101, 'professeur': 102, 'secours': 103, 'joyeux': 104, 'anniversaire': 105, 'félicitations': 106}\n",
      "English tokenizer:\n",
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, '.': 4, 'i': 5, '?': 6, 'you': 7, 'am': 8, 'is': 9, 'a': 10, 'have': 11, \"it's\": 12, 'how': 13, 'nice': 14, 'like': 15, 'it': 16, 'the': 17, 'do': 18, '!': 19, 'much': 20, 'are': 21, 'happy': 22, 'what': 23, 'your': 24, 'name': 25, 'to': 26, \"don't\": 27, 'where': 28, 'station': 29, 'live': 30, 'old': 31, 'hello': 32, 'goodbye': 33, 'thank': 34, 'very': 35, 'please': 36, 'fine': 37, 'tired': 38, 'my': 39, 'john': 40, 'meet': 41, 'day': 42, 'good': 43, 'evening': 44, 'see': 45, 'tomorrow': 46, 'coffee': 47, 'tea': 48, 'time': 49, 'three': 50, \"o'\": 51, 'clock': 52, 'train': 53, 'nearby': 54, 'too': 55, 'expensive': 56, 'speak': 57, 'english': 58, 'little': 59, 'understand': 60, 'can': 61, 'repeat': 62, 'sorry': 63, 'no': 64, 'problem': 65, 'enjoy': 66, 'meal': 67, 'cheers': 68, 'hungry': 69, 'thirsty': 70, 'weather': 71, 'today': 72, 'raining': 73, 'cold': 74, 'hot': 75, 'work': 76, 'here': 77, 'in': 78, 'paris': 79, 'twenty': 80, '-': 81, 'five': 82, 'years': 83, 'brothers': 84, 'and': 85, 'sisters': 86, 'sister': 87, 'cat': 88, 'travel': 89, 'student': 90, 'teacher': 91, 'help': 92, 'birthday': 93, 'congratulations': 94}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  src_vocab_size=107,\n",
      "  tgt_vocab_size=95,\n",
      "  d_model=256,\n",
      "  n_heads=8,\n",
      "  n_encoder_layers=4,\n",
      "  n_decoder_layers=4,\n",
      "  d_ff=1024,\n",
      "  dropout_rate=0.3,\n",
      "  max_sequence_length=8\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size_fr,\n",
    "    tgt_vocab_size=vocab_size_en,\n",
    "    d_model=256,        \n",
    "    n_heads=8,         \n",
    "    n_encoder_layers=4,\n",
    "    n_decoder_layers=4,\n",
    "    d_ff=1024,           \n",
    "    dropout_rate=0.3,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function=SequenceCrossEntropy(\n",
    "        label_smoothing=0.1,\n",
    "    ),\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9\n",
    "    ),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527483e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting training data:\n",
      "\n",
      "Example 1:\n",
      "Input sequence:\n",
      "Raw: [ 2 32  4  3  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'bonjour', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 2 32  4  3  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'hello', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 2:\n",
      "Input sequence:\n",
      "Raw: [ 2 19 33  4  3  0  0  0]\n",
      "Tokens: ['<SOS>', 'au', 'revoir', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 2 33  4  3  0  0  0  0]\n",
      "Tokens: ['<SOS>', 'goodbye', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Example 3:\n",
      "Input sequence:\n",
      "Raw: [ 2 34 35  4  3  0  0  0]\n",
      "Tokens: ['<SOS>', 'merci', 'beaucoup', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 2 34  7 35 20  4  3  0]\n",
      "Tokens: ['<SOS>', 'thank', 'you', 'very', 'much', '.', '<EOS>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "def inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer):\n",
    "    print(\"\\nInspecting training data:\")\n",
    "    for i in range(min(3, len(x_train_padded))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"Input sequence:\")\n",
    "        print(f\"Raw: {x_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [fr_tokenizer.index_word.get(idx, '<UNK>') for idx in x_train_padded[i]])\n",
    "        \n",
    "        print(\"\\nOutput sequence:\")\n",
    "        print(f\"Raw: {y_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [en_tokenizer.index_word.get(idx, '<UNK>') for idx in y_train_padded[i]])\n",
    "\n",
    "inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/20 - loss: 11.3915 -  - 1.44s\n",
      "Epoch 0 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 74 73 76 69 73 73 76  3]\n",
      "Token 74 -> word: cold\n",
      "Token 73 -> word: raining\n",
      "Token 76 -> word: work\n",
      "Token 69 -> word: hungry\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: cold raining work hungry raining raining work\n",
      "Input: je vais bien\n",
      "Output: cold raining work hungry raining raining work\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 78 75 76 69 73 73 76  3]\n",
      "Token 78 -> word: in\n",
      "Token 75 -> word: hot\n",
      "Token 76 -> word: work\n",
      "Token 69 -> word: hungry\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: in hot work hungry raining raining work\n",
      "Input: comment allez-vous ?\n",
      "Output: in hot work hungry raining raining work\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 73 73 76 72 73 76 76  3]\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 76 -> word: work\n",
      "Token 72 -> word: today\n",
      "Token 73 -> word: raining\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: raining raining work today raining work work\n",
      "Input: bonjour\n",
      "Output: raining raining work today raining work work\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 2/20 - loss: 7.1450 -  - 1.31s\n",
      "[==============================] 100% Epoch 3/20 - loss: 3.9769 -  - 1.34s\n",
      "[==============================] 100% Epoch 4/20 - loss: 2.0568 -  - 1.37s\n",
      "[==============================] 100% Epoch 5/20 - loss: 1.4395 -  - 1.34s\n",
      "[==============================] 100% Epoch 6/20 - loss: 1.1793 -  - 1.26s\n",
      "Epoch 5 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 73 75 76 73 73 75 75  3]\n",
      "Token 73 -> word: raining\n",
      "Token 75 -> word: hot\n",
      "Token 76 -> word: work\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 75 -> word: hot\n",
      "Token 75 -> word: hot\n",
      "Found EOS token: 3\n",
      "Final translation: raining hot work raining raining hot hot\n",
      "Input: je vais bien\n",
      "Output: raining hot work raining raining hot hot\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 74 76 77 74 74 76 76  3]\n",
      "Token 74 -> word: cold\n",
      "Token 76 -> word: work\n",
      "Token 77 -> word: here\n",
      "Token 74 -> word: cold\n",
      "Token 74 -> word: cold\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: cold work here cold cold work work\n",
      "Input: comment allez-vous ?\n",
      "Output: cold work here cold cold work work\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 69 73 73 68 73 73 73  3]\n",
      "Token 69 -> word: hungry\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 68 -> word: cheers\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Found EOS token: 3\n",
      "Final translation: hungry raining raining cheers raining raining raining\n",
      "Input: bonjour\n",
      "Output: hungry raining raining cheers raining raining raining\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 7/20 - loss: 1.0879 -  - 1.27s\n",
      "[==============================] 100% Epoch 8/20 - loss: 0.6378 -  - 1.28s\n",
      "[==============================] 100% Epoch 9/20 - loss: 0.5606 -  - 1.25s\n",
      "[==============================] 100% Epoch 10/20 - loss: 0.4282 -  - 1.23s\n",
      "[==============================] 100% Epoch 11/20 - loss: 0.2802 -  - 1.22s\n",
      "Epoch 10 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 76 76 76 76 76 76 76  3]\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: work work work work work work work\n",
      "Input: je vais bien\n",
      "Output: work work work work work work work\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 76 76 76 76 76 76 76  3]\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: work work work work work work work\n",
      "Input: comment allez-vous ?\n",
      "Output: work work work work work work work\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 76 76 76 76 76 76 76  3]\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Token 76 -> word: work\n",
      "Found EOS token: 3\n",
      "Final translation: work work work work work work work\n",
      "Input: bonjour\n",
      "Output: work work work work work work work\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 12/20 - loss: 0.2131 -  - 1.21s\n",
      "[==============================] 100% Epoch 13/20 - loss: 0.1464 -  - 1.25s\n",
      "[==============================] 100% Epoch 14/20 - loss: 0.1343 -  - 1.26s\n",
      "[==============================] 100% Epoch 15/20 - loss: 0.0998 -  - 1.25s\n",
      "[==============================] 100% Epoch 16/20 - loss: 0.0886 -  - 1.25s\n",
      "Epoch 15 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 73 73 73 73 73 73 73  3]\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Found EOS token: 3\n",
      "Final translation: raining raining raining raining raining raining raining\n",
      "Input: je vais bien\n",
      "Output: raining raining raining raining raining raining raining\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 73 73 73 73 73 73 73  3]\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Found EOS token: 3\n",
      "Final translation: raining raining raining raining raining raining raining\n",
      "Input: comment allez-vous ?\n",
      "Output: raining raining raining raining raining raining raining\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 73 73 73 73 73 73 73  3]\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Found EOS token: 3\n",
      "Final translation: raining raining raining raining raining raining raining\n",
      "Input: bonjour\n",
      "Output: raining raining raining raining raining raining raining\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 17/20 - loss: 0.0809 -  - 1.26s\n",
      "[==============================] 100% Epoch 18/20 - loss: 0.0661 -  - 1.33s\n",
      "[==============================] 100% Epoch 19/20 - loss: 0.0611 -  - 1.31s\n",
      "[==============================] 100% Epoch 20/20 - loss: 0.0574 -  - 1.25s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monitor = TrainingMonitor(model, fr_tokenizer, en_tokenizer, test_sentences)\n",
    "lr = LearningRateScheduler(\n",
    "    schedule=\"warmup_cosine\",\n",
    "    initial_learning_rate=0.0001,\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=20,\n",
    "    batch_size=12,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss', patience=20),\n",
    "        monitor,\n",
    "        lr\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes:\n",
      "French vocab size: 107\n",
      "English vocab size: 95\n",
      "\n",
      "==================================================\n",
      "Testing: je vais bien\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [6, 40, 41]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [2, 6, 40, 41, 3]\n",
      "Padded sequence: [[ 2  6 40 41  3  0  0  0]]\n",
      "Raw prediction: [ 2 73 73 73 73 73 73 73  3]\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Found EOS token: 3\n",
      "Final translation: raining raining raining raining raining raining raining\n",
      "Translation: raining raining raining raining raining raining raining\n",
      "\n",
      "==================================================\n",
      "Testing: comment allez-vous ?\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [38, 39, 8, 7, 5]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [2, 38, 39, 8, 7, 5, 3]\n",
      "Padded sequence: [[ 2 38 39  8  7  5  3  0]]\n",
      "Raw prediction: [ 2 73 73 73 73 73 73 73  3]\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Found EOS token: 3\n",
      "Final translation: raining raining raining raining raining raining raining\n",
      "Translation: raining raining raining raining raining raining raining\n",
      "\n",
      "==================================================\n",
      "Testing: bonjour\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [32]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [2, 32, 3]\n",
      "Padded sequence: [[ 2 32  3  0  0  0  0  0]]\n",
      "Raw prediction: [ 2 73 73 73 73 73 73 73  3]\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Token 73 -> word: raining\n",
      "Found EOS token: 3\n",
      "Final translation: raining raining raining raining raining raining raining\n",
      "Translation: raining raining raining raining raining raining raining\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary sizes:\")\n",
    "print(f\"French vocab size: {len(fr_tokenizer.word_index)}\")\n",
    "print(f\"English vocab size: {len(en_tokenizer.word_index)}\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer, \n",
    "                      temperature=0.8,\n",
    "                      beam_size=10,\n",
    "                      min_length=2) \n",
    "    print(f\"Translation: {translation}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e205b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: je vais bien\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 8, 7, 8)\n",
      "First attention head values:\n",
      "[[0.20029563 0.19942448 0.20725221 0.19974921 0.19327846 0.\n",
      "  0.         0.        ]\n",
      " [0.20029325 0.19943252 0.20725276 0.19975518 0.19326629 0.\n",
      "  0.         0.        ]\n",
      " [0.20028801 0.1994326  0.20725065 0.19975687 0.19327187 0.\n",
      "  0.         0.        ]\n",
      " [0.20028121 0.19943791 0.20725839 0.19975882 0.19326367 0.\n",
      "  0.         0.        ]\n",
      " [0.20027662 0.19944207 0.20725907 0.19975834 0.19326389 0.\n",
      "  0.         0.        ]\n",
      " [0.20027687 0.19944068 0.20725896 0.19975863 0.19326486 0.\n",
      "  0.         0.        ]\n",
      " [0.20028094 0.19943636 0.20725508 0.19976103 0.19326659 0.\n",
      "  0.         0.        ]]\n",
      "\n",
      "Analyzing: comment allez-vous ?\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (1, 8, 7, 8)\n",
      "First attention head values:\n",
      "[[0.20057462 0.19977678 0.20656328 0.19895658 0.19412874 0.\n",
      "  0.         0.        ]\n",
      " [0.20057219 0.19978558 0.20656474 0.19896115 0.19411634 0.\n",
      "  0.         0.        ]\n",
      " [0.2005668  0.19978587 0.20656169 0.19896385 0.19412179 0.\n",
      "  0.         0.        ]\n",
      " [0.20055986 0.19979208 0.20656916 0.19896555 0.19411335 0.\n",
      "  0.         0.        ]\n",
      " [0.20055546 0.19979656 0.20657003 0.19896445 0.1941135  0.\n",
      "  0.         0.        ]\n",
      " [0.20055563 0.19979538 0.20656987 0.19896485 0.19411427 0.\n",
      "  0.         0.        ]\n",
      " [0.20055996 0.199791   0.20656618 0.19896662 0.19411623 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def analyze_attention_weights(model, input_sentence, fr_tokenizer):\n",
    "    tokens = fr_tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    \n",
    "    _ = model.predict(padded)\n",
    "    \n",
    "    attention_weights = model.decoder_layers[-1].cross_attention.attention_weights\n",
    "    \n",
    "    print(\"\\nAttention Analysis:\")\n",
    "    print(f\"Attention shape: {attention_weights.shape}\")\n",
    "    print(\"First attention head values:\")\n",
    "    print(attention_weights[0, 0])\n",
    "\n",
    "for sent in test_sentences[:2]:\n",
    "    print(f\"\\nAnalyzing: {sent}\")\n",
    "    analyze_attention_weights(model, sent, fr_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
