{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a036f9b8eee0491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.303382900Z",
     "start_time": "2024-11-12T00:29:31.023112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.losses import SequenceCrossEntropy\n",
    "from neuralnetlib.optimizers import Adam\n",
    "from neuralnetlib.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcb1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str, model, fr_tokenizer, en_tokenizer, \n",
    "             temperature=0.8, beam_size=10, min_length=2) -> str:\n",
    "    print(f\"\\nProcessing: {sentence}\")\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = fr_tokenizer.texts_to_sequences([sentence], preprocess_ponctuation=True)[0]\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Original words: {[fr_tokenizer.index_word.get(t, '<UNK>') for t in tokens]}\")\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    print(f\"With special tokens: {tokens}\")\n",
    "    \n",
    "    # Padding\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    print(f\"Padded sequence: {padded}\")\n",
    "    \n",
    "    pred = model.predict(padded, \n",
    "                        max_length=model.max_sequence_length,\n",
    "                        beam_size=beam_size,      \n",
    "                        alpha=0.6,         \n",
    "                        min_length=min_length,\n",
    "                        temperature=temperature)[0]\n",
    "    \n",
    "    print(f\"Raw prediction: {pred}\")\n",
    "    \n",
    "    words = []\n",
    "    for idx in pred[1:]:  # Skip SOS\n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.eos_token]:\n",
    "            print(f\"Found EOS token: {idx}\")\n",
    "            break\n",
    "            \n",
    "        if idx == en_tokenizer.word_index[en_tokenizer.pad_token]:\n",
    "            print(f\"Skipping PAD token: {idx}\")\n",
    "            continue\n",
    "            \n",
    "        word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "        print(f\"Token {idx} -> word: {word}\")\n",
    "        words.append(word)\n",
    "    \n",
    "    if not words:\n",
    "        print(\"Warning: Empty translation, using default handling...\")\n",
    "        for idx in pred[1:]:  # Skip SOS\n",
    "            if idx not in [en_tokenizer.word_index[token] for token in \n",
    "                         [en_tokenizer.pad_token, en_tokenizer.eos_token]]:\n",
    "                word = en_tokenizer.index_word.get(idx, en_tokenizer.unk_token)\n",
    "                words.append(word)\n",
    "    \n",
    "    translation = \" \".join(words) if words else \"[Translation failed]\"\n",
    "    print(f\"Final translation: {translation}\")\n",
    "    return translation\n",
    "\n",
    "class TrainingMonitor(Callback):\n",
    "    def __init__(self, model, fr_tokenizer, en_tokenizer, test_sentences):\n",
    "        self.model = model\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.test_sentences = test_sentences\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:  # Check every 5 epochs\n",
    "            print(f\"\\nEpoch {epoch} validation:\")\n",
    "            for sent in self.test_sentences:\n",
    "                translation = translate(\n",
    "                    sent, self.model, self.fr_tokenizer, self.en_tokenizer,\n",
    "                    temperature=0.8, beam_size=5, min_length=2\n",
    "                )\n",
    "                print(f\"Input: {sent}\")\n",
    "                print(f\"Output: {translation}\\n\")\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"je vais bien\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"bonjour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be237a3421e586a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.319383Z",
     "start_time": "2024-11-12T00:29:31.304382500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_sentences = [\n",
    "    \"bonjour.\",\n",
    "    \"au revoir.\",\n",
    "    \"merci beaucoup.\",\n",
    "    \"s'il vous plaît.\",\n",
    "    \"comment allez-vous ?\",\n",
    "    \"je vais bien.\",\n",
    "    \"je suis fatigué.\",\n",
    "    \"je suis content.\",\n",
    "    \"quel est votre nom ?\",\n",
    "    \"mon nom est Jean.\",\n",
    "    \"enchanté de vous rencontrer.\",\n",
    "    \"bonne journée.\",\n",
    "    \"bonne soirée.\",\n",
    "    \"à demain.\",\n",
    "    \"j'aime le café.\",\n",
    "    \"je n'aime pas le thé.\",\n",
    "    \"quelle heure est-il ?\",\n",
    "    \"il est trois heures.\",\n",
    "    \"où est la gare ?\",\n",
    "    \"la gare est près d'ici.\",\n",
    "    \"combien ça coûte ?\",\n",
    "    \"c'est trop cher.\",\n",
    "    \"parlez-vous anglais ?\",\n",
    "    \"un peu.\",\n",
    "    \"je ne comprends pas.\",\n",
    "    \"pouvez-vous répéter ?\",\n",
    "    \"je suis désolé.\",\n",
    "    \"pas de problème.\",\n",
    "    \"bon appétit.\",\n",
    "    \"à votre santé.\",\n",
    "    \"j'ai faim.\",\n",
    "    \"j'ai soif.\",\n",
    "    \"il fait beau aujourd'hui.\",\n",
    "    \"il pleut.\",\n",
    "    \"il fait froid.\",\n",
    "    \"il fait chaud.\",\n",
    "    \"je travaille ici.\",\n",
    "    \"où habitez-vous ?\",\n",
    "    \"j'habite à Paris.\",\n",
    "    \"quel âge avez-vous ?\",\n",
    "    \"j'ai vingt-cinq ans.\",\n",
    "    \"avez-vous des frères et sœurs ?\",\n",
    "    \"j'ai une sœur.\",\n",
    "    \"j'ai un chat.\",\n",
    "    \"j'aime voyager.\",\n",
    "    \"je suis étudiant.\",\n",
    "    \"je suis professeur.\",\n",
    "    \"au secours !\",\n",
    "    \"joyeux anniversaire !\",\n",
    "    \"félicitations !\"\n",
    "]\n",
    "\n",
    "en_sentences = [\n",
    "    \"hello.\",\n",
    "    \"goodbye.\",\n",
    "    \"thank you very much.\",\n",
    "    \"please.\",\n",
    "    \"how are you?\",\n",
    "    \"i am fine.\",\n",
    "    \"i am tired.\",\n",
    "    \"i am happy.\",\n",
    "    \"what is your name?\",\n",
    "    \"my name is John.\",\n",
    "    \"nice to meet you.\",\n",
    "    \"have a nice day.\",\n",
    "    \"have a good evening.\",\n",
    "    \"see you tomorrow.\",\n",
    "    \"i like coffee.\",\n",
    "    \"i don't like tea.\",\n",
    "    \"what time is it?\",\n",
    "    \"it is three o'clock.\",\n",
    "    \"where is the train station?\",\n",
    "    \"the station is nearby.\",\n",
    "    \"how much is it?\",\n",
    "    \"it's too expensive.\",\n",
    "    \"do you speak english?\",\n",
    "    \"a little.\",\n",
    "    \"i don't understand.\",\n",
    "    \"can you repeat?\",\n",
    "    \"i am sorry.\",\n",
    "    \"no problem.\",\n",
    "    \"enjoy your meal.\",\n",
    "    \"cheers.\",\n",
    "    \"i am hungry.\",\n",
    "    \"i am thirsty.\",\n",
    "    \"the weather is nice today.\",\n",
    "    \"it's raining.\",\n",
    "    \"it's cold.\",\n",
    "    \"it's hot.\",\n",
    "    \"i work here.\",\n",
    "    \"where do you live?\",\n",
    "    \"i live in Paris.\",\n",
    "    \"how old are you?\",\n",
    "    \"i am twenty-five years old.\",\n",
    "    \"do you have brothers and sisters?\",\n",
    "    \"i have a sister.\",\n",
    "    \"i have a cat.\",\n",
    "    \"i like to travel.\",\n",
    "    \"i am a student.\",\n",
    "    \"i am a teacher.\",\n",
    "    \"help!\",\n",
    "    \"happy birthday!\",\n",
    "    \"congratulations!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c0d8598f0ba7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T00:29:31.350386300Z",
     "start_time": "2024-11-12T00:29:31.320384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "en_tokenizer = Tokenizer(filters=\"\")  # else the tokenizer would remove the special characters including ponctuation\n",
    "\n",
    "fr_tokenizer.fit_on_texts(fr_sentences, preprocess_ponctuation=True)\n",
    "en_tokenizer.fit_on_texts(en_sentences, preprocess_ponctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67338439",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fr_tokenizer.texts_to_sequences(fr_sentences, preprocess_ponctuation=True)\n",
    "y_train = en_tokenizer.texts_to_sequences(en_sentences, preprocess_ponctuation=True)\n",
    "\n",
    "max_len_x = max(len(seq) for seq in x_train)\n",
    "max_len_y = max(len(seq) for seq in y_train)\n",
    "max_seq_len = max(max_len_x, max_len_y)\n",
    "\n",
    "vocab_size_fr = len(fr_tokenizer.word_index)\n",
    "vocab_size_en = len(en_tokenizer.word_index)\n",
    "max_vocab_size = max(vocab_size_fr, vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5501a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_en: 95, vocab_size_fr: 107\n",
      "max_len_x: 8, max_len_y: 8, max_vocab_size: 107, max_seq_len: 8\n",
      "French sentences:\n",
      "['bonjour.', 'au revoir.', 'merci beaucoup.', \"s'il vous plaît.\", 'comment allez-vous ?', 'je vais bien.', 'je suis fatigué.', 'je suis content.', 'quel est votre nom ?', 'mon nom est Jean.', 'enchanté de vous rencontrer.', 'bonne journée.', 'bonne soirée.', 'à demain.', \"j'aime le café.\", \"je n'aime pas le thé.\", 'quelle heure est-il ?', 'il est trois heures.', 'où est la gare ?', \"la gare est près d'ici.\", 'combien ça coûte ?', \"c'est trop cher.\", 'parlez-vous anglais ?', 'un peu.', 'je ne comprends pas.', 'pouvez-vous répéter ?', 'je suis désolé.', 'pas de problème.', 'bon appétit.', 'à votre santé.', \"j'ai faim.\", \"j'ai soif.\", \"il fait beau aujourd'hui.\", 'il pleut.', 'il fait froid.', 'il fait chaud.', 'je travaille ici.', 'où habitez-vous ?', \"j'habite à Paris.\", 'quel âge avez-vous ?', \"j'ai vingt-cinq ans.\", 'avez-vous des frères et sœurs ?', \"j'ai une sœur.\", \"j'ai un chat.\", \"j'aime voyager.\", 'je suis étudiant.', 'je suis professeur.', 'au secours !', 'joyeux anniversaire !', 'félicitations !']\n",
      "English sentences:\n",
      "['hello.', 'goodbye.', 'thank you very much.', 'please.', 'how are you?', 'i am fine.', 'i am tired.', 'i am happy.', 'what is your name?', 'my name is John.', 'nice to meet you.', 'have a nice day.', 'have a good evening.', 'see you tomorrow.', 'i like coffee.', \"i don't like tea.\", 'what time is it?', \"it is three o'clock.\", 'where is the train station?', 'the station is nearby.', 'how much is it?', \"it's too expensive.\", 'do you speak english?', 'a little.', \"i don't understand.\", 'can you repeat?', 'i am sorry.', 'no problem.', 'enjoy your meal.', 'cheers.', 'i am hungry.', 'i am thirsty.', 'the weather is nice today.', \"it's raining.\", \"it's cold.\", \"it's hot.\", 'i work here.', 'where do you live?', 'i live in Paris.', 'how old are you?', 'i am twenty-five years old.', 'do you have brothers and sisters?', 'i have a sister.', 'i have a cat.', 'i like to travel.', 'i am a student.', 'i am a teacher.', 'help!', 'happy birthday!', 'congratulations!']\n",
      "French tokenizer:\n",
      "{'<PAD>': 0, '.': 1, '?': 2, 'je': 3, 'vous': 4, '-': 5, \"j'\": 6, 'il': 7, 'est': 8, 'suis': 9, 'ai': 10, 'à': 11, 'aime': 12, 'pas': 13, 'fait': 14, '!': 15, 'au': 16, 'quel': 17, 'votre': 18, 'nom': 19, 'de': 20, 'bonne': 21, 'le': 22, 'où': 23, 'la': 24, 'gare': 25, 'ici': 26, 'un': 27, 'avez': 28, 'bonjour': 29, 'revoir': 30, 'merci': 31, 'beaucoup': 32, \"s'\": 33, 'plaît': 34, 'comment': 35, 'allez': 36, 'vais': 37, 'bien': 38, 'fatigué': 39, 'content': 40, 'mon': 41, 'jean': 42, 'enchanté': 43, 'rencontrer': 44, 'journée': 45, 'soirée': 46, 'demain': 47, 'café': 48, \"n'\": 49, 'thé': 50, 'quelle': 51, 'heure': 52, 'trois': 53, 'heures': 54, 'près': 55, \"d'\": 56, 'combien': 57, 'ça': 58, 'coûte': 59, \"c'\": 60, 'trop': 61, 'cher': 62, 'parlez': 63, 'anglais': 64, 'peu': 65, 'ne': 66, 'comprends': 67, 'pouvez': 68, 'répéter': 69, 'désolé': 70, 'problème': 71, 'bon': 72, 'appétit': 73, 'santé': 74, 'faim': 75, 'soif': 76, 'beau': 77, \"aujourd'hui\": 78, 'pleut': 79, 'froid': 80, 'chaud': 81, 'travaille': 82, 'habitez': 83, 'habite': 84, 'paris': 85, 'âge': 86, 'vingt': 87, 'cinq': 88, 'ans': 89, 'des': 90, 'frères': 91, 'et': 92, 'sœurs': 93, 'une': 94, 'sœur': 95, 'chat': 96, 'voyager': 97, 'étudiant': 98, 'professeur': 99, 'secours': 100, 'joyeux': 101, 'anniversaire': 102, 'félicitations': 103, '<UNK>': 105, '<SOS>': 106, '<EOS>': 107}\n",
      "English tokenizer:\n",
      "{'<PAD>': 0, '.': 1, 'i': 2, '?': 3, 'you': 4, 'am': 5, 'is': 6, 'a': 7, 'have': 8, \"it's\": 9, 'how': 10, 'nice': 11, 'like': 12, 'it': 13, 'the': 14, 'do': 15, '!': 16, 'much': 17, 'are': 18, 'happy': 19, 'what': 20, 'your': 21, 'name': 22, 'to': 23, \"don't\": 24, 'where': 25, 'station': 26, 'live': 27, 'old': 28, 'hello': 29, 'goodbye': 30, 'thank': 31, 'very': 32, 'please': 33, 'fine': 34, 'tired': 35, 'my': 36, 'john': 37, 'meet': 38, 'day': 39, 'good': 40, 'evening': 41, 'see': 42, 'tomorrow': 43, 'coffee': 44, 'tea': 45, 'time': 46, 'three': 47, \"o'\": 48, 'clock': 49, 'train': 50, 'nearby': 51, 'too': 52, 'expensive': 53, 'speak': 54, 'english': 55, 'little': 56, 'understand': 57, 'can': 58, 'repeat': 59, 'sorry': 60, 'no': 61, 'problem': 62, 'enjoy': 63, 'meal': 64, 'cheers': 65, 'hungry': 66, 'thirsty': 67, 'weather': 68, 'today': 69, 'raining': 70, 'cold': 71, 'hot': 72, 'work': 73, 'here': 74, 'in': 75, 'paris': 76, 'twenty': 77, '-': 78, 'five': 79, 'years': 80, 'brothers': 81, 'and': 82, 'sisters': 83, 'sister': 84, 'cat': 85, 'travel': 86, 'student': 87, 'teacher': 88, 'help': 89, 'birthday': 90, 'congratulations': 91, '<UNK>': 93, '<SOS>': 94, '<EOS>': 95}\n"
     ]
    }
   ],
   "source": [
    "# Verify all data\n",
    "print(f\"vocab_size_en: {vocab_size_en}, vocab_size_fr: {vocab_size_fr}\")\n",
    "print(f\"max_len_x: {max_len_x}, max_len_y: {max_len_y}, max_vocab_size: {max_vocab_size}, max_seq_len: {max_seq_len}\")\n",
    "print(\"French sentences:\")\n",
    "print(fr_sentences)\n",
    "print(\"English sentences:\")\n",
    "print(en_sentences)\n",
    "print(\"French tokenizer:\")\n",
    "print(fr_tokenizer.word_index)\n",
    "print(\"English tokenizer:\")\n",
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d2884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  vocab_size=107,\n",
      "  d_model=32,\n",
      "  n_heads=2,\n",
      "  n_encoder_layers=2,\n",
      "  n_decoder_layers=2,\n",
      "  d_ff=64,\n",
      "  dropout_rate=0.4,\n",
      "  max_sequence_length=8\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=max_vocab_size,\n",
    "    d_model=32,        \n",
    "    n_heads=2,         \n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    d_ff=64,           \n",
    "    dropout_rate=0.4,\n",
    "    max_sequence_length=max_seq_len,\n",
    "    random_state=42,\n",
    "    unk_idx=fr_tokenizer.word_index[fr_tokenizer.unk_token],\n",
    "    sos_idx=fr_tokenizer.word_index[fr_tokenizer.sos_token],\n",
    "    eos_idx=fr_tokenizer.word_index[fr_tokenizer.eos_token],\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss_function=SequenceCrossEntropy(\n",
    "        label_smoothing=0.1,\n",
    "    ),\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9\n",
    "    ),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train_padded = model.prepare_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527483e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting training data:\n",
      "\n",
      "Example 1:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.26337567 -0.88846732  1.47795107 -0.91079202 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.21345575 -0.84524867  1.5465388  -0.8678127  -0.8678127\n",
      " -0.8678127  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Example 2:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.55359679 -0.24105097 -0.88846732  1.47795107 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.19089172 -0.84524867  1.5465388  -0.8678127  -0.8678127\n",
      " -0.8678127  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Example 3:\n",
      "Input sequence:\n",
      "Raw: [ 1.45562637 -0.21872627 -0.19640156 -0.88846732  1.47795107 -0.91079202\n",
      " -0.91079202 -0.91079202]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n",
      "\n",
      "Output sequence:\n",
      "Raw: [ 1.52397477 -0.16832768 -0.77755657 -0.14576365 -0.48422414 -0.84524867\n",
      "  1.5465388  -0.8678127 ]\n",
      "Tokens: ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "def inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer):\n",
    "    print(\"\\nInspecting training data:\")\n",
    "    for i in range(min(3, len(x_train_padded))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"Input sequence:\")\n",
    "        print(f\"Raw: {x_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [fr_tokenizer.index_word.get(idx, '<UNK>') for idx in x_train_padded[i]])\n",
    "        \n",
    "        print(\"\\nOutput sequence:\")\n",
    "        print(f\"Raw: {y_train_padded[i]}\")\n",
    "        print(\"Tokens:\", [en_tokenizer.index_word.get(idx, '<UNK>') for idx in y_train_padded[i]])\n",
    "\n",
    "inspect_training_data(x_train_padded, y_train_padded, fr_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bdab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/300 - loss: 5.3306 -  - 0.23s\n",
      "Epoch 0 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  86  83  85  86  88  95  86 107]\n",
      "Token 86 -> word: travel\n",
      "Token 83 -> word: sisters\n",
      "Token 85 -> word: cat\n",
      "Token 86 -> word: travel\n",
      "Token 88 -> word: teacher\n",
      "Found EOS token: 95\n",
      "Final translation: travel sisters cat travel teacher\n",
      "Input: je vais bien\n",
      "Output: travel sisters cat travel teacher\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  90  85  87  98  99  99  99 107]\n",
      "Token 90 -> word: birthday\n",
      "Token 85 -> word: cat\n",
      "Token 87 -> word: student\n",
      "Token 98 -> word: <UNK>\n",
      "Token 99 -> word: <UNK>\n",
      "Token 99 -> word: <UNK>\n",
      "Token 99 -> word: <UNK>\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: birthday cat student <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: birthday cat student <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  83  85  82  85 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 83 -> word: sisters\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat sisters cat and cat <UNK>\n",
      "Input: bonjour\n",
      "Output: cat and cat sisters cat and cat <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 2/300 - loss: 4.2427 -  - 0.22s\n",
      "[==============================] 100% Epoch 3/300 - loss: 4.1811 -  - 0.22s\n",
      "[==============================] 100% Epoch 4/300 - loss: 4.1868 -  - 0.22s\n",
      "[==============================] 100% Epoch 5/300 - loss: 4.1316 -  - 0.23s\n",
      "[==============================] 100% Epoch 6/300 - loss: 4.1015 -  - 0.22s\n",
      "Epoch 5 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  85  87  92  85 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 87 -> word: student\n",
      "Token 92 -> word: <UNK>\n",
      "Token 85 -> word: cat\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat cat student <UNK> cat <UNK>\n",
      "Input: je vais bien\n",
      "Output: cat and cat cat student <UNK> cat <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  89  82  85  95  99  99  98 107]\n",
      "Token 89 -> word: help\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Found EOS token: 95\n",
      "Final translation: help and cat\n",
      "Input: comment allez-vous ?\n",
      "Output: help and cat\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  83  85  85  85 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 83 -> word: sisters\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat sisters cat cat cat <UNK>\n",
      "Input: bonjour\n",
      "Output: cat and cat sisters cat cat cat <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 7/300 - loss: 4.0682 -  - 0.21s\n",
      "[==============================] 100% Epoch 8/300 - loss: 3.5124 -  - 0.21s\n",
      "[==============================] 100% Epoch 9/300 - loss: 3.1903 -  - 0.23s\n",
      "[==============================] 100% Epoch 10/300 - loss: 2.1211 -  - 0.22s\n",
      "[==============================] 100% Epoch 11/300 - loss: 1.4592 -  - 0.23s\n",
      "Epoch 10 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  85  88  91  86 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 88 -> word: teacher\n",
      "Token 91 -> word: congratulations\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat cat teacher congratulations travel <UNK>\n",
      "Input: je vais bien\n",
      "Output: cat and cat cat teacher congratulations travel <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  88  83  85  86  99  99  92 107]\n",
      "Token 88 -> word: teacher\n",
      "Token 83 -> word: sisters\n",
      "Token 85 -> word: cat\n",
      "Token 86 -> word: travel\n",
      "Token 99 -> word: <UNK>\n",
      "Token 99 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: teacher sisters cat travel <UNK> <UNK> <UNK> <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: teacher sisters cat travel <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  82  85  85  85 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat and cat cat cat <UNK>\n",
      "Input: bonjour\n",
      "Output: cat and cat and cat cat cat <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 12/300 - loss: 1.3833 -  - 0.24s\n",
      "[==============================] 100% Epoch 13/300 - loss: 1.3386 -  - 0.21s\n",
      "[==============================] 100% Epoch 14/300 - loss: 1.3018 -  - 0.21s\n",
      "[==============================] 100% Epoch 15/300 - loss: 1.2757 -  - 0.21s\n",
      "[==============================] 100% Epoch 16/300 - loss: 1.2565 -  - 0.24s\n",
      "Epoch 15 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  85  87  88  86 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 87 -> word: student\n",
      "Token 88 -> word: teacher\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat cat student teacher travel <UNK>\n",
      "Input: je vais bien\n",
      "Output: cat and cat cat student teacher travel <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  88  82  85  88  98  98  88 107]\n",
      "Token 88 -> word: teacher\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 88 -> word: teacher\n",
      "Token 98 -> word: <UNK>\n",
      "Token 98 -> word: <UNK>\n",
      "Token 88 -> word: teacher\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: teacher and cat teacher <UNK> <UNK> teacher <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: teacher and cat teacher <UNK> <UNK> teacher <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  82  85  85  86 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat and cat cat travel <UNK>\n",
      "Input: bonjour\n",
      "Output: cat and cat and cat cat travel <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 17/300 - loss: 1.2340 -  - 0.21s\n",
      "[==============================] 100% Epoch 18/300 - loss: 1.2168 -  - 0.22s\n",
      "[==============================] 100% Epoch 19/300 - loss: 1.1993 -  - 0.21s\n",
      "[==============================] 100% Epoch 20/300 - loss: 1.1928 -  - 0.23s\n",
      "[==============================] 100% Epoch 21/300 - loss: 1.1887 -  - 0.23s\n",
      "Epoch 20 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  85  86  88  86 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 86 -> word: travel\n",
      "Token 88 -> word: teacher\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat cat travel teacher travel <UNK>\n",
      "Input: je vais bien\n",
      "Output: cat and cat cat travel teacher travel <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  88  82  85  88  98  92  91 107]\n",
      "Token 88 -> word: teacher\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 88 -> word: teacher\n",
      "Token 98 -> word: <UNK>\n",
      "Token 92 -> word: <UNK>\n",
      "Token 91 -> word: congratulations\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: teacher and cat teacher <UNK> <UNK> congratulations <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: teacher and cat teacher <UNK> <UNK> congratulations <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  82  85  85  86 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat and cat cat travel <UNK>\n",
      "Input: bonjour\n",
      "Output: cat and cat and cat cat travel <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 22/300 - loss: 1.1824 -  - 0.21s\n",
      "[==============================] 100% Epoch 23/300 - loss: 1.1833 -  - 0.21s\n",
      "[==============================] 100% Epoch 24/300 - loss: 1.1905 -  - 0.22s\n",
      "[==============================] 100% Epoch 25/300 - loss: 1.1924 -  - 0.22s\n",
      "[==============================] 100% Epoch 26/300 - loss: 1.1851 -  - 0.21s\n",
      "Epoch 25 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  85  85  86  86 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 85 -> word: cat\n",
      "Token 86 -> word: travel\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat cat cat travel travel <UNK>\n",
      "Input: je vais bien\n",
      "Output: cat and cat cat cat travel travel <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  86  83  83  83  89  99  89 107]\n",
      "Token 86 -> word: travel\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 83 -> word: sisters\n",
      "Token 89 -> word: help\n",
      "Token 99 -> word: <UNK>\n",
      "Token 89 -> word: help\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: travel sisters sisters sisters help <UNK> help <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: travel sisters sisters sisters help <UNK> help <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  85  82  85  82  85  86  86 107]\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 82 -> word: and\n",
      "Token 85 -> word: cat\n",
      "Token 86 -> word: travel\n",
      "Token 86 -> word: travel\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: cat and cat and cat travel travel <UNK>\n",
      "Input: bonjour\n",
      "Output: cat and cat and cat travel travel <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 27/300 - loss: 1.1796 -  - 0.22s\n",
      "[==============================] 100% Epoch 28/300 - loss: 1.1701 -  - 0.21s\n",
      "[==============================] 100% Epoch 29/300 - loss: 1.1730 -  - 0.21s\n",
      "[==============================] 100% Epoch 30/300 - loss: 1.1704 -  - 0.23s\n",
      "[==============================] 100% Epoch 31/300 - loss: 1.1676 -  - 0.21s\n",
      "Epoch 30 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  84  82  82  82  82  84  85 107]\n",
      "Token 84 -> word: sister\n",
      "Token 82 -> word: and\n",
      "Token 82 -> word: and\n",
      "Token 82 -> word: and\n",
      "Token 82 -> word: and\n",
      "Token 84 -> word: sister\n",
      "Token 85 -> word: cat\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sister and and and and sister cat <UNK>\n",
      "Input: je vais bien\n",
      "Output: sister and and and and sister cat <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  83  82  81  81  81  85  83 107]\n",
      "Token 83 -> word: sisters\n",
      "Token 82 -> word: and\n",
      "Token 81 -> word: brothers\n",
      "Token 81 -> word: brothers\n",
      "Token 81 -> word: brothers\n",
      "Token 85 -> word: cat\n",
      "Token 83 -> word: sisters\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sisters and brothers brothers brothers cat sisters <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: sisters and brothers brothers brothers cat sisters <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  84  82  82  82  83  84  85 107]\n",
      "Token 84 -> word: sister\n",
      "Token 82 -> word: and\n",
      "Token 82 -> word: and\n",
      "Token 82 -> word: and\n",
      "Token 83 -> word: sisters\n",
      "Token 84 -> word: sister\n",
      "Token 85 -> word: cat\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: sister and and and sisters sister cat <UNK>\n",
      "Input: bonjour\n",
      "Output: sister and and and sisters sister cat <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 32/300 - loss: 1.1643 -  - 0.22s\n",
      "[==============================] 100% Epoch 33/300 - loss: 1.1739 -  - 0.21s\n",
      "[==============================] 100% Epoch 34/300 - loss: 1.1843 -  - 0.20s\n",
      "[==============================] 100% Epoch 35/300 - loss: 1.1903 -  - 0.21s\n",
      "[==============================] 100% Epoch 36/300 - loss: 1.1929 -  - 0.19s\n",
      "Epoch 35 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  74  70  73  74  73  75  75 107]\n",
      "Token 74 -> word: here\n",
      "Token 70 -> word: raining\n",
      "Token 73 -> word: work\n",
      "Token 74 -> word: here\n",
      "Token 73 -> word: work\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: here raining work here work in in <UNK>\n",
      "Input: je vais bien\n",
      "Output: here raining work here work in in <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  67  66  64  64  64  64  69 107]\n",
      "Token 67 -> word: thirsty\n",
      "Token 66 -> word: hungry\n",
      "Token 64 -> word: meal\n",
      "Token 64 -> word: meal\n",
      "Token 64 -> word: meal\n",
      "Token 64 -> word: meal\n",
      "Token 69 -> word: today\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: thirsty hungry meal meal meal meal today <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: thirsty hungry meal meal meal meal today <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  73  81  75  73  73  75  75 107]\n",
      "Token 73 -> word: work\n",
      "Token 81 -> word: brothers\n",
      "Token 75 -> word: in\n",
      "Token 73 -> word: work\n",
      "Token 73 -> word: work\n",
      "Token 75 -> word: in\n",
      "Token 75 -> word: in\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: work brothers in work work in in <UNK>\n",
      "Input: bonjour\n",
      "Output: work brothers in work work in in <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 37/300 - loss: 1.1853 -  - 0.20s\n",
      "[==============================] 100% Epoch 38/300 - loss: 1.1824 -  - 0.20s\n",
      "[==============================] 100% Epoch 39/300 - loss: 1.1805 -  - 0.19s\n",
      "[==============================] 100% Epoch 40/300 - loss: 1.1791 -  - 0.19s\n",
      "[==============================] 100% Epoch 41/300 - loss: 1.1786 -  - 0.21s\n",
      "Epoch 40 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106  28  26  28  26  27  28  31 107]\n",
      "Token 28 -> word: old\n",
      "Token 26 -> word: station\n",
      "Token 28 -> word: old\n",
      "Token 26 -> word: station\n",
      "Token 27 -> word: live\n",
      "Token 28 -> word: old\n",
      "Token 31 -> word: thank\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: old station old station live old thank <UNK>\n",
      "Input: je vais bien\n",
      "Output: old station old station live old thank <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106  22  20  22  21  19  21  24 107]\n",
      "Token 22 -> word: name\n",
      "Token 20 -> word: what\n",
      "Token 22 -> word: name\n",
      "Token 21 -> word: your\n",
      "Token 19 -> word: happy\n",
      "Token 21 -> word: your\n",
      "Token 24 -> word: don't\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: name what name your happy your don't <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: name what name your happy your don't <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106  27  22  27  22  22  27  28 107]\n",
      "Token 27 -> word: live\n",
      "Token 22 -> word: name\n",
      "Token 27 -> word: live\n",
      "Token 22 -> word: name\n",
      "Token 22 -> word: name\n",
      "Token 27 -> word: live\n",
      "Token 28 -> word: old\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: live name live name name live old <UNK>\n",
      "Input: bonjour\n",
      "Output: live name live name name live old <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 42/300 - loss: 1.1822 -  - 0.20s\n",
      "[==============================] 100% Epoch 43/300 - loss: 1.1819 -  - 0.19s\n",
      "[==============================] 100% Epoch 44/300 - loss: 1.1748 -  - 0.20s\n",
      "[==============================] 100% Epoch 45/300 - loss: 1.1750 -  - 0.19s\n",
      "[==============================] 100% Epoch 46/300 - loss: 1.1705 -  - 0.20s\n",
      "Epoch 45 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: je vais bien\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: bonjour\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 47/300 - loss: 1.1690 -  - 0.19s\n",
      "[==============================] 100% Epoch 48/300 - loss: 1.1653 -  - 0.19s\n",
      "[==============================] 100% Epoch 49/300 - loss: 1.1713 -  - 0.20s\n",
      "[==============================] 100% Epoch 50/300 - loss: 1.1701 -  - 0.19s\n",
      "[==============================] 100% Epoch 51/300 - loss: 1.1737 -  - 0.20s\n",
      "Epoch 50 validation:\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: je vais bien\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: comment allez-vous ?\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Input: bonjour\n",
      "Output: . . . . . . . <UNK>\n",
      "\n",
      "\n",
      "[==============================] 100% Epoch 52/300 - loss: 1.1790 -  - 0.21s\n",
      "Early stopping triggered after epoch 52\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monitor = TrainingMonitor(model, fr_tokenizer, en_tokenizer, test_sentences)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train_padded,\n",
    "    epochs=300,\n",
    "    batch_size=2,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='loss', patience=20),\n",
    "        monitor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1dc335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sizes:\n",
      "French vocab size: 107\n",
      "English vocab size: 95\n",
      "\n",
      "==================================================\n",
      "Testing: je vais bien\n",
      "\n",
      "Processing: je vais bien\n",
      "Tokens: [3, 37, 38]\n",
      "Original words: ['je', 'vais', 'bien']\n",
      "With special tokens: [106, 3, 37, 38, 107]\n",
      "Padded sequence: [[106   3  37  38 107   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Translation: . . . . . . . <UNK>\n",
      "\n",
      "==================================================\n",
      "Testing: comment allez-vous ?\n",
      "\n",
      "Processing: comment allez-vous ?\n",
      "Tokens: [35, 36, 5, 4, 2]\n",
      "Original words: ['comment', 'allez', '-', 'vous', '?']\n",
      "With special tokens: [106, 35, 36, 5, 4, 2, 107]\n",
      "Padded sequence: [[106  35  36   5   4   2 107   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Translation: . . . . . . . <UNK>\n",
      "\n",
      "==================================================\n",
      "Testing: bonjour\n",
      "\n",
      "Processing: bonjour\n",
      "Tokens: [29]\n",
      "Original words: ['bonjour']\n",
      "With special tokens: [106, 29, 107]\n",
      "Padded sequence: [[106  29 107   0   0   0   0   0]]\n",
      "Raw prediction: [106   1   1   1   1   1   1   1 107]\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 1 -> word: .\n",
      "Token 107 -> word: <UNK>\n",
      "Final translation: . . . . . . . <UNK>\n",
      "Translation: . . . . . . . <UNK>\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary sizes:\")\n",
    "print(f\"French vocab size: {len(fr_tokenizer.word_index)}\")\n",
    "print(f\"English vocab size: {len(en_tokenizer.word_index)}\")\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Testing: {sent}\")\n",
    "    translation = translate(sent, model, fr_tokenizer, en_tokenizer, \n",
    "                      temperature=0.8,\n",
    "                      beam_size=10,\n",
    "                      min_length=2) \n",
    "    print(f\"Translation: {translation}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e205b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: je vais bien\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (2, 2, 7, 8)\n",
      "First attention head values:\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "Analyzing: comment allez-vous ?\n",
      "\n",
      "Attention Analysis:\n",
      "Attention shape: (2, 2, 7, 8)\n",
      "First attention head values:\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def analyze_attention_weights(model, input_sentence, fr_tokenizer):\n",
    "    tokens = fr_tokenizer.texts_to_sequences([input_sentence])[0]\n",
    "    tokens = fr_tokenizer.encode_special_tokens([tokens])[0]\n",
    "    padded = pad_sequences([tokens], max_length=model.max_sequence_length, padding='post', \n",
    "                          pad_value=fr_tokenizer.word_index[fr_tokenizer.pad_token])\n",
    "    \n",
    "    _ = model.predict(padded)\n",
    "    \n",
    "    attention_weights = model.decoder_layers[-1].cross_attention.attention_weights\n",
    "    \n",
    "    print(\"\\nAttention Analysis:\")\n",
    "    print(f\"Attention shape: {attention_weights.shape}\")\n",
    "    print(\"First attention head values:\")\n",
    "    print(attention_weights[0, 0])\n",
    "\n",
    "for sent in test_sentences[:2]:\n",
    "    print(f\"\\nAnalyzing: {sent}\")\n",
    "    analyze_attention_weights(model, sent, fr_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
