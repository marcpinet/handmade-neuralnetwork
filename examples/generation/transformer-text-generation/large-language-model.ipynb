{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralnetlib.models import Transformer\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.utils import train_test_split\n",
    "from neuralnetlib.losses import CrossEntropyWithLabelSmoothing\n",
    "from neuralnetlib.optimizers import Adam\n",
    "from neuralnetlib.callbacks import EarlyStopping, Callback, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_causal_lm_data(text_data, tokenizer, max_length=512, stride=256):\n",
    "    \"\"\"\n",
    "    Prépare les données pour l'entraînement causal LM en utilisant une fenêtre glissante\n",
    "    \"\"\"\n",
    "    # Tokenisation du texte complet\n",
    "    tokens = tokenizer.texts_to_sequences([text_data], add_special_tokens=True)[0]\n",
    "    \n",
    "    # Création des séquences d'entraînement avec une fenêtre glissante\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokens) - max_length + 1, stride):\n",
    "        sequence = tokens[i:i + max_length]\n",
    "        if len(sequence) == max_length:\n",
    "            sequences.append(sequence)\n",
    "    \n",
    "    # Conversion en array numpy\n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    # Création des entrées et cibles (décalées d'une position)\n",
    "    X = sequences[:, :-1]\n",
    "    y = sequences[:, 1:]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationCallback(Callback):\n",
    "    def __init__(self, model, tokenizer, prompt_texts, max_length=50, temperature=0.8):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_texts = prompt_texts\n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nGénération d'exemples après l'epoch {epoch}:\")\n",
    "        for prompt in self.prompt_texts:\n",
    "            sequence = self.tokenizer.texts_to_sequences([prompt], add_special_tokens=True)[0]\n",
    "            input_sequence = pad_sequences([sequence], max_length=self.model.max_sequence_length, \n",
    "                                        padding='post', pad_value=self.model.PAD_IDX)\n",
    "            \n",
    "            generated = self.model.predict(\n",
    "                input_sequence,\n",
    "                max_length=self.max_length,\n",
    "                temperature=self.temperature,\n",
    "                beam_size=1  # On utilise un beam search de 1 pour la génération simple\n",
    "            )\n",
    "            \n",
    "            generated_text = self.tokenizer.sequences_to_texts(generated.tolist())[0]\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Généré: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_causal_lm(text_data, model, tokenizer, max_length=512, batch_size=32, epochs=10):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle comme un LLM causal\n",
    "    \"\"\"\n",
    "    # Préparation des données\n",
    "    X, y = prepare_causal_lm_data(text_data, tokenizer, max_length)\n",
    "    \n",
    "    # Prompts de test pour la génération\n",
    "    test_prompts = [\n",
    "        \"Il était une fois\",\n",
    "        \"Le chat\",\n",
    "        \"Je pense que\",\n",
    "        \"Dans la forêt\"\n",
    "    ]\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='loss', patience=5, restore_best_weights=True),\n",
    "        TextGenerationCallback(model, tokenizer, test_prompts),\n",
    "        LearningRateScheduler(\n",
    "            schedule=\"warmup_cosine\",\n",
    "            initial_learning_rate=0.0001,\n",
    "            verbose=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Entraînement\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        metrics=['bleu'],\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Génère du texte à partir d'un prompt\n",
    "    \"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([prompt], add_special_tokens=True)[0]\n",
    "    input_sequence = pad_sequences([sequence], max_length=model.max_sequence_length, \n",
    "                                 padding='post', pad_value=model.PAD_IDX)\n",
    "    \n",
    "    generated = model.predict(\n",
    "        input_sequence,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        beam_size=1\n",
    "    )\n",
    "    \n",
    "    return tokenizer.sequences_to_texts(generated.tolist())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger votre corpus de texte\n",
    "with open('text8_light.txt', 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Créer et entraîner le tokenizer\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts([text_data])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    d_ff=516,\n",
    "    dropout_rate=0.1,\n",
    "    max_sequence_length=512,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss_function=CrossEntropyWithLabelSmoothing(label_smoothing=0.1),\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.98,\n",
    "        epsilon=1e-9,\n",
    "        clip_norm=1.0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.000100\n",
      "[=====-------------------------] 16% Epoch 1/10 - loss: 15.0754 - bleu: 9.1815612211e-09 - 206.08s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001C5708C25F0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_causal_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Générer du texte\u001b[39;00m\n\u001b[0;32m      5\u001b[0m generated \u001b[38;5;241m=\u001b[39m generate_text(model, tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis would ensure that\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mtrain_causal_lm\u001b[1;34m(text_data, model, tokenizer, max_length, batch_size, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     19\u001b[0m     TextGenerationCallback(model, tokenizer, test_prompts),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m ]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Entraînement\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbleu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mc:\\users\\marcp\\documents\\programming\\python\\neuralnetlib\\neuralnetlib\\models.py:1766\u001b[0m, in \u001b[0;36mTransformer.fit\u001b[1;34m(self, x_train, y_train, epochs, batch_size, verbose, metrics, random_state, validation_data, callbacks)\u001b[0m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m-> 1766\u001b[0m         metric_value \u001b[38;5;241m=\u001b[39m metric(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_list\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39mvstack(y_true_list))\n\u001b[0;32m   1767\u001b[0m         metrics_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_number(metric_value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1768\u001b[0m progress_bar(j \u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_batches,\n\u001b[0;32m   1769\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_number(error\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(j\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_str[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "history = train_causal_lm(text_data, model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposal proposal evaluate quantification decorations quba evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate quantification evaluate runner runner runner runner runner evaluate quantification quba evaluate quantification evaluate quba evaluate quantification evaluate quantification evaluate quantification evaluate quantification\n"
     ]
    }
   ],
   "source": [
    "# Générer du texte\n",
    "generated = generate_text(model, tokenizer, \"this would ensure that\", temperature=0.8)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
