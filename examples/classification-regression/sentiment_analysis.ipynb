{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple cancer binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:48:27.381361400Z",
     "start_time": "2024-11-09T23:48:19.040830100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from neuralnetlib.model import Model\n",
    "from neuralnetlib.layers import Input, Dense, Embedding, LSTM, Bidirectional, Attention, GlobalAveragePooling1D\n",
    "from neuralnetlib.preprocessing import Tokenizer, pad_sequences\n",
    "from neuralnetlib.metrics import accuracy_score\n",
    "from neuralnetlib.utils import train_test_split\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:48:28.917027800Z",
     "start_time": "2024-11-09T23:48:27.382360400Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:48:29.180263400Z",
     "start_time": "2024-11-09T23:48:28.918028800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (20000, 200)\n",
      "x_test shape: (25000, 200)\n",
      "y_train shape: (20000,)\n",
      "y_test shape: (25000,)\n",
      "x_train[0]: [4.500e+01 1.080e+02 1.000e+01 1.000e+01 1.100e+01 4.000e+00 6.500e+01\n",
      " 3.960e+03 9.000e+00 1.100e+01 4.100e+01 4.020e+02 2.000e+00 7.800e+02\n",
      " 3.300e+01 2.000e+00 6.130e+03 1.100e+01 2.000e+00 4.000e+00 2.763e+03\n",
      " 8.440e+02 2.600e+01 2.000e+00 2.240e+02 5.000e+00 1.930e+02 3.960e+03\n",
      " 3.900e+01 4.400e+01 7.900e+02 1.530e+02 1.540e+02 1.430e+02 4.100e+01\n",
      " 2.521e+03 5.600e+01 8.000e+00 4.100e+01 2.028e+03 5.590e+02 1.100e+01\n",
      " 4.000e+00 2.000e+01 4.400e+01 6.383e+03 5.284e+03 4.740e+02 4.820e+02\n",
      " 1.300e+01 6.600e+01 9.200e+01 1.040e+02 2.250e+02 6.000e+00 4.040e+02\n",
      " 5.240e+02 1.800e+01 3.960e+03 1.800e+01 1.110e+02 7.000e+00 1.780e+02\n",
      " 3.960e+03 4.510e+02 4.420e+02 7.600e+01 9.900e+01 9.760e+02 6.000e+00\n",
      " 1.369e+03 1.100e+01 2.630e+02 2.000e+00 4.600e+02 8.519e+03 2.000e+00\n",
      " 9.000e+00 3.084e+03 5.900e+01 9.000e+00 5.500e+01 7.207e+03 2.000e+00\n",
      " 5.000e+00 2.000e+00 5.900e+01 4.700e+01 7.750e+02 7.000e+00 9.963e+03\n",
      " 5.900e+01 4.700e+01 6.000e+00 8.700e+01 3.930e+02 3.100e+01 1.500e+01\n",
      " 3.775e+03 1.100e+01 1.290e+02 3.300e+02 7.300e+01 1.030e+02 4.000e+00\n",
      " 2.000e+01 9.000e+00 1.200e+02 1.793e+03 8.000e+00 2.000e+00 2.000e+00\n",
      " 2.000e+00 5.071e+03 3.960e+03 4.700e+01 2.470e+02 6.000e+00 5.879e+03\n",
      " 8.220e+02 7.400e+01 2.000e+00 2.100e+01 1.460e+02 1.688e+03 8.000e+00\n",
      " 4.909e+03 1.500e+01 4.800e+01 2.000e+00 1.999e+03 1.100e+01 4.000e+00\n",
      " 2.170e+02 1.300e+01 1.040e+02 5.900e+01 8.000e+01 2.700e+03 8.300e+01\n",
      " 1.200e+01 4.300e+01 1.700e+01 3.960e+03 3.418e+03 5.300e+01 9.760e+02\n",
      " 5.000e+00 6.861e+03 1.700e+01 5.900e+01 2.140e+02 9.220e+02 2.000e+00\n",
      " 4.600e+02 5.603e+03 2.000e+00 4.860e+02 5.000e+00 1.557e+03 2.000e+00\n",
      " 5.500e+01 7.300e+01 1.400e+02 1.404e+03 5.000e+00 8.510e+02 1.400e+01\n",
      " 2.000e+01 4.500e+01 2.400e+01 4.000e+01 2.330e+02 3.340e+02 8.740e+02\n",
      " 1.100e+02 5.000e+00 6.000e+01 1.510e+02 1.200e+01 1.600e+01 5.260e+02\n",
      " 3.400e+01 1.091e+03 2.000e+00 1.200e+01 9.000e+00 3.680e+02 7.000e+00\n",
      " 2.000e+00 2.442e+03 8.700e+01 3.700e+02 1.102e+03 1.524e+03 5.000e+00\n",
      " 7.300e+01 2.240e+02 2.060e+02 8.440e+02]\n",
      "y_train[0]: 1\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "x_train = pad_sequences(x_train, max_length=max_len)\n",
    "x_test = pad_sequences(x_test, max_length=max_len)\n",
    "\n",
    "# cuz we don't want to overfit on test data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}')\n",
    "\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')\n",
    "\n",
    "print(f'x_train[0]: {x_train[0]}')\n",
    "print(f'y_train[0]: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:48:29.225261400Z",
     "start_time": "2024-11-09T23:48:29.181263400Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.add(Input(max_len))\n",
    "model.add(Embedding(max_words, 100, weights_init='xavier'))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Attention())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:48:29.231258700Z",
     "start_time": "2024-11-09T23:48:29.197260800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "-------------------------------------------------\n",
      "Layer 1: Input(input_shape=(200,))\n",
      "Layer 2: Embedding(input_dim=10000, output_dim=100)\n",
      "Layer 3: Bidirectional(layer=LSTM(units=32, return_sequences=True, return_state=False, random_state=None, clip_value=5.0))\n",
      "Layer 4: Attention(use_scale=True, score_mode=dot, return_sequences=False)\n",
      "Layer 5: Dense(units=1)\n",
      "Layer 6: Activation(Sigmoid)\n",
      "-------------------------------------------------\n",
      "Loss function: BinaryCrossentropy\n",
      "Optimizer: Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clip_norm=None, clip_value=None)\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss_function='binary_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:48:29.644801100Z",
     "start_time": "2024-11-09T23:48:29.212262100Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\model.py:262\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x_train, y_train, epochs, batch_size, verbose, metrics, random_state, validation_data, callbacks, plot_decision_boundary)\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_batch\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    261\u001B[0m     y_batch \u001B[38;5;241m=\u001B[39m y_batch\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 262\u001B[0m error \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    263\u001B[0m predictions_list\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictions)\n\u001B[0;32m    264\u001B[0m y_true_list\u001B[38;5;241m.\u001B[39mappend(y_batch)\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\model.py:166\u001B[0m, in \u001B[0;36mModel.train_on_batch\u001B[1;34m(self, x_batch, y_batch)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], (LSTM, Bidirectional, GRU)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mreturn_sequences:\n\u001B[0;32m    164\u001B[0m     error \u001B[38;5;241m=\u001B[39m error\u001B[38;5;241m.\u001B[39mreshape(error\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], error\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 166\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\model.py:106\u001B[0m, in \u001B[0;36mModel.backward_pass\u001B[1;34m(self, error)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(layer, (LSTM, GRU, Bidirectional)):\n\u001B[1;32m--> 106\u001B[0m         error, _, _, layer_gradients \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m         all_gradients\u001B[38;5;241m.\u001B[39mextend(layer_gradients\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\layers.py:1799\u001B[0m, in \u001B[0;36mBidirectional.backward_pass\u001B[1;34m(self, output_error)\u001B[0m\n\u001B[0;32m   1796\u001B[0m     forward_error \u001B[38;5;241m=\u001B[39m output_error[:, :forward_dim]\n\u001B[0;32m   1797\u001B[0m     backward_error \u001B[38;5;241m=\u001B[39m output_error[:, forward_dim:]\n\u001B[1;32m-> 1799\u001B[0m forward_dx, forward_h_next, forward_c_next, forward_grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mforward_error\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1800\u001B[0m backward_dx, backward_h_next, backward_c_next, backward_grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackward_layer\u001B[38;5;241m.\u001B[39mbackward_pass(backward_error)\n\u001B[0;32m   1802\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(output_error\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\layers.py:1692\u001B[0m, in \u001B[0;36mLSTM.backward_pass\u001B[1;34m(self, output_error)\u001B[0m\n\u001B[0;32m   1689\u001B[0m     dh \u001B[38;5;241m=\u001B[39m output_error[:, t, :] \u001B[38;5;241m+\u001B[39m dh_next\n\u001B[0;32m   1691\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcell\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache[t]\n\u001B[1;32m-> 1692\u001B[0m     dx_t, dh_next, dc_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcell\u001B[38;5;241m.\u001B[39mbackward(dh, dc_next)\n\u001B[0;32m   1693\u001B[0m     dx[:, t, :] \u001B[38;5;241m=\u001B[39m dx_t\n\u001B[0;32m   1695\u001B[0m \u001B[38;5;66;03m# Moyenner les gradients sur les pas de temps\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test), metrics=['accuracy'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:48:29.646800900Z",
     "start_time": "2024-11-09T23:48:29.644801100Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, _ = model.evaluate(x_val, y_val)\n",
    "print(f'Loss: {loss}')\n",
    "\n",
    "predictions = model.predict(x_val)\n",
    "y_pred = np.where(predictions > 0.5, 1, 0)\n",
    "accuracy = accuracy_score(y_pred, y_val)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
