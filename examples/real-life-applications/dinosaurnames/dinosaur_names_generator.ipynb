{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from neuralnetlib.layers import Input, Embedding, LSTM, Dense\n",
    "from neuralnetlib.model import Model\n",
    "from neuralnetlib.preprocessing import one_hot_encode\n",
    "from neuralnetlib.callbacks import EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:06:56.985216500Z",
     "start_time": "2024-11-09T17:06:56.707537600Z"
    }
   },
   "id": "a036f9b8eee0491"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aachenosaurus', 'Aardonyx', 'Abdallahsaurus', 'Abelisaurus', 'Abrictosaurus']\n"
     ]
    }
   ],
   "source": [
    "with open('dinos.txt', 'r', encoding='utf-8') as f:\n",
    "    names = [line.strip() for line in f]\n",
    "\n",
    "print(names[:5])  # we display the first 5 names of the list to see if they were correctly loaded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:06:57.001570400Z",
     "start_time": "2024-11-09T17:06:56.987217Z"
    }
   },
   "id": "be237a3421e586a2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 26\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(name) for name in names]\n",
    "max_length = max(lengths)\n",
    "print(f\"Maximum length: {max_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:06:57.046402500Z",
     "start_time": "2024-11-09T17:06:57.001570400Z"
    }
   },
   "id": "f4c0d8598f0ba7a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 54\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "PAD_TOKEN = ''  # Padding token (index 0)\n",
    "EOS_TOKEN = '$'  # End of sequence token (index 1)\n",
    "max_length = 15  # Maximum sequence length\n",
    "\n",
    "# Mapping dictionaries\n",
    "char_to_index = {PAD_TOKEN: 0, EOS_TOKEN: 1}\n",
    "index_to_char = {0: PAD_TOKEN, 1: EOS_TOKEN}\n",
    "\n",
    "# Extract unique characters and sort them\n",
    "unique_chars = sorted(set(''.join(names)))\n",
    "\n",
    "# Build character <-> index mappings starting at 2\n",
    "for idx, char in enumerate(unique_chars, start=2):\n",
    "    char_to_index[char] = idx\n",
    "    index_to_char[idx] = char\n",
    "\n",
    "vocab_size = len(char_to_index)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:06:57.051521Z",
     "start_time": "2024-11-09T17:06:57.018296300Z"
    }
   },
   "id": "c410380fa90e7694"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 54\n",
      "Shape of X data: (18374, 15)\n",
      "Shape of y labels: (18374, 54)\n",
      "\n",
      "Example for Aachenosaurus:\n",
      "Input sequence: [ 0  0  0  0  0  0  0  0  0 28 28 30 35 32 41]\n",
      "Expected output: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Decoding example sequence:\n",
      "['', '', '', '', '', '', '', '', '', 'a', 'a', 'c', 'h', 'e', 'n']\n",
      "Next character: o\n"
     ]
    }
   ],
   "source": [
    "# Training sequences\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "# Create sequences and their next characters\n",
    "for name in names:\n",
    "    name = name.lower()\n",
    "    name_chars = list(name) + [EOS_TOKEN]\n",
    "\n",
    "    for i in range(len(name_chars) - 1):\n",
    "        # Extract sequence\n",
    "        seq = name_chars[max(0, i - max_length + 1):i + 1]\n",
    "\n",
    "        # Padding and conversion to indices\n",
    "        padded_seq = [0] * (max_length - len(seq)) + [char_to_index[char] for char in seq]\n",
    "\n",
    "        sequences.append(padded_seq)\n",
    "        next_chars.append(char_to_index[name_chars[i + 1]])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(sequences)\n",
    "y = one_hot_encode(np.array(next_chars), vocab_size)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Shape of X data: {X.shape}\")\n",
    "print(f\"Shape of y labels: {y.shape}\")\n",
    "\n",
    "# Display an example for verification\n",
    "print(f\"\\nExample for {names[0]}:\")\n",
    "print(f\"Input sequence: {X[5]}\")\n",
    "print(f\"Expected output: {y[5]}\")\n",
    "\n",
    "# Visualize tokens for the first example\n",
    "print(\"\\nDecoding example sequence:\")\n",
    "print([index_to_char[idx] for idx in X[5]])\n",
    "print(f\"Next character: {index_to_char[next_chars[5]]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:06:57.064138800Z",
     "start_time": "2024-11-09T17:06:57.034316700Z"
    }
   },
   "id": "1364a6786997a8f5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "-------------------------------------------------\n",
      "Layer 1: Input(input_shape=(15,))\n",
      "Layer 2: Embedding(input_dim=54, output_dim=32)\n",
      "Layer 3: LSTM(units=128, return_sequences=False, return_state=False)\n",
      "Layer 4: Dense(units=54)\n",
      "Layer 5: Activation(Softmax)\n",
      "-------------------------------------------------\n",
      "Loss function: CategoricalCrossentropy\n",
      "Optimizer: Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clip_norm=None, clip_value=None)\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model creation\n",
    "embedding_dim = 32\n",
    "lstm_units = 128\n",
    "\n",
    "model = Model()\n",
    "model.add(Input(max_length))\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(LSTM(units=lstm_units))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss_function='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:06:57.124184500Z",
     "start_time": "2024-11-09T17:06:57.064138800Z"
    }
   },
   "id": "317820e906d2065"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================] 100% Epoch 1/100 - loss: 2.5075 -  - 16.45s\n",
      "[==============================] 100% Epoch 2/100 - loss: 1.9356 -  - 17.93s\n",
      "[==============================] 100% Epoch 3/100 - loss: 1.7980 -  - 18.04s\n",
      "[==============================] 100% Epoch 4/100 - loss: 1.7408 -  - 17.66s\n",
      "[==============================] 100% Epoch 5/100 - loss: 1.7028 -  - 18.69s\n",
      "[==============================] 100% Epoch 6/100 - loss: 1.6683 -  - 17.95s\n",
      "[======================--------] 73% Epoch 7/100 - loss: 1.6499 -  - 13.32s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 9\u001B[0m\n\u001B[0;32m      2\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m EarlyStopping(\n\u001B[0;32m      3\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      4\u001B[0m     patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,\n\u001B[0;32m      5\u001B[0m     restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\model.py:218\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x_train, y_train, epochs, batch_size, verbose, metrics, random_state, validation_data, callbacks, plot_decision_boundary)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_batch\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    217\u001B[0m     y_batch \u001B[38;5;241m=\u001B[39m y_batch\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 218\u001B[0m error \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    219\u001B[0m predictions_list\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictions)\n\u001B[0;32m    220\u001B[0m y_true_list\u001B[38;5;241m.\u001B[39mappend(y_batch)\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\model.py:112\u001B[0m, in \u001B[0;36mModel.train_on_batch\u001B[1;34m(self, x_batch, y_batch)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_on_batch\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_batch: np\u001B[38;5;241m.\u001B[39mndarray, y_batch: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_true \u001B[38;5;241m=\u001B[39m y_batch\n\u001B[1;32m--> 112\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictions\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m    114\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_function(y_batch, predictions)\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\model.py:79\u001B[0m, in \u001B[0;36mModel.forward_pass\u001B[1;34m(self, X, training)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(layer, (Dropout, LSTM, Bidirectional)):\n\u001B[1;32m---> 79\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     81\u001B[0m         X \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mforward_pass(X)\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\layers.py:1551\u001B[0m, in \u001B[0;36mLSTM.forward_pass\u001B[1;34m(self, x, training)\u001B[0m\n\u001B[0;32m   1549\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(timesteps):\n\u001B[0;32m   1550\u001B[0m     x_t \u001B[38;5;241m=\u001B[39m x[:, t, :]\n\u001B[1;32m-> 1551\u001B[0m     h, c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1552\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_sequences:\n\u001B[0;32m   1553\u001B[0m         all_h\u001B[38;5;241m.\u001B[39mappend(h)\n",
      "File \u001B[1;32m~\\Documents\\Programming\\Python\\neuralnetlib\\neuralnetlib\\layers.py:1404\u001B[0m, in \u001B[0;36mLSTMCell.forward\u001B[1;34m(self, x_t, h_prev, c_prev)\u001B[0m\n\u001B[0;32m   1401\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_prev \u001B[38;5;241m=\u001B[39m c_prev\n\u001B[0;32m   1403\u001B[0m \u001B[38;5;66;03m# Forget gate\u001B[39;00m\n\u001B[1;32m-> 1404\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_gate_input \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(h_prev, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mUf) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf\n\u001B[0;32m   1405\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msigmoid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_gate_input)\n\u001B[0;32m   1407\u001B[0m \u001B[38;5;66;03m# Input gate\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Create EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(X, y),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:09:36.063324200Z",
     "start_time": "2024-11-09T17:06:57.079615300Z"
    }
   },
   "id": "ccca9fb1b43dd948"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate new names\n",
    "def generate_name(model, min_length=5):\n",
    "    current_sequence = [0] * max_length\n",
    "    generated_name = \"\"\n",
    "\n",
    "    while len(generated_name) < max_length:\n",
    "        x = np.array([current_sequence])\n",
    "        preds = model.predict(x)[0]\n",
    "\n",
    "        # Select next character using random.choices\n",
    "        next_char_idx = random.choices(range(vocab_size), weights=preds, k=1)[0]\n",
    "        next_char = index_to_char[next_char_idx]\n",
    "\n",
    "        # STOP if minimum length reached and EOS encountered\n",
    "        if len(generated_name) >= min_length and next_char == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "        # Add character if it's neither PAD nor EOS\n",
    "        if next_char not in [PAD_TOKEN, EOS_TOKEN]:\n",
    "            generated_name += next_char\n",
    "\n",
    "        # Update current sequence\n",
    "        current_sequence = current_sequence[1:] + [next_char_idx]\n",
    "\n",
    "    return generated_name.capitalize() if len(generated_name) >= min_length else None\n",
    "\n",
    "# Generate multiple names\n",
    "generated_names = []\n",
    "number_of_names = 5\n",
    "min_length = 5\n",
    "\n",
    "while len(generated_names) < number_of_names:\n",
    "    name = generate_name(model, min_length)\n",
    "    if name is not None and name not in generated_names:\n",
    "        generated_names.append(name)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGenerated names:\")\n",
    "for name in generated_names:\n",
    "    print(f\"{name} ({len(name)} characters)\")\n",
    "\n",
    "# Check originality\n",
    "print(\"\\nAre all names original?\", all(name.lower() not in [n.lower() for n in names] for name in generated_names))\n",
    "\n",
    "# Length statistics\n",
    "lengths = [len(name) for name in generated_names]\n",
    "print(f\"\\nAverage length: {sum(lengths)/len(lengths):.1f} characters\")\n",
    "print(f\"Minimum length: {min(lengths)} characters\")\n",
    "print(f\"Maximum length: {max(lengths)} characters\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-09T17:09:36.064322Z",
     "start_time": "2024-11-09T17:09:36.064322Z"
    }
   },
   "id": "68ec75af38129a34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
