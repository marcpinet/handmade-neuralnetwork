{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnetlib.preprocessing import pad_sequences, clip_gradients, normalize_gradient, cosine_similarity, StandardScaler, MinMaxScaler, Imputer\n",
    "from neuralnetlib.utils import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Séquences d'origine:\n",
      "[1, 2, 3]\n",
      "[4, 5]\n",
      "[6, 7, 8, 9]\n",
      "[0]\n",
      "\n",
      "Séquences après padding:\n",
      "[1 2 3 0 0]\n",
      "[4 5 0 0 0]\n",
      "[6 7 8 9 0]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "sequences = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8, 9],\n",
    "    [0]\n",
    "]\n",
    "\n",
    "max_len = 5\n",
    "padded_sequences = pad_sequences(sequences, max_length=max_len, padding='post', truncating='post', pad_value=0)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Séquences d'origine:\")\n",
    "for seq in sequences:\n",
    "    print(seq)\n",
    "\n",
    "print(\"\\nSéquences après padding:\")\n",
    "for seq in padded_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip & Normalize gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original gradients: [ 3. 14.  0.]\n",
      "Clipped gradients (threshold=5.0): [1.04764544 4.88901207 0.        ]\n",
      "Normalized gradients (scale=1.0): [0.20952909 0.97780241 0.        ]\n"
     ]
    }
   ],
   "source": [
    "gradients = np.array([3.0, 14.0, 0.0])\n",
    "\n",
    "threshold = 5.0\n",
    "clipped_gradients = clip_gradients(gradients, threshold=threshold)\n",
    "\n",
    "scale = 1.0\n",
    "normalized_gradients = normalize_gradient(gradients, scale=scale)\n",
    "\n",
    "print(\"Original gradients:\", gradients)\n",
    "print(f\"Clipped gradients (threshold={threshold}):\", clipped_gradients)\n",
    "print(f\"Normalized gradients (scale={scale}):\", normalized_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between vector1 and vector2: 0.9746318461970762\n",
      "Cosine similarity between vector1 and vector3: 0.0\n"
     ]
    }
   ],
   "source": [
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 5, 6])\n",
    "vector3 = np.array([0, 0, 0])\n",
    "\n",
    "similarity1 = cosine_similarity(vector1, vector2)\n",
    "similarity2 = cosine_similarity(vector1, vector3)\n",
    "\n",
    "print(\"Cosine similarity between vector1 and vector2:\", similarity1)\n",
    "print(\"Cosine similarity between vector1 and vector3:\", similarity2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard & Min-Max Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Standard Scaled Data:\n",
      "[[-1.22474487 -1.22474487 -1.22474487]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.22474487  1.22474487  1.22474487]]\n",
      "\n",
      "MinMax Scaled Data:\n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "data_standard = scaler_standard.fit_transform(data)\n",
    "print(\"\\nStandard Scaled Data:\")\n",
    "print(data_standard)\n",
    "\n",
    "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "data_minmax = scaler_minmax.fit_transform(data)\n",
    "print(\"\\nMinMax Scaled Data:\")\n",
    "print(data_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "y: [0 1 0 1 0 1 0 1 0 1]\n",
      "\n",
      "x_train: [6 7 1 8 4 3 5]\n",
      "x_test: [10  2  9]\n",
      "y_train: [1 0 0 1 1 0 0]\n",
      "y_test: [1 1 0]\n",
      "\n",
      "x_train: [6 7 1 8 4 3 5]\n",
      "x_test: [10  2  9]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "print(\"\\nx_train:\", x_train)\n",
    "print(\"x_test:\", x_test)\n",
    "print(\"y_train:\", y_train)\n",
    "print(\"y_test:\", y_test)\n",
    "\n",
    "x_train, x_test = train_test_split(x, test_size=0.3, random_state=42)\n",
    "print(\"\\nx_train:\", x_train)\n",
    "print(\"x_test:\", x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 1. nan  3.]\n",
      " [nan  2.  4.]\n",
      " [ 5.  6. nan]]\n",
      "\n",
      "Testing strategy: mean\n",
      "Imputed Data:\n",
      "[[1.  4.  3. ]\n",
      " [3.  2.  4. ]\n",
      " [5.  6.  3.5]]\n",
      "\n",
      "Testing strategy: median\n",
      "Imputed Data:\n",
      "[[1.  4.  3. ]\n",
      " [3.  2.  4. ]\n",
      " [5.  6.  3.5]]\n",
      "\n",
      "Testing strategy: mode\n",
      "Imputed Data:\n",
      "[[1. 2. 3.]\n",
      " [1. 2. 4.]\n",
      " [5. 6. 3.]]\n",
      "\n",
      "Testing strategy: constant\n",
      "Imputed Data:\n",
      "[[1. 0. 3.]\n",
      " [0. 2. 4.]\n",
      " [5. 6. 0.]]\n",
      "\n",
      "Testing strategy: random\n",
      "Imputed Data:\n",
      "[[1. 2. 3.]\n",
      " [1. 2. 4.]\n",
      " [5. 6. 3.]]\n",
      "\n",
      "Original 1D Data: [ 1. nan  3. nan  5.]\n",
      "Imputed 1D Data: [1. 3. 3. 3. 5.]\n",
      "\n",
      "Imputed Data with Missing Indicators:\n",
      "[[1.  4.  3.  0.  1.  0. ]\n",
      " [3.  2.  4.  1.  0.  0. ]\n",
      " [5.  6.  3.5 0.  0.  1. ]]\n",
      "\n",
      "Imputed data (with indicators):\n",
      "[[1.  4.  3.  0.  1.  0. ]\n",
      " [3.  2.  4.  1.  0.  0. ]\n",
      " [5.  6.  3.5 0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "strategies = [\"mean\", \"median\", \"mode\", \"constant\", \"random\"]\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nTesting strategy: {strategy}\")\n",
    "    \n",
    "    if strategy == \"constant\":\n",
    "        imputer = Imputer(strategy=strategy, fill_value=0)\n",
    "    else:\n",
    "        imputer = Imputer(strategy=strategy, random_state=42)\n",
    "    \n",
    "    imputer.fit(data)\n",
    "    transformed_data = imputer.transform(data)\n",
    "    \n",
    "    print(\"Imputed Data:\")\n",
    "    print(transformed_data)\n",
    "\n",
    "data_1d = np.array([1.0, np.nan, 3.0, np.nan, 5.0])\n",
    "imputer = Imputer(strategy=\"mean\")\n",
    "print(\"\\nOriginal 1D Data:\", data_1d)\n",
    "transformed_1d = imputer.fit_transform(data_1d)\n",
    "print(\"Imputed 1D Data:\", transformed_1d)\n",
    "\n",
    "imputer_with_indicator = Imputer(strategy=\"mean\", add_indicator=True)\n",
    "imputer_with_indicator.fit(data)\n",
    "transformed_with_indicators = imputer_with_indicator.transform(data)\n",
    "print(\"\\nImputed Data with Missing Indicators:\")\n",
    "print(transformed_with_indicators)\n",
    "\n",
    "data = np.array([\n",
    "    [1.0, np.nan, 3.0],\n",
    "    [np.nan, 2.0, 4.0],\n",
    "    [5.0, 6.0, np.nan]\n",
    "])\n",
    "\n",
    "imputer = Imputer(strategy=\"mean\", add_indicator=True)\n",
    "\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"\\nImputed data (with indicators):\")\n",
    "print(imputed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
