{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnetlib.preprocessing import NGram, CountVectorizer, Tokenizer\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aachenosaurus', 'Aardonyx', 'Abdallahsaurus', 'Abelisaurus', 'Abrictosaurus']\n"
     ]
    }
   ],
   "source": [
    "with open('dinos.txt', 'r', encoding='utf-8') as f:\n",
    "    names = [line.strip() for line in f]\n",
    "\n",
    "print(names[:5])  # display the first 5 names of the list to check if they were loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated names using the model:\n",
      "\n",
      "Limicosaurus\n",
      "Sucaraptosaurus\n",
      "Kitang\n",
      "Kitalchecon\n",
      "Walus\n",
      "Notitaveirus\n",
      "Elmaryosaurus\n",
      "Eocephosaurus\n",
      "Koptosaurus\n",
      "Salasaurus\n",
      "\n",
      "All generated names are unique!\n"
     ]
    }
   ],
   "source": [
    "model = NGram(n=3, token_type=\"char\")\n",
    "model.fit(names)\n",
    "\n",
    "print(\"Generated names using the model:\\n\")\n",
    "\n",
    "generated_names = model.generate_sequences(\n",
    "    n_sequences=10,\n",
    "    min_length=5,\n",
    "    max_length=15\n",
    ")\n",
    "\n",
    "for name in generated_names:\n",
    "    print(name)\n",
    "    \n",
    "\n",
    "for name in generated_names:\n",
    "    assert name not in names\n",
    "print(\"\\nAll generated names are unique!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "{'brown': 0, 'dog': 1, 'fox': 2, 'is': 3, 'jump': 4, 'jumps': 5, 'lazy': 6, 'never': 7, 'over': 8, 'quick': 9, 'quickly': 10, 'the': 11, 'very': 12}\n",
      "\n",
      "Feature Names:\n",
      "['brown' 'dog' 'fox' 'is' 'jump' 'jumps' 'lazy' 'never' 'over' 'quick'\n",
      " 'quickly' 'the' 'very']\n",
      "\n",
      "Transformed Matrix:\n",
      "[[1 1 1 0 0 1 1 0 1 1 0 2 0]\n",
      " [0 1 0 0 1 0 1 1 1 0 1 1 0]\n",
      " [1 0 1 1 0 0 0 0 0 2 0 1 1]]\n",
      "\n",
      "Transformed New Documents:\n",
      "[[1 0 1 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"The quick brown fox is very quick\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_vocabulary())\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTransformed Matrix:\")\n",
    "print(X)\n",
    "\n",
    "new_documents = [\n",
    "    \"The quick brown fox\",\n",
    "    \"The lazy dog sleeps\"\n",
    "]\n",
    "X_new = vectorizer.transform(new_documents)\n",
    "\n",
    "print(\"\\nTransformed New Documents:\")\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 'char' mode:\n",
      "Sequences (char mode): [[2, 10, 11, 5, 4, 12, 7, 9, 13, 14, 4, 19, 8, 6, 20, 17, 4, 21, 6, 22, 4, 23, 7, 24, 25, 26, 4, 6, 15, 5, 8, 4, 10, 11, 5, 4, 18, 27, 28, 16, 4, 29, 6, 30, 1, 3], [2, 17, 5, 15, 5, 8, 4, 23, 7, 24, 25, 4, 6, 15, 5, 8, 4, 10, 11, 5, 4, 18, 27, 28, 16, 4, 29, 6, 30, 4, 12, 7, 9, 13, 14, 18, 16, 1, 3], [2, 10, 11, 5, 4, 12, 7, 9, 13, 14, 4, 19, 8, 6, 20, 17, 4, 21, 6, 22, 4, 9, 26, 4, 15, 5, 8, 16, 4, 12, 7, 9, 13, 14, 1, 3]]\n",
      "Vocabulary size: 31\n",
      "Reconstructed text: ['<SOS>the quick brown fox jumps over the lazy dog<UNK><EOS>', '<SOS>never jump over the lazy dog quickly<UNK><EOS>', '<SOS>the quick brown fox is very quick<UNK><EOS>'] \n",
      "\n",
      "Testing 'word' mode:\n",
      "Sequences (word mode): [[2, 4, 5, 6, 7, 11, 8, 4, 9, 1, 3], [2, 12, 13, 8, 4, 9, 10, 1, 3], [2, 4, 5, 6, 7, 15, 16, 1, 3]]\n",
      "Vocabulary size: 17\n",
      "Reconstructed text: ['<SOS> the quick brown fox jumps over the lazy <UNK> <EOS>', '<SOS> never jump over the lazy dog <UNK> <EOS>', '<SOS> the quick brown fox is very <UNK> <EOS>'] \n",
      "\n",
      "Testing 'bpe' mode:\n",
      "Sequences (BPE mode): [[2, 5, 6, 10, 4, 11, 12, 4, 13, 14, 15, 16, 17, 18, 4, 7, 5, 9, 19, 20, 8, 21, 4, 22, 1, 3], [2, 23, 24, 7, 14, 15, 16, 17, 4, 7, 5, 9, 19, 20, 8, 21, 4, 22, 6, 9, 8, 1, 3], [2, 5, 6, 10, 4, 11, 12, 4, 13, 25, 18, 7, 8, 6, 1, 3]]\n",
      "Vocabulary size: 26\n",
      "Reconstructed text: ['<SOS> the quick br o wn f o x j u m p s o ver the l a z y d o g <UNK> <EOS>', '<SOS> n e ver j u m p o ver the l a z y d o g quick l y <UNK> <EOS>', '<SOS> the quick br o wn f o x i s ver y quick <UNK> <EOS>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog quickly.\",\n",
    "    \"The quick brown fox is very quick.\"\n",
    "]\n",
    "\n",
    "print(\"Testing 'char' mode:\")\n",
    "tokenizer_char = Tokenizer(mode='char')\n",
    "tokenizer_char.fit_on_texts(texts)\n",
    "char_sequences = tokenizer_char.texts_to_sequences(texts)\n",
    "print(\"Sequences (char mode):\", char_sequences)\n",
    "print(\"Vocabulary size:\", tokenizer_char.get_vocab_size())\n",
    "print(\"Reconstructed text:\", tokenizer_char.sequences_to_texts(char_sequences), \"\\n\")\n",
    "\n",
    "print(\"Testing 'word' mode:\")\n",
    "tokenizer_word = Tokenizer(mode='word')\n",
    "tokenizer_word.fit_on_texts(texts)\n",
    "word_sequences = tokenizer_word.texts_to_sequences(texts)\n",
    "print(\"Sequences (word mode):\", word_sequences)\n",
    "print(\"Vocabulary size:\", tokenizer_word.get_vocab_size())\n",
    "print(\"Reconstructed text:\", tokenizer_word.sequences_to_texts(word_sequences), \"\\n\")\n",
    "\n",
    "print(\"Testing 'bpe' mode:\")\n",
    "tokenizer_bpe = Tokenizer(mode='bpe', bpe_merges=10)\n",
    "tokenizer_bpe.fit_on_texts(texts)\n",
    "bpe_sequences = tokenizer_bpe.texts_to_sequences(texts)\n",
    "print(\"Sequences (BPE mode):\", bpe_sequences)\n",
    "print(\"Vocabulary size:\", tokenizer_bpe.get_vocab_size())\n",
    "print(\"Reconstructed text:\", tokenizer_bpe.sequences_to_texts(bpe_sequences), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
